(minigpt4) D:\Pycoding\CoLLM-main\CoLLM-main>python train_collm_mf_din.py --cfg-path=train_configs/collm_pretrain_mf_ood.yaml

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
binary_path: D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll
CUDA SETUP: Loading binary D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll...
W1012 22:03:08.019638 30116 site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
Not using distributed mode
2025-10-12 22:03:08,100 [INFO] Building datasets...
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\train data size: (33891, 7)
Movie OOD datasets, max history length: 10
2025-10-12 22:03:08,307 [INFO] Movie OOD datasets, max history length:10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\valid_small data size: (5200, 7)
Movie OOD datasets, max history length: 10
2025-10-12 22:03:08,346 [INFO] Movie OOD datasets, max history length:10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\test data size: (7331, 7)
Movie OOD datasets, max history length: 10
2025-10-12 22:03:08,522 [INFO] Movie OOD datasets, max history length:10
data dir: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\
2025-10-12 22:03:08,681 [INFO]
=====  Running Parameters    =====
2025-10-12 22:03:08,681 [INFO] {
    "amp": true,
    "batch_size_eval": 64,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_url": "env://",
    "distributed": false,
    "evaluate": false,
    "init_lr": 0.001,
    "iters_per_epoch": 50,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 200,
    "min_lr": 8e-05,
    "mode": "v2",
    "num_workers": 0,
    "output_dir": "minigpt4rec-log",
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "rec_pretrain",
    "test_splits": [
        "test",
        "valid"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "valid"
    ],
    "warmup_lr": 1e-05,
    "warmup_steps": 200,
    "weight_decay": 0.001,
    "world_size": 1
}
2025-10-12 22:03:08,681 [INFO]
======  Dataset Attributes  ======
2025-10-12 22:03:08,681 [INFO]
======== movie_ood =======
2025-10-12 22:03:08,683 [INFO] {
    "build_info": {
        "storage": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
    },
    "data_type": "default",
    "path": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
}
2025-10-12 22:03:08,683 [INFO]
======  Model Attributes  ======
2025-10-12 22:03:08,683 [INFO] {
    "ans_type": "v2",
    "arch": "mini_gpt4rec_v2",
    "end_sym": "###",
    "freeze_lora": false,
    "freeze_proj": true,
    "freeze_rec": true,
    "item_num": -100,
    "llama_model": "Vicuna-7b-delta-v0",
    "lora_config": {
        "alpha": 16,
        "dropout": 0.05,
        "r": 8,
        "target_modules": [
            "q_proj",
            "v_proj"
        ],
        "use_lora": true
    },
    "max_txt_len": 1024,
    "model_type": "pretrain_vicuna",
    "proj_drop": 0,
    "proj_mid_times": 10,
    "proj_token_num": 1,
    "prompt_path": "prompts/tallrec_movie.txt",
    "prompt_template": "{}",
    "rec_config": {
        "embedding_size": 256,
        "item_num": 3256,
        "pretrained_path": "collm-trained-models/0912_ml1m_oodv2_best_model_d256lr-0.001wd0.0001.pth",
        "user_num": 839
    },
    "rec_model": "MF",
    "user_num": -100
}
runing MiniGPT4Rec_v2 ......
Loading Rec_model
### rec_encoder: MF
creat MF model, user num: 839 item num: 3256
successfully load the pretrained model......
2025-10-12 22:03:08,720 [INFO] freeze rec encoder
freeze rec encoder
Loading Rec_model Done
Loading LLAMA
`torch_dtype` is deprecated! Use `dtype` instead!
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 85.84it/s]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at Vicuna-7b-delta-v0 and are newly initialized: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.s
elf_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.
self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.
self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24
.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.2
9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading LLAMA Done
Setting Lora
Setting Lora Done
type: <class 'int'> 10
2025-10-12 22:03:13,292 [INFO] !!!! freeze llama_proj...
Load 4 training prompts
Prompt List:
['#Question: A user has given high ratings to the following movies: <ItemTitleList>. Leverage the information to predict whether the user would enjoy the movie titled <TargetItemTitle>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A use
r has given high ratings to the following movies: <ItemTitleList>. Leverage the information to predict whether the user would enjoy the movie titled <TargetItemTitle>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high r
atings to the following movies: <ItemTitleList>. Leverage the information to predict whether the user would enjoy the movie titled <TargetItemTitle>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Leverage the information to predict whether the user would enjoy the movie titled <TargetItemTitle>? Answer with "Yes" or "No". \\n#Answer:']
answer token ids: pos: 3869 neg ids: 1939
Prompt Pos Example
#Question: A user has given high ratings to the following movies: <ItemTitleList>. Leverage the information to predict whether the user would enjoy the movie titled <TargetItemTitle>? Answer with "Yes" or "No". \n#Answer: Yes or No
2025-10-12 22:03:18,765 [INFO] Start training
2025-10-12 22:03:18,778 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2025-10-12 22:03:18,778 [INFO] Loaded 33891 records for train split from the dataset.
2025-10-12 22:03:18,779 [INFO] Loaded 5200 records for valid split from the dataset.
2025-10-12 22:03:18,779 [INFO] Loaded 7331 records for test split from the dataset.
llama_model.model.layers.0.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.0.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.0.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.0.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.1.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.1.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.1.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.1.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.2.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.2.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.2.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.2.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.3.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.3.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.3.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.3.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.4.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.4.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.4.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.4.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.5.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.5.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.5.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.5.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.6.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.6.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.6.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.6.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.7.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.7.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.7.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.7.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.8.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.8.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.8.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.8.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.9.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.9.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.9.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.9.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.10.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.10.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.10.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.10.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.11.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.11.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.11.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.11.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.12.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.12.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.12.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.12.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.13.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.13.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.13.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.13.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.14.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.14.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.14.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.14.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.15.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.15.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.15.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.15.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.16.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.16.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.16.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.16.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.17.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.17.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.17.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.17.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.18.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.18.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.18.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.18.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.19.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.19.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.19.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.19.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.20.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.20.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.20.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.20.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.21.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.21.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.21.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.21.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.22.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.22.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.22.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.22.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.23.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.23.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.23.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.23.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.24.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.24.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.24.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.24.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.25.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.25.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.25.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.25.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.26.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.26.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.26.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.26.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.27.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.27.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.27.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.27.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.28.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.28.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.28.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.28.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.29.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.29.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.29.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.29.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.30.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.30.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.30.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.30.self_attn.v_proj.lora_B.default.weight
llama_model.model.layers.31.self_attn.q_proj.lora_A.default.weight
llama_model.model.layers.31.self_attn.q_proj.lora_B.default.weight
llama_model.model.layers.31.self_attn.v_proj.lora_A.default.weight
llama_model.model.layers.31.self_attn.v_proj.lora_B.default.weight
2025-10-12 22:03:18,792 [INFO] number of trainable parameters: 4194304
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\runners\runner_base.py:153: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
2025-10-12 22:03:18,794 [INFO] Start training epoch 0, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
prompt example: <s>#Question: A user has given high ratings to the following movies: "Monty Python and the Holy Grail (1974)", "Unforgiven (1992)", "Snow White and the Seven Dwarfs (1937)", "Terminator, The (1984)", "Elephant Man, The (1980)",
"American Werewolf in London, An (1981)", "Courage Under Fire (1996)", "Mask of Zorro, The (1998)", "Predator (1987)", "Basic Instinct (1992)". Leverage the information to predict whether the user would enjoy the movie titled "Open Your Eyes (Abre los ojos) (1997)"? Answer with "Yes" or "No". \n#Answer:
#######prmpt decoded example:  </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s
> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> <s> # Question : A user has given high ratings to the followin
g mov ies : " Re qu iem for a Dream ( 2 0 0 0 )", " An na and the King ( 1 9 9 9 )", " In the Line of Fire ( 1 9 9 3 )", " Th el ma & Louise ( 1 9 9 1 )", " Single White Fem ale ( 1 9 9 2 ) ". Le verage the information to predict whether the user would enjoy the movie titled " G eta way , The ( 1 9 9 4 )" ? Answer with " Yes " or " No ". \ n # Answer :
Train: data epoch: [0]  [ 0/50]  eta: 1:48:31  lr: 0.000010  loss: 8.2090  time: 130.2300  data: 0.0000  max mem: 36430
Train: data epoch: [0]  [49/50]  eta: 0:02:09  lr: 0.000253  loss: 0.6749  time: 125.8739  data: 0.0000  max mem: 40427
Train: data epoch: [0] Total time: 1:48:09 (129.7892 s / it)
2025-10-12 23:51:28,253 [INFO] Averaged stats: lr: 0.000131  loss: 2.226366
2025-10-12 23:51:28,255 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:19  loss: 0.6684  acc: 0.3594  time: 9.0215  data: 0.0223  max mem: 40427
Evaluation  [16/82]  eta: 0:08:27  loss: 0.7424  acc: 0.2812  time: 7.6895  data: 0.0049  max mem: 40427
Evaluation  [32/82]  eta: 0:06:27  loss: 0.8003  acc: 0.1719  time: 7.8567  data: 0.0037  max mem: 40427
Evaluation  [48/82]  eta: 0:04:39  loss: 0.7596  acc: 0.2969  time: 8.6222  data: 0.0051  max mem: 40427
Evaluation  [64/82]  eta: 0:02:27  loss: 0.7604  acc: 0.3125  time: 7.9771  data: 0.0072  max mem: 40427
Evaluation  [80/82]  eta: 0:00:15  loss: 0.7459  acc: 0.2812  time: 7.2916  data: 0.0058  max mem: 40427
Evaluation  [81/82]  eta: 0:00:07  loss: 0.7054  acc: 0.2500  time: 6.9855  data: 0.0057  max mem: 40427
Evaluation Total time: 0:10:40 (7.8055 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.24692487716674805 uauc: 0.5245566583480298
2025-10-13 00:02:08,593 [INFO] Averaged stats: loss: 0.719017  acc: 0.293826 ***auc: 0.5255851060749528 ***uauc:(0.5245566583480298, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.63392857, 1.        , 0.52564103, 1.        ,
       0.6       , 0.6122449 , 0.17391304, 0.54761905, 0.5       ,
       0.45833333, 0.65546218, 0.77777778, 0.45711806, 0.375     ,
       0.63764881, 0.33333333, 0.16666667, 0.51111111, 0.74736842,
       0.51944444, 0.5       , 0.66666667, 1.        , 0.5       ,
       0.53314394, 0.47619048, 0.8       , 0.8125    , 0.2       ,
       0.72222222, 0.83333333, 1.        , 0.390625  , 0.5       ,
       0.46296296, 0.55952381, 0.42160279, 0.43055556, 0.        ,
       1.        , 1.        , 0.42      , 0.5       , 0.5       ,
       0.        , 0.        , 0.50694444, 1.        , 0.75      ,
       0.        , 0.64285714, 0.08333333, 0.51851852, 0.55      ,
       0.16666667, 0.375     , 0.375     , 0.5       , 0.59183673,
       0.25      , 0.8       , 0.61      , 0.43478261, 0.51648352,
       0.5       , 1.        , 0.83333333, 0.5       , 0.67391304,
       0.59090909, 0.33333333, 0.25      , 0.37012263, 0.50294118,
       0.5625    , 0.6031746 , 0.54736842, 0.58944099, 0.20833333,
       0.38888889, 0.62592593, 0.78571429, 0.52380952, 0.46527778,
       0.57614379, 0.54817708, 0.3627451 , 0.72807018, 0.65882353,
       0.69747899, 0.41269841, 0.75      , 0.75      , 0.        ,
       0.45833333, 1.        , 0.1       , 1.        , 0.375     ,
       0.66666667, 0.5       , 0.38095238, 0.33333333, 0.        ,
       0.37145969, 0.53125   , 0.83333333, 0.8       , 0.        ,
       0.58      , 0.71296296, 0.66666667, 0.55517827, 0.66666667,
       0.        , 0.55555556, 0.66666667, 0.        , 0.53333333,
       0.55744256, 0.9047619 , 0.44313725, 1.        , 0.41052632,
       0.89285714, 0.5       , 0.        , 0.55999066, 0.58695652,
       1.        , 0.75      , 0.66666667, 0.6       , 0.40972222,
       1.        , 0.        , 0.        , 0.69473684, 1.        ,
       0.62857143, 0.5       , 0.375     , 0.5625    , 0.5       ,
       0.67346939, 0.5       , 0.28888889, 0.60962567, 0.16666667,
       0.38888889, 0.        , 0.        , 0.63333333, 0.        ,
       0.58119658, 0.58761905, 0.3125    , 0.        , 0.66666667,
       1.        , 0.125     , 0.6875    , 1.        , 0.73137255,
       0.41575092, 0.51785714, 0.55952381, 0.44444444, 0.13333333,
       0.66544118, 0.53535354, 0.22222222, 0.44932475, 0.25454545,
       0.11111111, 0.5       , 0.54893384, 0.5       , 0.5       ,
       1.        , 0.58333333, 1.        , 0.45185185, 0.61363636,
       0.75      , 0.5       , 1.        , 0.8       , 0.56      ,
       0.33333333, 0.        , 0.54545455, 0.        , 0.448     ,
       0.33333333, 0.        , 0.66666667, 0.77777778, 0.57019928,
       0.48      , 0.3245614 , 0.43067227, 0.21428571, 0.38235294,
       0.35416667, 0.72222222, 0.75      , 0.66666667, 0.23076923,
       0.        , 0.66666667, 0.53333333, 0.8125    , 0.25      ,
       1.        , 0.35714286, 0.        , 0.        , 1.        ,
       0.49230769, 0.5       , 0.72727273, 0.85      , 1.        ,
       0.44444444, 0.66666667, 0.57142857, 1.        , 0.76923077,
       0.42708333, 0.16666667, 0.43597858, 0.33333333, 0.68181818,
       0.7       , 0.67226891, 0.66666667, 0.54166667]))
rank_0 auc: 0.5255851060749528
2025-10-13 00:02:08,617 [INFO] Saving checkpoint at epoch 0 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 00:02:26,102 [INFO] Start training
2025-10-13 00:02:26,132 [INFO] Start training epoch 1, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [1]  [ 0/50]  eta: 1:03:40  lr: 0.000010  loss: 0.7540  time: 76.4018  data: 0.0000  max mem: 40427
Train: data epoch: [1]  [49/50]  eta: 0:01:47  lr: 0.000253  loss: 0.8690  time: 109.7524  data: 0.0000  max mem: 40427
Train: data epoch: [1] Total time: 1:29:30 (107.4040 s / it)
2025-10-13 01:31:56,330 [INFO] Averaged stats: lr: 0.000131  loss: 0.708023
2025-10-13 01:31:56,334 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:42  loss: 0.8202  acc: 0.0000  time: 9.2995  data: 0.0097  max mem: 40427
Evaluation  [16/82]  eta: 0:08:21  loss: 0.7416  acc: 0.0000  time: 7.6028  data: 0.0042  max mem: 40427
Evaluation  [32/82]  eta: 0:06:31  loss: 0.6809  acc: 0.0000  time: 8.0427  data: 0.0046  max mem: 40427
Evaluation  [48/82]  eta: 0:04:32  loss: 0.7199  acc: 0.0000  time: 8.0780  data: 0.0048  max mem: 40427
Evaluation  [64/82]  eta: 0:02:23  loss: 0.7513  acc: 0.0000  time: 7.7309  data: 0.0047  max mem: 40427
Evaluation  [80/82]  eta: 0:00:15  loss: 0.8440  acc: 0.0000  time: 7.3885  data: 0.0054  max mem: 40427
Evaluation  [81/82]  eta: 0:00:07  loss: 0.7372  acc: 0.0000  time: 7.0859  data: 0.0053  max mem: 40427
Evaluation Total time: 0:10:26 (7.6415 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.22596287727355957 uauc: 0.5307475521839982
2025-10-13 01:42:23,184 [INFO] Averaged stats: loss: 0.788132  acc: 0.000000 ***auc: 0.5542635112170233 ***uauc:(0.5307475521839982, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.53571429, 1.        , 0.62820513, 1.        ,
       0.8       , 0.69387755, 0.26956522, 0.57142857, 0.375     ,
       0.41666667, 0.55042017, 0.66666667, 0.51623264, 0.25      ,
       0.65178571, 1.        , 0.33333333, 0.46666667, 0.72631579,
       0.47083333, 0.5       , 0.4       , 1.        , 0.5       ,
       0.48863636, 0.5952381 , 0.73333333, 0.625     , 0.33333333,
       0.65277778, 0.5       , 1.        , 0.375     , 0.5       ,
       0.58045977, 0.58928571, 0.51045296, 0.52777778, 0.        ,
       0.625     , 0.66666667, 0.50692308, 1.        , 0.8       ,
       0.        , 0.33333333, 0.49305556, 1.        , 0.75      ,
       1.        , 0.63392857, 0.25      , 0.53703704, 0.7       ,
       0.33333333, 0.375     , 0.5       , 0.51388889, 0.59693878,
       0.07142857, 0.6       , 0.62458333, 0.43167702, 0.6510989 ,
       0.        , 1.        , 0.66666667, 0.5       , 0.75652174,
       0.54545455, 0.33333333, 0.25      , 0.43338907, 0.57254902,
       0.625     , 0.67460317, 0.38947368, 0.57018634, 0.41666667,
       0.44444444, 0.68888889, 0.64285714, 0.51940035, 0.56944444,
       0.63235294, 0.55729167, 0.45588235, 0.72807018, 0.74117647,
       0.76470588, 0.44444444, 0.825     , 0.5       , 0.        ,
       0.5       , 0.        , 0.        , 1.        , 0.625     ,
       0.61111111, 0.5       , 0.19047619, 0.66666667, 0.        ,
       0.33769063, 0.59375   , 0.66666667, 0.6       , 0.        ,
       0.42      , 0.60185185, 0.66666667, 0.56281834, 0.53333333,
       0.75      , 0.61111111, 1.        , 0.        , 0.6       ,
       0.62104562, 0.66666667, 0.55294118, 1.        , 0.66666667,
       0.75      , 0.5       , 1.        , 0.59173669, 0.4673913 ,
       0.        , 0.375     , 0.66666667, 0.8       , 0.39583333,
       1.        , 1.        , 0.625     , 0.76842105, 0.        ,
       0.25714286, 1.        , 0.375     , 0.59895833, 0.16666667,
       0.67346939, 0.6       , 0.26666667, 0.62032086, 0.33333333,
       0.48611111, 0.        , 0.25      , 0.47222222, 0.        ,
       0.52136752, 0.56904762, 0.6875    , 0.25      , 1.        ,
       1.        , 0.25      , 0.375     , 0.        , 0.81568627,
       0.32539683, 0.46428571, 0.38095238, 0.44444444, 0.1       ,
       0.70588235, 0.37373737, 0.33333333, 0.48782027, 0.43636364,
       0.18518519, 0.25      , 0.55986878, 0.625     , 1.        ,
       1.        , 0.66666667, 0.33333333, 0.51111111, 0.65909091,
       0.5       , 0.45833333, 0.5       , 0.6       , 0.26      ,
       0.33333333, 0.        , 0.54545455, 0.        , 0.512     ,
       0.        , 0.        , 0.33333333, 0.66666667, 0.51041667,
       0.76      , 0.29824561, 0.48823529, 0.28571429, 0.55882353,
       0.54166667, 0.63888889, 1.        , 1.        , 0.32692308,
       0.        , 0.66666667, 0.46666667, 0.9375    , 1.        ,
       0.        , 0.32857143, 0.        , 0.        , 1.        ,
       0.54102564, 0.31632653, 0.90909091, 0.65      , 1.        ,
       0.44444444, 0.83333333, 0.71428571, 1.        , 0.60897436,
       0.55208333, 0.33333333, 0.4967543 , 0.33333333, 0.62121212,
       0.6       , 0.70168067, 0.6       , 0.60416667]))
rank_0 auc: 0.5542635112170233
2025-10-13 01:42:23,215 [INFO] Saving checkpoint at epoch 1 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 01:42:42,548 [INFO] Start training
2025-10-13 01:42:42,592 [INFO] Start training epoch 2, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [2]  [ 0/50]  eta: 1:01:31  lr: 0.000010  loss: 0.8348  time: 73.8349  data: 0.0000  max mem: 40427
Train: data epoch: [2]  [49/50]  eta: 0:01:43  lr: 0.000253  loss: 0.7260  time: 91.8532  data: 0.0000  max mem: 40427
Train: data epoch: [2] Total time: 1:26:08 (103.3762 s / it)
2025-10-13 03:08:51,401 [INFO] Averaged stats: lr: 0.000131  loss: 0.748730
2025-10-13 03:08:51,404 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:09:14  loss: 0.6861  acc: 0.0156  time: 6.7669  data: 0.0107  max mem: 40427
Evaluation  [16/82]  eta: 0:07:54  loss: 0.6945  acc: 0.0312  time: 7.1943  data: 0.0045  max mem: 40427
Evaluation  [32/82]  eta: 0:06:11  loss: 0.7012  acc: 0.0000  time: 7.7457  data: 0.0059  max mem: 40427
Evaluation  [48/82]  eta: 0:04:37  loss: 0.6683  acc: 0.0156  time: 9.0416  data: 0.0067  max mem: 40427
Evaluation  [64/82]  eta: 0:02:23  loss: 0.6841  acc: 0.0156  time: 7.4449  data: 0.0042  max mem: 40427
Evaluation  [80/82]  eta: 0:00:15  loss: 0.7091  acc: 0.0312  time: 7.4586  data: 0.0030  max mem: 40427
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6721  acc: 0.0000  time: 7.1702  data: 0.0029  max mem: 40427
Evaluation Total time: 0:10:30 (7.6941 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.21335434913635254 uauc: 0.5269074797235517
2025-10-13 03:19:22,558 [INFO] Averaged stats: loss: 0.691994  acc: 0.022485 ***auc: 0.5621414939509726 ***uauc:(0.5269074797235517, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.45535714, 1.        , 0.44871795, 1.        ,
       0.8       , 0.6122449 , 0.44347826, 0.35714286, 0.5       ,
       0.41666667, 0.59663866, 0.55555556, 0.60399306, 0.25      ,
       0.61607143, 0.66666667, 0.58333333, 0.43333333, 0.90526316,
       0.46666667, 0.25      , 0.66666667, 1.        , 0.5       ,
       0.48169192, 0.54761905, 0.8       , 0.5       , 0.46666667,
       0.5       , 0.41666667, 1.        , 0.453125  , 0.75      ,
       0.5083014 , 0.66269841, 0.55052265, 0.70833333, 0.5       ,
       0.5       , 1.        , 0.44153846, 1.        , 0.7       ,
       0.        , 0.        , 0.55555556, 1.        , 0.5       ,
       0.        , 0.63095238, 0.        , 0.53703704, 0.675     ,
       0.33333333, 0.5       , 0.625     , 0.51388889, 0.56632653,
       0.14285714, 0.6       , 0.68041667, 0.43478261, 0.6456044 ,
       0.        , 1.        , 1.        , 0.5       , 0.75217391,
       0.45454545, 0.        , 0.375     , 0.51421405, 0.57843137,
       0.625     , 0.72222222, 0.51052632, 0.64068323, 0.20833333,
       0.11111111, 0.4962963 , 0.64285714, 0.67548501, 0.54166667,
       0.65098039, 0.58984375, 0.42647059, 0.76315789, 0.81176471,
       0.63865546, 0.46031746, 0.8       , 0.125     , 0.5       ,
       0.47916667, 0.        , 0.1       , 0.66666667, 0.75      ,
       0.54166667, 0.5       , 0.23809524, 0.        , 0.        ,
       0.33115468, 0.390625  , 1.        , 0.65      , 1.        ,
       0.48      , 0.65740741, 0.5       , 0.55687606, 0.53333333,
       0.        , 0.52777778, 0.66666667, 0.        , 0.73333333,
       0.58041958, 0.57142857, 0.49411765, 0.        , 0.55087719,
       0.78571429, 0.33333333, 1.        , 0.67086835, 0.56521739,
       1.        , 0.41666667, 0.66666667, 0.8       , 0.44444444,
       1.        , 1.        , 0.375     , 0.8       , 0.        ,
       0.31428571, 1.        , 0.375     , 0.63368056, 0.66666667,
       0.6122449 , 0.7       , 0.42222222, 0.75935829, 0.33333333,
       0.51388889, 0.        , 0.25      , 0.44444444, 0.        ,
       0.52136752, 0.6047619 , 0.625     , 0.        , 1.        ,
       1.        , 0.125     , 0.4375    , 0.        , 0.8627451 ,
       0.39438339, 0.5       , 0.5       , 0.33333333, 0.13333333,
       0.61029412, 0.32323232, 0.22222222, 0.49753881, 0.52727273,
       0.07407407, 0.        , 0.5915801 , 0.5       , 1.        ,
       0.5       , 0.66666667, 1.        , 0.41481481, 0.68181818,
       0.5       , 0.55555556, 0.75      , 0.8       , 0.26      ,
       0.33333333, 0.        , 0.81818182, 0.        , 0.408     ,
       0.        , 0.        , 0.33333333, 0.77777778, 0.49426329,
       0.6       , 0.25730994, 0.46302521, 0.30952381, 0.55882353,
       0.3125    , 0.83333333, 1.        , 1.        , 0.25      ,
       0.        , 1.        , 0.4       , 0.65625   , 1.        ,
       0.        , 0.34285714, 1.        , 0.        , 1.        ,
       0.61538462, 0.5       , 0.87878788, 0.7       , 1.        ,
       0.66666667, 0.83333333, 0.85714286, 0.        , 0.76923077,
       0.52430556, 0.16666667, 0.50008114, 0.33333333, 0.39393939,
       0.5       , 0.62184874, 0.66666667, 0.68939394]))
rank_0 auc: 0.5621414939509726
2025-10-13 03:19:22,582 [INFO] Saving checkpoint at epoch 2 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 03:19:41,618 [INFO] Start training
2025-10-13 03:19:41,650 [INFO] Start training epoch 3, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [3]  [ 0/50]  eta: 1:00:35  lr: 0.000010  loss: 0.7145  time: 72.7171  data: 0.0000  max mem: 40427
Train: data epoch: [3]  [49/50]  eta: 0:01:49  lr: 0.000253  loss: 0.7966  time: 107.9834  data: 0.0000  max mem: 40524
Train: data epoch: [3] Total time: 1:31:23 (109.6783 s / it)
2025-10-13 04:51:05,566 [INFO] Averaged stats: lr: 0.000131  loss: 0.728144
2025-10-13 04:51:05,567 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:26  loss: 0.6719  acc: 0.0938  time: 9.1048  data: 0.0227  max mem: 40524
Evaluation  [16/82]  eta: 0:08:03  loss: 0.6678  acc: 0.0781  time: 7.3333  data: 0.0050  max mem: 40524
Evaluation  [32/82]  eta: 0:06:12  loss: 0.7088  acc: 0.0469  time: 7.6145  data: 0.0038  max mem: 40524
Evaluation  [48/82]  eta: 0:04:21  loss: 0.6952  acc: 0.0469  time: 7.9059  data: 0.0044  max mem: 40524
Evaluation  [64/82]  eta: 0:02:18  loss: 0.6816  acc: 0.0625  time: 7.6005  data: 0.0040  max mem: 40524
Evaluation  [80/82]  eta: 0:00:14  loss: 0.7042  acc: 0.0625  time: 7.3479  data: 0.0037  max mem: 40524
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6351  acc: 0.0000  time: 7.0596  data: 0.0037  max mem: 40524
Evaluation Total time: 0:10:07 (7.4120 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.21216487884521484 uauc: 0.535609016263893
2025-10-13 05:01:13,602 [INFO] Averaged stats: loss: 0.686298  acc: 0.067073 ***auc: 0.5587820367857961 ***uauc:(0.535609016263893, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121,
 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264
, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 44
4, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 6
28, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.50892857, 0.16666667, 0.47435897, 0.5       ,
       0.8       , 0.30612245, 0.43478261, 0.32142857, 0.5       ,
       0.39583333, 0.71428571, 0.44444444, 0.57821181, 0.25      ,
       0.43005952, 0.66666667, 0.33333333, 0.42222222, 0.71578947,
       0.40555556, 0.25      , 0.4       , 1.        , 0.25      ,
       0.50157828, 0.71428571, 0.73333333, 0.4375    , 1.        ,
       0.43055556, 0.5       , 1.        , 0.484375  , 0.75      ,
       0.4789272 , 0.69444444, 0.54181185, 0.72222222, 1.        ,
       0.5       , 0.        , 0.36615385, 1.        , 0.        ,
       0.5       , 0.        , 0.52777778, 1.        , 0.25      ,
       1.        , 0.38095238, 0.16666667, 0.60493827, 0.65      ,
       0.        , 0.25      , 0.64285714, 0.56944444, 0.57653061,
       0.32142857, 0.7       , 0.69583333, 0.41925466, 0.68131868,
       0.        , 1.        , 0.66666667, 0.5       , 0.74782609,
       0.71212121, 0.        , 0.75      , 0.57915273, 0.47058824,
       0.4375    , 0.74801587, 0.43684211, 0.54347826, 0.5       ,
       0.61111111, 0.47037037, 0.5       , 0.76234568, 0.50694444,
       0.66764706, 0.54817708, 0.44607843, 0.74561404, 0.81176471,
       0.47058824, 0.34920635, 0.775     , 0.125     , 1.        ,
       0.5       , 0.        , 0.4       , 0.33333333, 0.5       ,
       0.58333333, 0.5       , 0.47619048, 0.33333333, 0.5       ,
       0.37363834, 0.421875  , 0.83333333, 0.45      , 0.        ,
       0.58      , 0.74074074, 0.5       , 0.56876061, 0.26666667,
       0.        , 0.69444444, 1.        , 0.        , 0.6       ,
       0.64452214, 0.47619048, 0.41960784, 1.        , 0.45964912,
       0.78571429, 0.33333333, 1.        , 0.67436975, 0.70652174,
       0.        , 0.4375    , 0.66666667, 0.2       , 0.52083333,
       1.        , 1.        , 0.625     , 0.69473684, 1.        ,
       0.65714286, 0.5       , 0.        , 0.60763889, 0.83333333,
       0.65306122, 0.8       , 0.44444444, 0.70053476, 0.33333333,
       0.46527778, 0.        , 0.75      , 0.35555556, 0.        ,
       0.57264957, 0.55428571, 0.625     , 0.        , 1.        ,
       1.        , 0.25      , 0.75      , 1.        , 0.68235294,
       0.4993895 , 0.41071429, 0.69047619, 0.33333333, 0.2       ,
       0.64705882, 0.36363636, 0.33333333, 0.49835921, 0.67272727,
       0.22222222, 0.5       , 0.55713505, 0.5       , 1.        ,
       0.5       , 0.66666667, 1.        , 0.39259259, 0.29545455,
       0.75      , 0.54166667, 0.75      , 0.6       , 0.44      ,
       0.33333333, 0.        , 0.63636364, 0.        , 0.412     ,
       0.        , 0.33333333, 0.33333333, 0.77777778, 0.522343  ,
       0.76      , 0.23391813, 0.41092437, 0.33333333, 0.41176471,
       0.33333333, 0.80555556, 0.5       , 1.        , 0.25      ,
       0.        , 0.66666667, 0.73333333, 0.6875    , 1.        ,
       1.        , 0.4       , 1.        , 0.        , 1.        ,
       0.61282051, 0.62244898, 0.81818182, 0.8       , 1.        ,
       0.72222222, 0.16666667, 0.71428571, 1.        , 0.57692308,
       0.58333333, 0.25      , 0.4654333 , 0.33333333, 0.39393939,
       0.33333333, 0.66386555, 0.8       , 0.63636364]))
rank_0 auc: 0.5587820367857961
2025-10-13 05:01:13,630 [INFO] Start training
2025-10-13 05:01:13,661 [INFO] Start training epoch 4, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [4]  [ 0/50]  eta: 1:03:16  lr: 0.000999  loss: 0.6473  time: 75.9352  data: 0.0000  max mem: 40524
Train: data epoch: [4]  [49/50]  eta: 0:01:50  lr: 0.000999  loss: 0.6528  time: 101.7770  data: 0.0000  max mem: 42446
Train: data epoch: [4] Total time: 1:31:49 (110.1835 s / it)
2025-10-13 06:33:02,837 [INFO] Averaged stats: lr: 0.000999  loss: 0.967454
2025-10-13 06:33:02,839 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:12  loss: 0.7195  acc: 0.5312  time: 6.0106  data: 0.0411  max mem: 42446
Evaluation  [16/82]  eta: 0:07:53  loss: 0.7670  acc: 0.5000  time: 7.1738  data: 0.0063  max mem: 42446
Evaluation  [32/82]  eta: 0:06:33  loss: 0.8678  acc: 0.3750  time: 8.4305  data: 0.0046  max mem: 42446
Evaluation  [48/82]  eta: 0:05:26  loss: 0.7996  acc: 0.4375  time: 11.8076  data: 0.0045  max mem: 42446
Evaluation  [64/82]  eta: 0:02:43  loss: 0.7749  acc: 0.4531  time: 7.4997  data: 0.0056  max mem: 42446
Evaluation  [80/82]  eta: 0:00:17  loss: 0.7197  acc: 0.5625  time: 8.4951  data: 0.0058  max mem: 42446
Evaluation  [81/82]  eta: 0:00:08  loss: 0.7544  acc: 0.5000  time: 8.1986  data: 0.0058  max mem: 42446
Evaluation Total time: 0:12:04 (8.8340 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.22843289375305176 uauc: 0.5312576483084426
2025-10-13 06:45:07,496 [INFO] Averaged stats: loss: 0.740594  acc: 0.517721 ***auc: 0.5516778057109732 ***uauc:(0.5312576483084426, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.79910714, 1.        , 0.58974359, 0.66666667,
       1.        , 0.46938776, 0.73043478, 0.3452381 , 0.125     ,
       0.45833333, 0.58403361, 0.50793651, 0.66310764, 0.75      ,
       0.4375    , 0.        , 0.5       , 0.8       , 0.67368421,
       0.56527778, 1.        , 0.6       , 0.        , 0.        ,
       0.54135101, 0.73809524, 0.86666667, 0.25      , 0.46666667,
       0.38888889, 0.5       , 0.        , 0.734375  , 0.75      ,
       0.59163474, 0.72222222, 0.43641115, 0.72222222, 0.5       ,
       0.75      , 0.        , 0.44730769, 1.        , 0.4       ,
       1.        , 0.        , 0.4375    , 0.5       , 0.25      ,
       0.        , 0.49404762, 0.58333333, 0.58641975, 0.85      ,
       0.5       , 0.5       , 0.60714286, 0.52777778, 0.39285714,
       0.60714286, 0.5       , 0.64541667, 0.40838509, 0.73076923,
       0.5       , 0.5       , 0.5       , 0.5       , 0.49565217,
       0.34848485, 0.        , 0.        , 0.47073579, 0.55490196,
       0.5       , 0.71626984, 0.49473684, 0.50993789, 0.45833333,
       0.27777778, 0.51111111, 0.5       , 0.7989418 , 0.61458333,
       0.53496732, 0.45638021, 0.36764706, 0.5877193 , 0.4       ,
       0.71428571, 0.25396825, 0.75      , 1.        , 1.        ,
       0.54166667, 0.        , 0.4       , 0.33333333, 0.875     ,
       0.73611111, 1.        , 0.47619048, 0.77777778, 0.        ,
       0.41230937, 0.328125  , 0.83333333, 0.65      , 1.        ,
       0.64      , 0.60648148, 0.5       , 0.62054329, 1.        ,
       0.75      , 0.63888889, 0.33333333, 0.        , 0.4       ,
       0.50915751, 0.38095238, 0.53627451, 0.        , 0.39298246,
       0.85714286, 0.33333333, 0.66666667, 0.60714286, 0.60869565,
       1.        , 0.66666667, 0.33333333, 1.        , 0.58333333,
       0.5       , 1.        , 0.125     , 0.85263158, 0.        ,
       0.38571429, 1.        , 0.5       , 0.63628472, 1.        ,
       0.51020408, 0.5       , 0.44444444, 0.68181818, 0.66666667,
       0.56944444, 0.        , 0.25      , 0.32222222, 0.        ,
       0.78632479, 0.48809524, 0.4375    , 0.66666667, 0.66666667,
       1.        , 0.66666667, 0.875     , 1.        , 0.55294118,
       0.51953602, 0.76785714, 0.67857143, 1.        , 0.26666667,
       0.64338235, 0.53535354, 0.36111111, 0.5280828 , 0.37272727,
       0.48148148, 0.25      , 0.43685074, 0.375     , 0.25      ,
       0.5       , 0.41666667, 0.33333333, 0.52592593, 0.11363636,
       0.75      , 0.36111111, 0.75      , 0.5       , 0.48      ,
       0.66666667, 0.        , 0.36363636, 1.        , 0.474     ,
       0.66666667, 1.        , 1.        , 0.88888889, 0.59359903,
       0.52      , 0.42397661, 0.63109244, 0.26190476, 0.44117647,
       0.72916667, 0.5       , 0.25      , 0.        , 0.73076923,
       0.5       , 0.66666667, 0.73333333, 0.5625    , 0.        ,
       0.        , 0.51428571, 1.        , 0.        , 1.        ,
       0.57948718, 0.68367347, 0.87878788, 0.75      , 0.        ,
       0.72222222, 0.16666667, 0.57142857, 0.        , 0.57692308,
       0.35243056, 0.08333333, 0.54284323, 1.        , 0.3030303 ,
       0.33333333, 0.77310924, 0.73333333, 0.68276515]))
rank_0 auc: 0.5516778057109732
2025-10-13 06:45:07,525 [INFO] Start training
2025-10-13 06:45:07,554 [INFO] Start training epoch 5, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [5]  [ 0/50]  eta: 1:12:26  lr: 0.000999  loss: 0.6954  time: 86.9370  data: 0.0000  max mem: 42446
Train: data epoch: [5]  [49/50]  eta: 0:01:44  lr: 0.000998  loss: 0.7431  time: 90.2016  data: 0.0000  max mem: 42623
Train: data epoch: [5] Total time: 1:26:50 (104.2107 s / it)
2025-10-13 08:11:58,087 [INFO] Averaged stats: lr: 0.000998  loss: 0.724416
2025-10-13 08:11:58,089 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:24  loss: 0.7256  acc: 0.0000  time: 5.4265  data: 0.0151  max mem: 42623
Evaluation  [16/82]  eta: 0:07:02  loss: 0.6964  acc: 0.0000  time: 6.4026  data: 0.0036  max mem: 42623
Evaluation  [32/82]  eta: 0:05:26  loss: 0.6627  acc: 0.0000  time: 6.7299  data: 0.0025  max mem: 42623
Evaluation  [48/82]  eta: 0:03:51  loss: 0.6642  acc: 0.0000  time: 7.0601  data: 0.0026  max mem: 42623
Evaluation  [64/82]  eta: 0:02:03  loss: 0.6728  acc: 0.0000  time: 6.9168  data: 0.0027  max mem: 42623
Evaluation  [80/82]  eta: 0:00:13  loss: 0.6976  acc: 0.0156  time: 6.6831  data: 0.0028  max mem: 42623
Evaluation  [81/82]  eta: 0:00:06  loss: 0.6900  acc: 0.0000  time: 6.3890  data: 0.0026  max mem: 42623
Evaluation Total time: 0:09:02 (6.6214 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07558250427246094 uauc: 0.5379841970440642
2025-10-13 08:21:01,141 [INFO] Averaged stats: loss: 0.703448  acc: 0.001905 ***auc: 0.5545825297239573 ***uauc:(0.5379841970440642, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.53571429, 0.5       , 0.42948718, 0.83333333,
       1.        , 0.30612245, 0.64347826, 0.41666667, 0.375     ,
       0.625     , 0.61344538, 0.76190476, 0.54401042, 1.        ,
       0.59375   , 0.        , 0.5       , 0.72222222, 0.73684211,
       0.54722222, 1.        , 0.06666667, 0.        , 0.5       ,
       0.4760101 , 0.80952381, 0.86666667, 0.125     , 0.2       ,
       0.45833333, 0.16666667, 0.        , 0.765625  , 0.5       ,
       0.62260536, 0.47619048, 0.44947735, 0.81944444, 0.5       ,
       0.75      , 0.66666667, 0.52461538, 1.        , 0.6       ,
       0.75      , 0.        , 0.47916667, 0.5       , 0.25      ,
       1.        , 0.70238095, 0.41666667, 0.5617284 , 0.825     ,
       0.5       , 0.375     , 0.32142857, 0.23611111, 0.39285714,
       0.57142857, 0.7       , 0.60583333, 0.36490683, 0.80769231,
       0.        , 0.25      , 0.66666667, 1.        , 0.45652174,
       0.57575758, 0.        , 0.        , 0.59782609, 0.58137255,
       0.25      , 0.85416667, 0.47368421, 0.62484472, 0.125     ,
       0.55555556, 0.4037037 , 0.5       , 0.86948854, 0.81944444,
       0.52941176, 0.4921875 , 0.35294118, 0.43859649, 0.58823529,
       0.61344538, 0.3968254 , 0.6       , 1.        , 1.        ,
       0.58333333, 0.        , 0.5       , 0.33333333, 0.875     ,
       0.77777778, 1.        , 0.42857143, 0.66666667, 0.        ,
       0.32352941, 0.515625  , 1.        , 0.6       , 1.        ,
       0.38      , 0.71296296, 0.5       , 0.57385399, 0.33333333,
       1.        , 0.47222222, 0.        , 1.        , 0.46666667,
       0.57292707, 0.0952381 , 0.60196078, 0.        , 0.32631579,
       0.67857143, 0.33333333, 0.5       , 0.4423436 , 0.69565217,
       0.        , 0.85416667, 0.33333333, 0.8       , 0.40972222,
       0.5       , 1.        , 0.25      , 0.76842105, 0.        ,
       0.34285714, 0.        , 0.25      , 0.62326389, 1.        ,
       0.42857143, 0.2       , 0.6       , 0.47593583, 0.83333333,
       0.5       , 0.        , 0.        , 0.44444444, 1.        ,
       0.39316239, 0.56857143, 0.6875    , 0.33333333, 0.        ,
       1.        , 0.83333333, 0.75      , 1.        , 0.74901961,
       0.48229548, 0.5       , 0.52380952, 1.        , 0.46666667,
       0.55882353, 0.34343434, 1.        , 0.50107283, 0.16363636,
       0.40740741, 0.75      , 0.54510662, 0.625     , 0.        ,
       1.        , 0.33333333, 1.        , 0.37037037, 0.72727273,
       1.        , 0.5       , 1.        , 0.5       , 0.5       ,
       1.        , 0.        , 0.27272727, 0.5       , 0.42      ,
       0.66666667, 1.        , 1.        , 0.88888889, 0.55510266,
       0.72      , 0.44736842, 0.58991597, 0.76190476, 0.82352941,
       0.4375    , 0.47222222, 0.25      , 0.66666667, 0.38461538,
       0.5       , 1.        , 0.46666667, 0.875     , 0.75      ,
       0.        , 0.64285714, 0.        , 1.        , 1.        ,
       0.60769231, 0.7244898 , 0.51515152, 0.575     , 0.        ,
       0.58333333, 0.        , 0.28571429, 0.        , 0.69230769,
       0.43402778, 0.41666667, 0.46291788, 0.66666667, 0.59090909,
       0.58333333, 0.90756303, 0.73333333, 0.6875    ]))
rank_0 auc: 0.5545825297239573
2025-10-13 08:21:01,151 [INFO] Start training
2025-10-13 08:21:01,163 [INFO] Start training epoch 6, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [6]  [ 0/50]  eta: 1:29:54  lr: 0.000998  loss: 0.6632  time: 107.8823  data: 0.0015  max mem: 42623
Train: data epoch: [6]  [49/50]  eta: 0:01:33  lr: 0.000997  loss: 0.7543  time: 93.7582  data: 0.0000  max mem: 42623
Train: data epoch: [6] Total time: 1:17:36 (93.1356 s / it)
2025-10-13 09:38:37,945 [INFO] Averaged stats: lr: 0.000998  loss: 0.722580
2025-10-13 09:38:37,948 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:19  loss: 0.7050  acc: 0.0000  time: 5.3615  data: 0.0106  max mem: 42623
Evaluation  [16/82]  eta: 0:06:47  loss: 0.6934  acc: 0.0000  time: 6.1680  data: 0.0033  max mem: 42623
Evaluation  [32/82]  eta: 0:05:11  loss: 0.6758  acc: 0.0000  time: 6.4109  data: 0.0028  max mem: 42623
Evaluation  [48/82]  eta: 0:03:46  loss: 0.6844  acc: 0.0000  time: 7.1012  data: 0.0029  max mem: 42623
Evaluation  [64/82]  eta: 0:02:01  loss: 0.6792  acc: 0.0000  time: 6.9283  data: 0.0030  max mem: 42623
Evaluation  [80/82]  eta: 0:00:13  loss: 0.6958  acc: 0.0156  time: 6.8124  data: 0.0029  max mem: 42623
Evaluation  [81/82]  eta: 0:00:06  loss: 0.6735  acc: 0.0000  time: 6.5286  data: 0.0028  max mem: 42623
Evaluation Total time: 0:09:00 (6.5895 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0743112564086914 uauc: 0.5616847945934815
2025-10-13 09:47:38,380 [INFO] Averaged stats: loss: 0.696183  acc: 0.000191 ***auc: 0.5587005189080643 ***uauc:(0.5616847945934815, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.58928571, 0.33333333, 0.3525641 , 1.        ,
       0.8       , 0.24489796, 0.73913043, 0.42857143, 0.875     ,
       0.58333333, 0.69327731, 0.66666667, 0.64782986, 1.        ,
       0.49107143, 0.33333333, 0.5       , 0.34444444, 0.64210526,
       0.56666667, 0.5       , 0.53333333, 0.        , 0.        ,
       0.42487374, 0.76190476, 0.86666667, 0.125     , 0.33333333,
       0.30555556, 0.75      , 0.        , 0.609375  , 0.5       ,
       0.60983397, 0.46031746, 0.68989547, 0.75      , 1.        ,
       0.5       , 0.66666667, 0.42692308, 1.        , 0.3       ,
       0.75      , 0.        , 0.39583333, 0.75      , 0.5       ,
       1.        , 0.48214286, 0.58333333, 0.64814815, 0.7       ,
       0.5       , 0.625     , 0.32142857, 0.33333333, 0.71938776,
       0.53571429, 0.4       , 0.71666667, 0.39130435, 0.76648352,
       0.5       , 0.75      , 0.66666667, 1.        , 0.45217391,
       0.65151515, 0.        , 0.        , 0.63768116, 0.54705882,
       0.625     , 0.88492063, 0.42105263, 0.58509317, 0.66666667,
       0.33333333, 0.57407407, 0.78571429, 0.81481481, 0.83333333,
       0.53921569, 0.47135417, 0.6372549 , 0.45614035, 0.6       ,
       0.53781513, 0.3015873 , 0.75      , 0.375     , 1.        ,
       0.625     , 0.        , 0.        , 0.        , 0.625     ,
       0.68055556, 1.        , 0.42857143, 1.        , 0.5       ,
       0.41394336, 0.515625  , 0.66666667, 0.55      , 1.        ,
       0.46      , 0.69907407, 0.16666667, 0.41256367, 0.73333333,
       0.75      , 0.40277778, 0.66666667, 1.        , 0.46666667,
       0.57176157, 0.19047619, 0.6372549 , 0.        , 0.33333333,
       0.67857143, 0.5       , 0.66666667, 0.49813259, 0.70652174,
       1.        , 0.45833333, 0.66666667, 0.6       , 0.59027778,
       0.5       , 1.        , 0.5       , 0.71052632, 0.        ,
       0.57142857, 0.5       , 0.5       , 0.62673611, 1.        ,
       0.65306122, 0.6       , 0.53333333, 0.59358289, 0.5       ,
       0.625     , 0.        , 0.        , 0.65555556, 1.        ,
       0.41880342, 0.53809524, 0.6875    , 0.58333333, 0.66666667,
       1.        , 0.79166667, 0.75      , 0.5       , 0.74509804,
       0.47741148, 0.53571429, 0.53571429, 0.88888889, 0.13333333,
       0.72794118, 0.48484848, 0.77777778, 0.56626278, 0.32727273,
       0.51851852, 0.5       , 0.51476217, 0.75      , 0.5       ,
       1.        , 0.5       , 1.        , 0.34074074, 0.34090909,
       0.75      , 0.51388889, 1.        , 0.4       , 0.44      ,
       0.66666667, 0.        , 0.54545455, 0.5       , 0.532     ,
       0.66666667, 0.66666667, 1.        , 0.55555556, 0.57457729,
       0.2       , 0.38304094, 0.47647059, 0.54761905, 0.67647059,
       0.58333333, 1.        , 0.25      , 0.33333333, 0.53846154,
       0.5       , 0.83333333, 0.33333333, 0.5625    , 0.75      ,
       0.        , 0.57142857, 1.        , 0.        , 1.        ,
       0.66666667, 0.63265306, 0.66666667, 0.6       , 0.        ,
       0.88888889, 0.33333333, 0.57142857, 1.        , 0.58974359,
       0.61805556, 0.5       , 0.46105161, 0.66666667, 0.45454545,
       0.86666667, 0.64705882, 0.6       , 0.63541667]))
rank_0 auc: 0.5587005189080643
2025-10-13 09:47:38,391 [INFO] Start training
2025-10-13 09:47:38,403 [INFO] Start training epoch 7, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [7]  [ 0/50]  eta: 1:30:45  lr: 0.000997  loss: 0.6526  time: 108.9182  data: 0.0000  max mem: 42623
Train: data epoch: [7]  [49/50]  eta: 0:01:32  lr: 0.000996  loss: 0.7481  time: 88.2695  data: 0.0000  max mem: 42623
Train: data epoch: [7] Total time: 1:17:01 (92.4346 s / it)
2025-10-13 11:04:40,132 [INFO] Averaged stats: lr: 0.000997  loss: 0.683720
2025-10-13 11:04:40,133 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:12  loss: 0.6961  acc: 0.1094  time: 5.2737  data: 0.0130  max mem: 42623
Evaluation  [16/82]  eta: 0:06:55  loss: 0.7054  acc: 0.0781  time: 6.2935  data: 0.0033  max mem: 42623
Evaluation  [32/82]  eta: 0:05:19  loss: 0.7302  acc: 0.1250  time: 6.5754  data: 0.0027  max mem: 42623
Evaluation  [48/82]  eta: 0:03:49  loss: 0.7055  acc: 0.1719  time: 7.0716  data: 0.0029  max mem: 42623
Evaluation  [64/82]  eta: 0:02:02  loss: 0.6893  acc: 0.0938  time: 6.8598  data: 0.0028  max mem: 42623
Evaluation  [80/82]  eta: 0:00:13  loss: 0.6653  acc: 0.2188  time: 6.7568  data: 0.0025  max mem: 42623
Evaluation  [81/82]  eta: 0:00:06  loss: 0.6678  acc: 0.1875  time: 6.4750  data: 0.0025  max mem: 42623
Evaluation Total time: 0:09:05 (6.6486 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07831692695617676 uauc: 0.5624544576565493
2025-10-13 11:13:45,415 [INFO] Averaged stats: loss: 0.681076  acc: 0.164634 ***auc: 0.5991237347838864 ***uauc:(0.5624544576565493, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.55357143, 0.83333333, 0.28846154, 1.        ,
       0.4       , 0.26530612, 0.72173913, 0.51190476, 0.625     ,
       0.5625    , 0.72268908, 0.53968254, 0.78307292, 0.        ,
       0.55654762, 0.33333333, 0.66666667, 0.72222222, 0.75789474,
       0.65      , 0.5       , 0.53333333, 1.        , 0.        ,
       0.59532828, 0.80952381, 0.66666667, 0.4375    , 0.26666667,
       0.29166667, 0.58333333, 0.        , 0.765625  , 0.5       ,
       0.72190294, 0.59126984, 0.63501742, 0.59722222, 1.        ,
       0.25      , 0.        , 0.51769231, 0.        , 0.5       ,
       0.5       , 0.        , 0.5625    , 1.        , 0.        ,
       1.        , 0.67857143, 0.58333333, 0.80246914, 0.8       ,
       0.83333333, 0.75      , 0.53571429, 0.36111111, 0.50510204,
       0.42857143, 0.8       , 0.65708333, 0.51552795, 0.83791209,
       0.5       , 0.75      , 0.        , 1.        , 0.50869565,
       0.6969697 , 0.        , 0.        , 0.56187291, 0.5754902 ,
       0.1875    , 0.91269841, 0.55789474, 0.65372671, 0.875     ,
       0.55555556, 0.55925926, 0.85714286, 0.87477954, 0.95833333,
       0.50816993, 0.546875  , 0.64215686, 0.56140351, 0.78823529,
       0.78991597, 0.25396825, 0.775     , 0.875     , 0.5       ,
       0.66666667, 0.        , 1.        , 0.        , 0.875     ,
       0.65277778, 0.5       , 0.61904762, 1.        , 0.5       ,
       0.44008715, 0.546875  , 0.66666667, 0.4       , 1.        ,
       0.36      , 0.68981481, 0.16666667, 0.48726655, 0.13333333,
       1.        , 0.22222222, 0.66666667, 1.        , 0.86666667,
       0.62604063, 0.76190476, 0.61960784, 0.        , 0.48421053,
       1.        , 0.5       , 0.33333333, 0.4344071 , 0.80434783,
       0.        , 0.27083333, 0.33333333, 0.4       , 0.64930556,
       0.5       , 1.        , 0.375     , 0.63157895, 0.        ,
       0.11428571, 0.        , 0.375     , 0.60416667, 0.83333333,
       0.57142857, 0.6       , 0.51111111, 0.70053476, 0.5       ,
       0.59722222, 0.        , 0.5       , 0.61111111, 1.        ,
       0.53846154, 0.4547619 , 0.8125    , 0.58333333, 1.        ,
       1.        , 0.75      , 0.6875    , 0.5       , 0.74901961,
       0.46520147, 0.30357143, 0.39285714, 0.77777778, 0.46666667,
       0.51470588, 0.48484848, 0.5       , 0.58670958, 0.21818182,
       0.59259259, 0.5       , 0.59923455, 0.5       , 0.5       ,
       0.5       , 0.33333333, 1.        , 0.45185185, 0.65909091,
       1.        , 0.33333333, 0.25      , 0.4       , 0.76      ,
       0.33333333, 0.        , 0.45454545, 0.        , 0.572     ,
       0.66666667, 0.66666667, 1.        , 0.77777778, 0.69489734,
       0.56      , 0.45614035, 0.48151261, 0.66666667, 0.64705882,
       0.70833333, 0.61111111, 0.5       , 0.66666667, 0.44230769,
       0.5       , 0.66666667, 0.53333333, 0.5625    , 0.        ,
       0.        , 0.57142857, 1.        , 1.        , 1.        ,
       0.71538462, 0.65306122, 0.75757576, 0.85      , 1.        ,
       0.88888889, 0.66666667, 0.28571429, 0.        , 0.57692308,
       0.73611111, 0.66666667, 0.51087309, 0.66666667, 0.74242424,
       0.86666667, 0.74789916, 0.46666667, 0.78787879]))
rank_0 auc: 0.5991237347838864
2025-10-13 11:13:45,425 [INFO] Saving checkpoint at epoch 7 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 11:13:56,705 [INFO] Start training
2025-10-13 11:13:56,717 [INFO] Start training epoch 8, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [8]  [ 0/50]  eta: 1:25:23  lr: 0.000996  loss: 0.6050  time: 102.4680  data: 0.0000  max mem: 42623
Train: data epoch: [8]  [49/50]  eta: 0:01:55  lr: 0.000995  loss: 0.6382  time: 125.6666  data: 0.0000  max mem: 42623
Train: data epoch: [8] Total time: 1:36:06 (115.3257 s / it)
2025-10-13 12:50:03,004 [INFO] Averaged stats: lr: 0.000996  loss: 0.665924
2025-10-13 12:50:03,006 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:09:04  loss: 0.6769  acc: 0.3438  time: 6.6430  data: 0.0214  max mem: 42623
Evaluation  [16/82]  eta: 0:09:30  loss: 0.7289  acc: 0.2969  time: 8.6510  data: 0.0048  max mem: 42623
Evaluation  [32/82]  eta: 0:06:25  loss: 0.7567  acc: 0.1719  time: 8.1257  data: 0.0041  max mem: 42623
Evaluation  [48/82]  eta: 0:04:22  loss: 0.7661  acc: 0.2500  time: 7.3062  data: 0.0051  max mem: 42623
Evaluation  [64/82]  eta: 0:02:17  loss: 0.6926  acc: 0.2031  time: 7.2254  data: 0.0054  max mem: 42623
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6430  acc: 0.3125  time: 6.8417  data: 0.0036  max mem: 42623
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6729  acc: 0.3125  time: 6.5450  data: 0.0035  max mem: 42623
Evaluation Total time: 0:09:55 (7.2636 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.19307374954223633 uauc: 0.6175494800287534
2025-10-13 12:59:58,844 [INFO] Averaged stats: loss: 0.690052  acc: 0.291159 ***auc: 0.6182189635528965 ***uauc:(0.6175494800287534, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.5625    , 0.5       , 0.55128205, 1.        ,
       0.2       , 0.46938776, 0.8826087 , 0.58333333, 0.625     ,
       0.5625    , 0.7605042 , 0.50793651, 0.81631944, 0.25      ,
       0.4702381 , 0.66666667, 0.66666667, 0.56666667, 0.82105263,
       0.6       , 0.75      , 0.46666667, 1.        , 0.25      ,
       0.62405303, 0.83333333, 0.8       , 0.375     , 0.26666667,
       0.52777778, 0.5       , 1.        , 0.8125    , 0.5       ,
       0.75925926, 0.70238095, 0.69686411, 0.69444444, 1.        ,
       0.5       , 0.33333333, 0.49615385, 1.        , 0.        ,
       0.75      , 0.        , 0.61805556, 1.        , 0.75      ,
       1.        , 0.76190476, 0.5       , 0.88271605, 0.825     ,
       0.5       , 0.625     , 0.5       , 0.48611111, 0.48469388,
       0.39285714, 0.8       , 0.68      , 0.52795031, 0.76648352,
       0.5       , 0.75      , 0.5       , 1.        , 0.44130435,
       0.83333333, 0.        , 0.        , 0.67892977, 0.50784314,
       0.        , 0.8968254 , 0.48947368, 0.72546584, 0.875     ,
       0.5       , 0.56296296, 0.        , 0.89329806, 0.9375    ,
       0.60816993, 0.58138021, 0.5       , 0.5877193 , 0.78823529,
       0.68067227, 0.26984127, 0.875     , 0.75      , 1.        ,
       0.625     , 0.        , 0.3       , 0.        , 0.375     ,
       0.59722222, 0.5       , 0.80952381, 1.        , 1.        ,
       0.45152505, 0.84375   , 1.        , 0.3       , 1.        ,
       0.36      , 0.63888889, 0.16666667, 0.50933786, 0.53333333,
       1.        , 0.22222222, 0.66666667, 1.        , 0.8       ,
       0.61088911, 0.52380952, 0.66176471, 0.        , 0.64210526,
       1.        , 0.5       , 0.        , 0.51680672, 0.68478261,
       1.        , 0.1875    , 0.33333333, 0.4       , 0.75      ,
       1.        , 0.5       , 0.875     , 0.71578947, 1.        ,
       0.51428571, 0.5       , 0.25      , 0.65625   , 1.        ,
       0.63265306, 0.8       , 0.51111111, 0.73796791, 0.66666667,
       0.41666667, 1.        , 0.75      , 0.48888889, 1.        ,
       0.38461538, 0.5447619 , 0.8125    , 0.58333333, 1.        ,
       1.        , 0.91666667, 0.625     , 1.        , 0.73333333,
       0.49450549, 0.44642857, 0.60714286, 0.77777778, 0.23333333,
       0.64705882, 0.53535354, 0.77777778, 0.63618579, 0.27272727,
       0.48148148, 0.5       , 0.56205577, 0.25      , 0.5       ,
       0.        , 0.58333333, 0.66666667, 0.52592593, 0.5       ,
       0.75      , 0.34722222, 1.        , 0.4       , 0.96      ,
       1.        , 0.        , 0.45454545, 1.        , 0.472     ,
       0.66666667, 0.66666667, 1.        , 1.        , 0.70471014,
       0.48      , 0.36549708, 0.45798319, 0.69047619, 0.32352941,
       0.60416667, 0.63888889, 0.5       , 0.66666667, 0.61538462,
       0.5       , 1.        , 0.46666667, 0.46875   , 0.5       ,
       0.5       , 0.51428571, 1.        , 1.        , 1.        ,
       0.6025641 , 0.7244898 , 0.60606061, 0.7       , 1.        ,
       0.72222222, 0.66666667, 0.        , 1.        , 0.79487179,
       0.70138889, 0.83333333, 0.56742941, 0.66666667, 0.62121212,
       0.76666667, 0.88235294, 0.6       , 0.76609848]))
rank_0 auc: 0.6182189635528965
2025-10-13 12:59:58,867 [INFO] Saving checkpoint at epoch 8 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 13:00:16,857 [INFO] Start training
2025-10-13 13:00:16,906 [INFO] Start training epoch 9, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [9]  [ 0/50]  eta: 1:09:15  lr: 0.000995  loss: 0.6574  time: 83.1183  data: 0.0000  max mem: 42623
Train: data epoch: [9]  [49/50]  eta: 0:01:58  lr: 0.000994  loss: 0.6137  time: 129.9705  data: 0.0000  max mem: 42623
Train: data epoch: [9] Total time: 1:39:03 (118.8704 s / it)
2025-10-13 14:39:20,424 [INFO] Averaged stats: lr: 0.000995  loss: 0.668807
2025-10-13 14:39:20,429 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:20  loss: 0.7066  acc: 0.1094  time: 9.0323  data: 0.0234  max mem: 42623
Evaluation  [16/82]  eta: 0:07:58  loss: 0.7095  acc: 0.0781  time: 7.2434  data: 0.0052  max mem: 42623
Evaluation  [32/82]  eta: 0:06:03  loss: 0.6401  acc: 0.1094  time: 7.4120  data: 0.0045  max mem: 42623
Evaluation  [48/82]  eta: 0:04:17  loss: 0.6418  acc: 0.0938  time: 7.8167  data: 0.0040  max mem: 42623
Evaluation  [64/82]  eta: 0:02:11  loss: 0.6867  acc: 0.1250  time: 6.7335  data: 0.0054  max mem: 42623
Evaluation  [80/82]  eta: 0:00:15  loss: 0.6569  acc: 0.1875  time: 9.0008  data: 0.0053  max mem: 42623
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6068  acc: 0.1250  time: 8.7307  data: 0.0052  max mem: 42623
Evaluation Total time: 0:10:22 (7.5869 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.09956765174865723 uauc: 0.6629200256239131
2025-10-13 14:49:42,690 [INFO] Averaged stats: loss: 0.681050  acc: 0.115091 ***auc: 0.6517723308129113 ***uauc:(0.6629200256239131, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.64285714, 0.83333333, 0.65384615, 1.        ,
       0.6       , 0.3877551 , 0.96521739, 0.58333333, 1.        ,
       0.45833333, 0.82352941, 0.73015873, 0.82569444, 0.25      ,
       0.58482143, 1.        , 0.5       , 0.66666667, 0.86315789,
       0.72222222, 0.5       , 0.4       , 0.        , 1.        ,
       0.59027778, 0.9047619 , 0.73333333, 0.1875    , 0.53333333,
       0.72222222, 0.83333333, 1.        , 0.8125    , 1.        ,
       0.77490421, 0.81746032, 0.66202091, 0.73611111, 1.        ,
       0.5       , 0.33333333, 0.54076923, 1.        , 0.9       ,
       0.25      , 1.        , 0.70833333, 1.        , 0.        ,
       1.        , 0.81547619, 0.75      , 0.83333333, 0.975     ,
       0.83333333, 0.625     , 0.51785714, 0.43055556, 0.43877551,
       0.35714286, 1.        , 0.89166667, 0.55279503, 0.79120879,
       0.5       , 0.25      , 0.33333333, 1.        , 0.46521739,
       0.72727273, 0.33333333, 0.125     , 0.62848384, 0.53529412,
       0.3125    , 0.86904762, 0.4       , 0.74720497, 0.83333333,
       0.83333333, 0.45555556, 0.5       , 0.92151675, 0.80555556,
       0.61568627, 0.51041667, 0.60784314, 0.74561404, 0.92941176,
       0.64705882, 0.23809524, 0.85      , 1.        , 1.        ,
       0.83333333, 0.        , 0.4       , 0.        , 0.625     ,
       0.5       , 1.        , 0.80952381, 0.88888889, 1.        ,
       0.5245098 , 1.        , 0.66666667, 0.3       , 1.        ,
       0.48      , 0.7037037 , 0.33333333, 0.5483871 , 0.66666667,
       1.        , 0.22222222, 1.        , 1.        , 0.6       ,
       0.62720613, 0.85714286, 0.61568627, 1.        , 0.58596491,
       0.96428571, 0.83333333, 0.5       , 0.54201681, 0.73913043,
       1.        , 0.52083333, 0.66666667, 1.        , 0.60416667,
       1.        , 0.        , 0.875     , 0.74736842, 1.        ,
       0.6       , 0.        , 0.25      , 0.67013889, 1.        ,
       0.67346939, 0.4       , 0.44444444, 0.71657754, 0.66666667,
       0.51388889, 1.        , 0.5       , 0.82222222, 1.        ,
       0.38461538, 0.42428571, 0.8125    , 0.75      , 1.        ,
       1.        , 0.83333333, 0.6875    , 0.5       , 0.82352941,
       0.48473748, 0.21428571, 0.63095238, 0.44444444, 0.43333333,
       0.59558824, 0.47474747, 0.66666667, 0.74940048, 0.21818182,
       0.51851852, 0.5       , 0.57654456, 0.75      , 1.        ,
       0.5       , 0.41666667, 1.        , 0.48148148, 0.72727273,
       0.5       , 0.48611111, 1.        , 0.4       , 0.98      ,
       0.33333333, 0.        , 0.81818182, 1.        , 0.572     ,
       0.33333333, 1.        , 1.        , 0.77777778, 0.71135266,
       0.44      , 0.38888889, 0.32605042, 0.54761905, 0.35294118,
       0.58333333, 0.75      , 1.        , 0.66666667, 0.73076923,
       0.        , 0.83333333, 0.4       , 0.5       , 0.25      ,
       0.5       , 0.71428571, 1.        , 1.        , 1.        ,
       0.74871795, 0.73469388, 0.6969697 , 0.775     , 1.        ,
       0.88888889, 0.5       , 0.42857143, 1.        , 0.84615385,
       0.77951389, 0.91666667, 0.62560857, 0.66666667, 0.66666667,
       0.73333333, 0.72268908, 0.93333333, 0.6780303 ]))
rank_0 auc: 0.6517723308129113
2025-10-13 14:49:43,008 [INFO] Saving checkpoint at epoch 9 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 14:50:01,317 [INFO] Start training
2025-10-13 14:50:01,351 [INFO] Start training epoch 10, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [10]  [ 0/50]  eta: 1:00:33  lr: 0.000994  loss: 0.7020  time: 72.6717  data: 0.0000  max mem: 42623
Train: data epoch: [10]  [49/50]  eta: 0:01:41  lr: 0.000993  loss: 0.5819  time: 114.5694  data: 0.0000  max mem: 42623
Train: data epoch: [10] Total time: 1:24:37 (101.5546 s / it)
2025-10-13 16:14:39,080 [INFO] Averaged stats: lr: 0.000994  loss: 0.654148
2025-10-13 16:14:39,083 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:10:51  loss: 0.7198  acc: 0.0000  time: 7.9444  data: 0.0183  max mem: 42623
Evaluation  [16/82]  eta: 0:08:03  loss: 0.6753  acc: 0.0156  time: 7.3313  data: 0.0044  max mem: 42623
Evaluation  [32/82]  eta: 0:05:56  loss: 0.6350  acc: 0.0469  time: 6.9917  data: 0.0039  max mem: 42623
Evaluation  [48/82]  eta: 0:05:00  loss: 0.6443  acc: 0.0000  time: 11.0696  data: 0.0047  max mem: 42623
Evaluation  [64/82]  eta: 0:02:43  loss: 0.6774  acc: 0.0469  time: 9.3321  data: 0.0048  max mem: 42623
Evaluation  [80/82]  eta: 0:00:18  loss: 0.6715  acc: 0.0312  time: 10.7179  data: 0.0048  max mem: 42623
Evaluation  [81/82]  eta: 0:00:09  loss: 0.5879  acc: 0.0000  time: 10.4266  data: 0.0044  max mem: 42623
Evaluation Total time: 0:12:48 (9.3689 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.19856929779052734 uauc: 0.6478278775874952
2025-10-13 16:27:27,579 [INFO] Averaged stats: loss: 0.685488  acc: 0.019436 ***auc: 0.6590530354718586 ***uauc:(0.6478278775874952, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.625     , 0.83333333, 0.58974359, 1.        ,
       1.        , 0.44897959, 0.93913043, 0.6547619 , 1.        ,
       0.64583333, 0.8907563 , 0.66666667, 0.83567708, 0.625     ,
       0.55803571, 0.66666667, 0.5       , 0.56666667, 0.85263158,
       0.725     , 1.        , 0.53333333, 0.        , 0.5       ,
       0.61647727, 0.85714286, 0.6       , 0.25      , 0.66666667,
       0.77777778, 1.        , 1.        , 0.8125    , 1.        ,
       0.77458493, 0.87301587, 0.728223  , 0.73611111, 1.        ,
       0.625     , 0.33333333, 0.58769231, 1.        , 0.        ,
       0.75      , 0.33333333, 0.70833333, 1.        , 0.25      ,
       1.        , 0.80952381, 0.75      , 0.87345679, 0.975     ,
       0.83333333, 0.75      , 0.75      , 0.44444444, 0.47959184,
       0.39285714, 0.9       , 0.86333333, 0.51863354, 0.83791209,
       1.        , 0.        , 0.5       , 1.        , 0.42608696,
       0.8030303 , 0.        , 0.        , 0.56744705, 0.54313725,
       0.4375    , 0.8234127 , 0.48947368, 0.74037267, 0.75      ,
       0.77777778, 0.60925926, 0.78571429, 0.91843034, 0.75      ,
       0.63071895, 0.52864583, 0.43137255, 0.54385965, 0.94117647,
       0.87394958, 0.25396825, 0.9       , 1.        , 1.        ,
       0.79166667, 0.        , 0.2       , 0.        , 0.75      ,
       0.59027778, 1.        , 0.80952381, 0.66666667, 1.        ,
       0.60021786, 0.796875  , 0.83333333, 0.2       , 1.        ,
       0.52      , 0.46296296, 0.16666667, 0.53989813, 0.66666667,
       1.        , 0.33333333, 1.        , 1.        , 0.6       ,
       0.66333666, 0.80952381, 0.58235294, 1.        , 0.63333333,
       0.96428571, 0.83333333, 0.33333333, 0.49906629, 0.83695652,
       1.        , 0.375     , 0.66666667, 1.        , 0.71180556,
       1.        , 0.5       , 0.375     , 0.75789474, 0.        ,
       0.6       , 0.        , 0.375     , 0.68402778, 1.        ,
       0.79591837, 0.5       , 0.53333333, 0.67379679, 0.83333333,
       0.63888889, 0.        , 0.        , 0.75555556, 1.        ,
       0.4957265 , 0.37619048, 0.5625    , 1.        , 1.        ,
       1.        , 0.91666667, 0.8125    , 0.5       , 0.72941176,
       0.52503053, 0.10714286, 0.55952381, 0.33333333, 0.33333333,
       0.68382353, 0.50505051, 0.66666667, 0.75836173, 0.18181818,
       0.51851852, 0.5       , 0.58611263, 0.125     , 1.        ,
       1.        , 0.5       , 1.        , 0.35555556, 0.52272727,
       0.5       , 0.5       , 1.        , 0.3       , 0.92      ,
       1.        , 0.        , 0.54545455, 0.5       , 0.48      ,
       0.66666667, 1.        , 0.66666667, 0.77777778, 0.76871981,
       0.44      , 0.39473684, 0.41260504, 0.42857143, 0.26470588,
       0.60416667, 0.66666667, 0.75      , 0.33333333, 0.84615385,
       0.        , 1.        , 0.6       , 0.53125   , 0.25      ,
       0.        , 0.72857143, 1.        , 1.        , 1.        ,
       0.75641026, 0.96938776, 0.66666667, 0.825     , 1.        ,
       0.77777778, 0.5       , 0.71428571, 1.        , 0.76923077,
       0.76736111, 1.        , 0.60938007, 0.        , 0.74242424,
       0.8       , 0.7394958 , 0.86666667, 0.69507576]))
rank_0 auc: 0.6590530354718586
2025-10-13 16:27:27,603 [INFO] Saving checkpoint at epoch 10 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 16:27:46,185 [INFO] Start training
2025-10-13 16:27:46,217 [INFO] Start training epoch 11, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [11]  [ 0/50]  eta: 1:01:45  lr: 0.000993  loss: 0.5345  time: 74.1139  data: 0.0000  max mem: 42623
Train: data epoch: [11]  [49/50]  eta: 0:02:00  lr: 0.000992  loss: 0.5398  time: 120.9596  data: 0.0000  max mem: 42623
Train: data epoch: [11] Total time: 1:40:08 (120.1609 s / it)
2025-10-13 18:07:54,262 [INFO] Averaged stats: lr: 0.000993  loss: 0.673969
2025-10-13 18:07:54,263 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:59  loss: 0.6843  acc: 0.0781  time: 5.8494  data: 0.0199  max mem: 42623
Evaluation  [16/82]  eta: 0:07:13  loss: 0.6673  acc: 0.0625  time: 6.5669  data: 0.0057  max mem: 42623
Evaluation  [32/82]  eta: 0:06:30  loss: 0.6382  acc: 0.0469  time: 8.7130  data: 0.0041  max mem: 42623
Evaluation  [48/82]  eta: 0:05:11  loss: 0.6488  acc: 0.0625  time: 10.7523  data: 0.0043  max mem: 42623
Evaluation  [64/82]  eta: 0:02:36  loss: 0.6708  acc: 0.0781  time: 7.0555  data: 0.0063  max mem: 42623
Evaluation  [80/82]  eta: 0:00:16  loss: 0.6744  acc: 0.0938  time: 6.9056  data: 0.0045  max mem: 42623
Evaluation  [81/82]  eta: 0:00:08  loss: 0.5984  acc: 0.0625  time: 6.6317  data: 0.0044  max mem: 42623
Evaluation Total time: 0:11:05 (8.1146 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.19873952865600586 uauc: 0.6462418669592385
2025-10-13 18:18:59,897 [INFO] Averaged stats: loss: 0.668741  acc: 0.078316 ***auc: 0.673927969377787 ***uauc:(0.6462418669592385, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121,
 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264
, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 44
4, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 6
28, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.66071429, 1.        , 0.6474359 , 1.        ,
       0.8       , 0.30612245, 0.95652174, 0.57142857, 1.        ,
       0.5625    , 0.8697479 , 0.6031746 , 0.84348958, 0.375     ,
       0.61607143, 0.66666667, 0.5       , 0.68888889, 0.84210526,
       0.69722222, 0.5       , 0.66666667, 0.        , 0.        ,
       0.64835859, 0.85714286, 0.73333333, 0.3125    , 0.46666667,
       0.77777778, 1.        , 1.        , 0.765625  , 1.        ,
       0.73243934, 0.82539683, 0.65505226, 0.625     , 1.        ,
       0.625     , 0.        , 0.60384615, 1.        , 0.5       ,
       1.        , 0.66666667, 0.64583333, 1.        , 0.        ,
       0.        , 0.76785714, 0.66666667, 0.91358025, 1.        ,
       0.83333333, 0.625     , 0.69642857, 0.40277778, 0.51530612,
       0.42857143, 1.        , 0.86958333, 0.55590062, 0.86263736,
       0.5       , 0.5       , 0.33333333, 1.        , 0.52173913,
       0.71212121, 0.33333333, 0.125     , 0.65607581, 0.52156863,
       0.3125    , 0.89087302, 0.46315789, 0.77515528, 0.70833333,
       0.66666667, 0.61851852, 0.78571429, 0.94179894, 0.77777778,
       0.68235294, 0.52018229, 0.48529412, 0.71929825, 0.90588235,
       0.87394958, 0.25396825, 0.9       , 1.        , 1.        ,
       0.79166667, 0.        , 0.25      , 0.        , 0.75      ,
       0.59722222, 1.        , 0.80952381, 0.88888889, 1.        ,
       0.48039216, 0.96875   , 0.83333333, 0.2       , 1.        ,
       0.44      , 0.72222222, 0.33333333, 0.60271647, 0.33333333,
       1.        , 0.30555556, 0.66666667, 1.        , 0.46666667,
       0.67299367, 0.80952381, 0.58823529, 0.        , 0.66666667,
       0.96428571, 1.        , 0.66666667, 0.53454715, 0.82608696,
       1.        , 0.54166667, 0.66666667, 1.        , 0.72916667,
       1.        , 0.        , 0.5       , 0.78947368, 1.        ,
       0.48571429, 0.        , 0.25      , 0.73958333, 1.        ,
       0.73469388, 0.6       , 0.41111111, 0.72192513, 0.66666667,
       0.47222222, 1.        , 0.        , 0.84444444, 1.        ,
       0.41025641, 0.43857143, 0.75      , 0.75      , 1.        ,
       1.        , 0.91666667, 0.75      , 1.        , 0.78039216,
       0.54822955, 0.23214286, 0.5       , 0.33333333, 0.23333333,
       0.59926471, 0.56565657, 0.55555556, 0.75766755, 0.18181818,
       0.59259259, 0.5       , 0.58365227, 0.75      , 1.        ,
       0.5       , 0.41666667, 1.        , 0.4962963 , 0.68181818,
       0.75      , 0.5       , 0.75      , 0.3       , 0.84      ,
       0.66666667, 0.        , 0.81818182, 0.5       , 0.428     ,
       0.66666667, 0.66666667, 0.33333333, 0.77777778, 0.77506039,
       0.48      , 0.39473684, 0.43739496, 0.54761905, 0.32352941,
       0.64583333, 0.69444444, 0.75      , 0.33333333, 0.82692308,
       0.        , 1.        , 0.4       , 0.4375    , 0.        ,
       0.        , 0.7       , 1.        , 1.        , 1.        ,
       0.77435897, 0.98979592, 0.78787879, 0.85      , 1.        ,
       0.77777778, 0.5       , 0.85714286, 1.        , 0.85897436,
       0.76388889, 1.        , 0.61952288, 0.        , 0.5       ,
       0.9       , 0.84033613, 0.86666667, 0.73863636]))
rank_0 auc: 0.673927969377787
2025-10-13 18:18:59,920 [INFO] Saving checkpoint at epoch 11 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\minigpt4rec-log\20251012220\checkpoint_best.pth.
2025-10-13 18:19:18,586 [INFO] Start training
2025-10-13 18:19:18,617 [INFO] Start training epoch 12, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [12]  [ 0/50]  eta: 0:56:57  lr: 0.000992  loss: 0.6994  time: 68.3548  data: 0.0000  max mem: 42623
Train: data epoch: [12]  [49/50]  eta: 0:01:53  lr: 0.000990  loss: 0.5585  time: 114.6615  data: 0.0000  max mem: 42623
Train: data epoch: [12] Total time: 1:34:49 (113.7893 s / it)
2025-10-13 19:54:08,084 [INFO] Averaged stats: lr: 0.000991  loss: 0.640799
2025-10-13 19:54:08,088 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:09:48  loss: 0.6710  acc: 0.4062  time: 7.1812  data: 0.0122  max mem: 42623
Evaluation  [16/82]  eta: 0:07:42  loss: 0.7097  acc: 0.3906  time: 7.0002  data: 0.0054  max mem: 42623
Evaluation  [32/82]  eta: 0:07:38  loss: 0.7839  acc: 0.2500  time: 10.7302  data: 0.0043  max mem: 42623
Evaluation  [48/82]  eta: 0:04:58  loss: 0.7336  acc: 0.3438  time: 7.6036  data: 0.0048  max mem: 42623
Evaluation  [64/82]  eta: 0:02:31  loss: 0.7185  acc: 0.3438  time: 7.2937  data: 0.0057  max mem: 42623
Evaluation  [80/82]  eta: 0:00:16  loss: 0.6660  acc: 0.4375  time: 7.4606  data: 0.0038  max mem: 42623
Evaluation  [81/82]  eta: 0:00:08  loss: 0.6847  acc: 0.4375  time: 7.1664  data: 0.0037  max mem: 42623
Evaluation Total time: 0:10:59 (8.0476 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.19050145149230957 uauc: 0.6540899895648828
2025-10-13 20:05:08,217 [INFO] Averaged stats: loss: 0.685617  acc: 0.387957 ***auc: 0.6611958123271364 ***uauc:(0.6540899895648828, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.57142857, 0.83333333, 0.67307692, 1.        ,
       1.        , 0.32653061, 0.96521739, 0.60714286, 0.875     ,
       0.5       , 0.86134454, 0.65079365, 0.82170139, 0.25      ,
       0.57217262, 1.        , 0.5       , 0.68888889, 0.76842105,
       0.64166667, 0.25      , 0.46666667, 0.        , 0.25      ,
       0.64962121, 0.85714286, 0.73333333, 0.3125    , 0.46666667,
       0.80555556, 0.75      , 1.        , 0.765625  , 1.        ,
       0.74712644, 0.84126984, 0.66550523, 0.76388889, 1.        ,
       0.75      , 0.        , 0.52961538, 1.        , 0.5       ,
       0.25      , 1.        , 0.73611111, 1.        , 0.        ,
       1.        , 0.73214286, 0.75      , 0.87654321, 0.9       ,
       1.        , 0.625     , 0.58928571, 0.40277778, 0.41836735,
       0.35714286, 0.9       , 0.84833333, 0.60248447, 0.77747253,
       0.5       , 0.        , 0.5       , 1.        , 0.46956522,
       0.71212121, 0.33333333, 0.125     , 0.643534  , 0.49411765,
       0.375     , 0.90674603, 0.38421053, 0.79440994, 0.70833333,
       0.72222222, 0.62962963, 0.71428571, 0.92680776, 0.72916667,
       0.69575163, 0.47721354, 0.43137255, 0.65789474, 0.92941176,
       0.8487395 , 0.33333333, 0.925     , 0.875     , 1.        ,
       0.75      , 0.        , 0.3       , 0.        , 0.75      ,
       0.58333333, 1.        , 0.80952381, 0.77777778, 1.        ,
       0.50381264, 1.        , 0.83333333, 0.1       , 1.        ,
       0.48      , 0.7037037 , 0.5       , 0.55517827, 0.46666667,
       1.        , 0.38888889, 1.        , 1.        , 0.53333333,
       0.63220113, 0.80952381, 0.6372549 , 1.        , 0.60350877,
       0.96428571, 0.83333333, 0.66666667, 0.54248366, 0.79347826,
       1.        , 0.375     , 1.        , 1.        , 0.72222222,
       1.        , 0.5       , 0.625     , 0.73684211, 1.        ,
       0.6       , 0.        , 0.5       , 0.71354167, 1.        ,
       0.65306122, 0.4       , 0.51111111, 0.67914439, 0.83333333,
       0.54166667, 1.        , 0.        , 0.83333333, 1.        ,
       0.36752137, 0.48857143, 0.875     , 0.91666667, 1.        ,
       1.        , 0.83333333, 0.875     , 1.        , 0.75294118,
       0.53724054, 0.30357143, 0.52380952, 0.44444444, 0.4       ,
       0.61764706, 0.54545455, 0.66666667, 0.74435189, 0.32727273,
       0.7037037 , 0.5       , 0.52706397, 0.5       , 1.        ,
       0.5       , 0.5       , 1.        , 0.6       , 0.52272727,
       0.5       , 0.40277778, 0.75      , 0.3       , 0.86      ,
       1.        , 0.        , 0.90909091, 0.5       , 0.496     ,
       0.66666667, 0.33333333, 0.66666667, 0.88888889, 0.73082729,
       0.48      , 0.33479532, 0.36134454, 0.47619048, 0.17647059,
       0.5       , 0.77777778, 1.        , 0.66666667, 0.71153846,
       0.        , 0.83333333, 0.53333333, 0.46875   , 0.        ,
       0.        , 0.7       , 1.        , 1.        , 1.        ,
       0.71538462, 0.98979592, 0.72727273, 0.7       , 1.        ,
       0.77777778, 0.5       , 0.71428571, 1.        , 0.79487179,
       0.70833333, 1.        , 0.63429081, 0.        , 0.57575758,
       0.86666667, 0.81512605, 0.86666667, 0.78219697]))
rank_0 auc: 0.6611958123271364
2025-10-13 20:05:08,247 [INFO] Start training
2025-10-13 20:05:08,275 [INFO] Start training epoch 13, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [13]  [ 0/50]  eta: 1:09:48  lr: 0.000990  loss: 0.3842  time: 83.7604  data: 0.0000  max mem: 42623
Train: data epoch: [13]  [49/50]  eta: 0:02:08  lr: 0.000989  loss: 0.4271  time: 142.3625  data: 0.0000  max mem: 42623
Train: data epoch: [13] Total time: 1:46:50 (128.2067 s / it)
2025-10-13 21:51:58,611 [INFO] Averaged stats: lr: 0.000990  loss: 0.629670
2025-10-13 21:51:58,612 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:11:56  loss: 0.6562  acc: 0.3750  time: 8.7415  data: 0.0198  max mem: 42623
Evaluation  [16/82]  eta: 0:09:30  loss: 0.7131  acc: 0.3438  time: 8.6511  data: 0.0056  max mem: 42623
Evaluation  [32/82]  eta: 0:07:05  loss: 0.7559  acc: 0.2188  time: 9.4687  data: 0.0047  max mem: 42623
Evaluation  [48/82]  eta: 0:04:43  loss: 0.7087  acc: 0.2812  time: 7.5920  data: 0.0037  max mem: 42623
Evaluation  [64/82]  eta: 0:02:43  loss: 0.7075  acc: 0.2812  time: 10.5542  data: 0.0043  max mem: 42623
Evaluation  [80/82]  eta: 0:00:17  loss: 0.6250  acc: 0.4219  time: 7.2645  data: 0.0045  max mem: 42623
Evaluation  [81/82]  eta: 0:00:08  loss: 0.5824  acc: 0.4375  time: 6.9695  data: 0.0044  max mem: 42623
Evaluation Total time: 0:11:39 (8.5262 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.20832586288452148 uauc: 0.6529720757962945
2025-10-13 22:03:38,013 [INFO] Averaged stats: loss: 0.670319  acc: 0.356898 ***auc: 0.6655513763825929 ***uauc:(0.6529720757962945, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.59821429, 0.83333333, 0.73076923, 1.        ,
       1.        , 0.33673469, 0.96521739, 0.61904762, 0.875     ,
       0.5       , 0.81512605, 0.61904762, 0.84079861, 0.125     ,
       0.59970238, 0.66666667, 0.5       , 0.61111111, 0.84210526,
       0.74722222, 0.25      , 0.6       , 0.        , 0.5       ,
       0.63257576, 0.9047619 , 0.86666667, 0.375     , 0.4       ,
       0.75      , 0.91666667, 1.        , 0.765625  , 1.        ,
       0.75798212, 0.86507937, 0.69512195, 0.66666667, 1.        ,
       0.875     , 0.33333333, 0.58692308, 1.        , 1.        ,
       1.        , 1.        , 0.66666667, 1.        , 0.        ,
       0.        , 0.71428571, 0.83333333, 0.94444444, 0.975     ,
       1.        , 0.625     , 0.76785714, 0.58333333, 0.37244898,
       0.46428571, 0.8       , 0.865     , 0.61490683, 0.79395604,
       0.5       , 0.5       , 0.5       , 1.        , 0.50869565,
       0.75757576, 0.        , 0.        , 0.64827202, 0.49803922,
       0.25      , 0.9047619 , 0.44210526, 0.77267081, 0.75      ,
       0.88888889, 0.72592593, 0.64285714, 0.92107584, 0.77083333,
       0.65555556, 0.5       , 0.53431373, 0.75438596, 0.92941176,
       0.8487395 , 0.26984127, 0.875     , 0.875     , 1.        ,
       0.72916667, 0.        , 0.5       , 0.33333333, 0.5       ,
       0.56944444, 1.        , 0.80952381, 0.77777778, 1.        ,
       0.54357298, 0.984375  , 0.66666667, 0.15      , 1.        ,
       0.54      , 0.72222222, 0.5       , 0.52886248, 0.53333333,
       1.        , 0.44444444, 0.66666667, 1.        , 0.33333333,
       0.65501166, 0.85714286, 0.6254902 , 1.        , 0.54385965,
       0.92857143, 0.83333333, 0.66666667, 0.55182073, 0.82608696,
       1.        , 0.54166667, 0.33333333, 1.        , 0.69444444,
       1.        , 0.5       , 0.4375    , 0.82105263, 0.        ,
       0.6       , 0.        , 0.375     , 0.68576389, 1.        ,
       0.73469388, 0.4       , 0.48888889, 0.6684492 , 0.83333333,
       0.5       , 1.        , 0.        , 0.81111111, 1.        ,
       0.34188034, 0.50095238, 0.875     , 0.75      , 1.        ,
       1.        , 0.91666667, 0.875     , 1.        , 0.85098039,
       0.51648352, 0.35714286, 0.52380952, 0.33333333, 0.3       ,
       0.65441176, 0.5959596 , 0.55555556, 0.75463839, 0.43636364,
       0.62962963, 0.25      , 0.57627119, 0.5       , 1.        ,
       0.        , 0.66666667, 1.        , 0.4962963 , 0.84090909,
       0.5       , 0.54166667, 1.        , 0.4       , 0.84      ,
       1.        , 0.        , 0.90909091, 0.5       , 0.572     ,
       0.33333333, 0.33333333, 0.33333333, 0.66666667, 0.76283213,
       0.48      , 0.3625731 , 0.37310924, 0.5       , 0.02941176,
       0.54166667, 0.63888889, 1.        , 0.33333333, 0.67307692,
       0.        , 0.83333333, 0.53333333, 0.5625    , 0.        ,
       0.        , 0.71428571, 1.        , 1.        , 1.        ,
       0.70512821, 0.96938776, 0.72727273, 0.775     , 1.        ,
       0.77777778, 0.5       , 0.71428571, 1.        , 0.88461538,
       0.73263889, 1.        , 0.63883479, 0.33333333, 0.54545455,
       0.86666667, 0.8487395 , 0.86666667, 0.75189394]))
rank_0 auc: 0.6655513763825929
2025-10-13 22:03:38,047 [INFO] Start training
2025-10-13 22:03:38,080 [INFO] Start training epoch 14, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [14]  [ 0/50]  eta: 1:03:17  lr: 0.000989  loss: 0.5621  time: 75.9580  data: 0.0000  max mem: 42623
Train: data epoch: [14]  [49/50]  eta: 0:01:48  lr: 0.000987  loss: 0.5456  time: 112.1070  data: 0.0000  max mem: 44665
Train: data epoch: [14] Total time: 1:30:03 (108.0728 s / it)
2025-10-13 23:33:41,720 [INFO] Averaged stats: lr: 0.000988  loss: 0.606854
2025-10-13 23:33:41,721 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:45  loss: 0.6548  acc: 0.1406  time: 6.4075  data: 0.0138  max mem: 44665
Evaluation  [16/82]  eta: 0:08:19  loss: 0.6839  acc: 0.1250  time: 7.5638  data: 0.0046  max mem: 44665
Evaluation  [32/82]  eta: 0:06:21  loss: 0.6640  acc: 0.1094  time: 7.7642  data: 0.0042  max mem: 44665
Evaluation  [48/82]  eta: 0:04:34  loss: 0.6356  acc: 0.1406  time: 8.5358  data: 0.0049  max mem: 44665
Evaluation  [64/82]  eta: 0:02:24  loss: 0.6772  acc: 0.1094  time: 7.8484  data: 0.0065  max mem: 44665
Evaluation  [80/82]  eta: 0:00:15  loss: 0.6329  acc: 0.1719  time: 7.3533  data: 0.0044  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.5870  acc: 0.1250  time: 7.0389  data: 0.0043  max mem: 44665
Evaluation Total time: 0:10:29 (7.6755 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0964512825012207 uauc: 0.6217770744521376
2025-10-13 23:44:11,228 [INFO] Averaged stats: loss: 0.657034  acc: 0.142149 ***auc: 0.6511965087184781 ***uauc:(0.6217770744521376, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.73214286, 0.83333333, 0.57692308, 1.        ,
       1.        , 0.36734694, 0.93043478, 0.64285714, 0.875     ,
       0.5625    , 0.78991597, 0.47619048, 0.86449653, 0.25      ,
       0.67559524, 0.66666667, 0.5       , 0.47777778, 0.82105263,
       0.70833333, 0.5       , 0.53333333, 0.        , 0.5       ,
       0.66729798, 0.76190476, 0.73333333, 0.375     , 0.53333333,
       0.5       , 0.75      , 1.        , 0.734375  , 1.        ,
       0.69731801, 0.76984127, 0.7195122 , 0.65277778, 1.        ,
       0.75      , 0.33333333, 0.53846154, 1.        , 0.8       ,
       0.75      , 0.33333333, 0.70833333, 1.        , 0.        ,
       0.        , 0.80357143, 0.75      , 0.81481481, 0.95      ,
       1.        , 0.875     , 0.91071429, 0.59722222, 0.58673469,
       0.39285714, 0.9       , 0.77      , 0.60869565, 0.82142857,
       0.        , 0.5       , 0.33333333, 1.        , 0.60869565,
       0.60606061, 0.33333333, 0.        , 0.50334448, 0.52352941,
       0.1875    , 0.87698413, 0.54210526, 0.78074534, 0.95833333,
       0.66666667, 0.7962963 , 0.57142857, 0.91446208, 0.61805556,
       0.66405229, 0.484375  , 0.45098039, 0.74561404, 0.92941176,
       0.94957983, 0.36507937, 0.875     , 0.5       , 1.        ,
       0.75      , 0.        , 0.3       , 0.        , 0.5       ,
       0.48611111, 1.        , 0.52380952, 0.44444444, 0.5       ,
       0.4956427 , 0.90625   , 0.83333333, 0.3       , 1.        ,
       0.48      , 0.59259259, 0.33333333, 0.49151104, 0.4       ,
       1.        , 0.38888889, 0.66666667, 1.        , 0.46666667,
       0.66250416, 0.9047619 , 0.48627451, 1.        , 0.55789474,
       0.92857143, 0.5       , 0.33333333, 0.53127918, 0.94565217,
       1.        , 0.6875    , 0.        , 1.        , 0.70833333,
       0.5       , 0.        , 0.75      , 0.74736842, 1.        ,
       0.45714286, 0.5       , 0.5       , 0.68402778, 1.        ,
       0.80612245, 0.4       , 0.42222222, 0.65240642, 0.66666667,
       0.59722222, 0.        , 0.        , 0.77777778, 1.        ,
       0.41880342, 0.40952381, 0.75      , 0.75      , 1.        ,
       1.        , 0.75      , 0.75      , 0.5       , 0.83529412,
       0.51648352, 0.28571429, 0.44047619, 0.44444444, 0.36666667,
       0.66176471, 0.61616162, 0.5       , 0.71172536, 0.49090909,
       0.62962963, 0.5       , 0.59704757, 0.5       , 0.75      ,
       0.        , 0.66666667, 1.        , 0.53703704, 0.65909091,
       0.25      , 0.58333333, 0.25      , 0.4       , 0.7       ,
       0.33333333, 0.        , 0.81818182, 0.        , 0.584     ,
       0.66666667, 0.33333333, 0.        , 0.77777778, 0.73128019,
       0.64      , 0.33333333, 0.39915966, 0.57142857, 0.44117647,
       0.79166667, 0.61111111, 0.75      , 0.66666667, 0.65384615,
       0.        , 1.        , 0.2       , 0.65625   , 0.        ,
       0.5       , 0.58571429, 1.        , 1.        , 1.        ,
       0.69230769, 0.97959184, 0.78787879, 0.85      , 1.        ,
       0.66666667, 0.83333333, 0.42857143, 1.        , 0.83333333,
       0.74652778, 1.        , 0.57132425, 0.66666667, 0.59090909,
       0.83333333, 0.83193277, 0.73333333, 0.76893939]))
rank_0 auc: 0.6511965087184781
2025-10-13 23:44:11,262 [INFO] Start training
2025-10-13 23:44:11,295 [INFO] Start training epoch 15, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [15]  [ 0/50]  eta: 1:10:11  lr: 0.000987  loss: 0.7219  time: 84.2342  data: 0.0000  max mem: 44665
Train: data epoch: [15]  [49/50]  eta: 0:01:51  lr: 0.000986  loss: 0.7973  time: 108.3592  data: 0.0000  max mem: 44665
Train: data epoch: [15] Total time: 1:32:59 (111.5927 s / it)
2025-10-14 01:17:10,929 [INFO] Averaged stats: lr: 0.000986  loss: 0.640132
2025-10-14 01:17:10,930 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:10:59  loss: 0.6574  acc: 0.3125  time: 8.0418  data: 0.0136  max mem: 44665
Evaluation  [16/82]  eta: 0:08:07  loss: 0.6950  acc: 0.3438  time: 7.3936  data: 0.0056  max mem: 44665
Evaluation  [32/82]  eta: 0:06:21  loss: 0.7291  acc: 0.1719  time: 7.7977  data: 0.0059  max mem: 44665
Evaluation  [48/82]  eta: 0:04:22  loss: 0.7074  acc: 0.2812  time: 7.5626  data: 0.0053  max mem: 44665
Evaluation  [64/82]  eta: 0:02:39  loss: 0.6795  acc: 0.2344  time: 11.2361  data: 0.0049  max mem: 44665
Evaluation  [80/82]  eta: 0:00:16  loss: 0.6484  acc: 0.3750  time: 6.7003  data: 0.0039  max mem: 44665
Evaluation  [81/82]  eta: 0:00:08  loss: 0.5618  acc: 0.3750  time: 6.4266  data: 0.0038  max mem: 44665
Evaluation Total time: 0:11:14 (8.2238 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.18251705169677734 uauc: 0.6435939315257977
2025-10-14 01:28:25,486 [INFO] Averaged stats: loss: 0.666826  acc: 0.322980 ***auc: 0.6550519034275966 ***uauc:(0.6435939315257977, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.67857143, 1.        , 0.63461538, 1.        ,
       0.8       , 0.3877551 , 0.93913043, 0.71428571, 1.        ,
       0.66666667, 0.79411765, 0.58730159, 0.85086806, 0.25      ,
       0.59375   , 0.66666667, 0.5       , 0.54444444, 0.86315789,
       0.68055556, 1.        , 0.6       , 0.        , 0.25      ,
       0.66003788, 0.78571429, 0.66666667, 0.4375    , 0.6       ,
       0.55555556, 0.83333333, 0.        , 0.5625    , 1.        ,
       0.70114943, 0.6984127 , 0.771777  , 0.76388889, 1.        ,
       0.625     , 0.        , 0.565     , 1.        , 0.3       ,
       0.75      , 0.33333333, 0.75694444, 1.        , 0.        ,
       1.        , 0.72619048, 0.75      , 0.78395062, 1.        ,
       1.        , 0.625     , 0.76785714, 0.59722222, 0.55102041,
       0.32142857, 1.        , 0.8       , 0.59627329, 0.76923077,
       1.        , 0.25      , 0.5       , 1.        , 0.51521739,
       0.78787879, 0.33333333, 0.        , 0.60172798, 0.4627451 ,
       0.375     , 0.83829365, 0.48421053, 0.76956522, 0.875     ,
       0.72222222, 0.77777778, 0.71428571, 0.88800705, 0.73611111,
       0.65490196, 0.50716146, 0.43137255, 0.64912281, 0.94117647,
       0.89915966, 0.3015873 , 0.9       , 0.75      , 1.        ,
       0.79166667, 0.        , 0.1       , 0.33333333, 0.75      ,
       0.63888889, 1.        , 0.66666667, 0.55555556, 0.5       ,
       0.46949891, 0.828125  , 0.83333333, 0.25      , 1.        ,
       0.56      , 0.62962963, 0.5       , 0.43293718, 0.26666667,
       1.        , 0.44444444, 0.66666667, 1.        , 0.53333333,
       0.67898768, 0.85714286, 0.55686275, 1.        , 0.56842105,
       0.89285714, 0.83333333, 0.16666667, 0.51353875, 0.90217391,
       0.        , 0.47916667, 0.66666667, 0.6       , 0.73611111,
       1.        , 1.        , 0.75      , 0.76842105, 1.        ,
       0.65714286, 0.5       , 0.25      , 0.69097222, 1.        ,
       0.81632653, 0.6       , 0.4       , 0.72727273, 0.66666667,
       0.625     , 0.        , 0.        , 0.73333333, 1.        ,
       0.44444444, 0.49142857, 0.75      , 0.66666667, 1.        ,
       1.        , 0.875     , 0.75      , 0.5       , 0.8       ,
       0.55433455, 0.25      , 0.44047619, 0.55555556, 0.46666667,
       0.73529412, 0.5959596 , 0.61111111, 0.73614792, 0.33636364,
       0.7037037 , 0.5       , 0.58693275, 0.375     , 0.75      ,
       1.        , 0.75      , 1.        , 0.57037037, 0.29545455,
       0.5       , 0.54166667, 1.        , 0.5       , 0.9       ,
       0.33333333, 0.        , 0.72727273, 0.5       , 0.58      ,
       0.33333333, 0.33333333, 1.        , 0.77777778, 0.73671498,
       0.68      , 0.34502924, 0.41092437, 0.57142857, 0.41176471,
       0.5625    , 0.75      , 0.75      , 0.66666667, 0.76923077,
       0.        , 1.        , 0.33333333, 0.5       , 0.        ,
       0.5       , 0.52857143, 1.        , 0.        , 1.        ,
       0.74615385, 0.95918367, 0.72727273, 0.775     , 1.        ,
       0.88888889, 0.66666667, 0.28571429, 1.        , 0.69230769,
       0.72569444, 1.        , 0.60621551, 1.        , 0.76515152,
       0.86666667, 0.77310924, 0.73333333, 0.76325758]))
rank_0 auc: 0.6550519034275966
2025-10-14 01:28:25,513 [INFO] Start training
2025-10-14 01:28:25,543 [INFO] Start training epoch 16, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [16]  [ 0/50]  eta: 1:02:46  lr: 0.000986  loss: 0.7855  time: 75.3260  data: 0.0000  max mem: 44665
Train: data epoch: [16]  [49/50]  eta: 0:01:52  lr: 0.000984  loss: 0.5494  time: 111.6603  data: 0.0000  max mem: 44665
Train: data epoch: [16] Total time: 1:33:54 (112.6812 s / it)
2025-10-14 03:02:19,603 [INFO] Averaged stats: lr: 0.000985  loss: 0.640693
2025-10-14 03:02:19,605 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:35  loss: 0.6507  acc: 0.2031  time: 6.2905  data: 0.0378  max mem: 44665
Evaluation  [16/82]  eta: 0:07:39  loss: 0.6605  acc: 0.2344  time: 6.9678  data: 0.0064  max mem: 44665
Evaluation  [32/82]  eta: 0:05:49  loss: 0.6685  acc: 0.1094  time: 7.0970  data: 0.0040  max mem: 44665
Evaluation  [48/82]  eta: 0:04:08  loss: 0.6690  acc: 0.1719  time: 7.6393  data: 0.0042  max mem: 44665
Evaluation  [64/82]  eta: 0:02:11  loss: 0.6582  acc: 0.1250  time: 7.1839  data: 0.0039  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6550  acc: 0.2656  time: 6.9797  data: 0.0043  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.5552  acc: 0.1875  time: 6.7024  data: 0.0041  max mem: 44665
Evaluation Total time: 0:09:38 (7.0541 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.2128145694732666 uauc: 0.6500990162384402
2025-10-14 03:11:58,301 [INFO] Averaged stats: loss: 0.651077  acc: 0.200267 ***auc: 0.6581309480024333 ***uauc:(0.6500990162384402, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.6875    , 1.        , 0.76923077, 0.66666667,
       0.8       , 0.40816327, 0.92173913, 0.71428571, 0.75      ,
       0.47916667, 0.7394958 , 0.82539683, 0.83133681, 0.25      ,
       0.62202381, 0.33333333, 0.5       , 0.61111111, 0.85263158,
       0.69444444, 0.5       , 0.46666667, 0.        , 0.5       ,
       0.69507576, 0.80952381, 0.6       , 0.4375    , 0.53333333,
       0.75      , 0.91666667, 1.        , 0.53125   , 1.        ,
       0.76947637, 0.56349206, 0.7369338 , 0.68055556, 1.        ,
       0.625     , 0.        , 0.54923077, 1.        , 0.5       ,
       0.75      , 0.66666667, 0.74305556, 1.        , 0.        ,
       1.        , 0.73214286, 0.83333333, 0.85802469, 0.825     ,
       1.        , 0.625     , 0.67857143, 0.59722222, 0.58163265,
       0.32142857, 1.        , 0.84833333, 0.67236025, 0.84065934,
       0.5       , 0.5       , 0.33333333, 1.        , 0.52608696,
       0.72727273, 0.33333333, 0.        , 0.58974359, 0.48823529,
       0.4375    , 0.81150794, 0.51052632, 0.75714286, 0.79166667,
       0.77777778, 0.67777778, 0.71428571, 0.88359788, 0.66666667,
       0.59542484, 0.48697917, 0.54656863, 0.57017544, 0.91764706,
       0.88235294, 0.36507937, 0.85      , 0.75      , 1.        ,
       0.77083333, 0.        , 0.5       , 0.66666667, 0.875     ,
       0.55555556, 1.        , 0.9047619 , 0.44444444, 0.5       ,
       0.38997821, 0.890625  , 0.66666667, 0.3       , 1.        ,
       0.56      , 0.76851852, 0.5       , 0.42105263, 0.8       ,
       1.        , 0.5       , 0.66666667, 1.        , 0.8       ,
       0.71611722, 0.66666667, 0.54901961, 1.        , 0.49473684,
       0.96428571, 0.83333333, 0.        , 0.53034547, 0.86956522,
       1.        , 0.5       , 0.        , 0.8       , 0.68055556,
       1.        , 1.        , 0.375     , 0.71578947, 1.        ,
       0.68571429, 0.        , 0.375     , 0.68923611, 1.        ,
       0.73469388, 0.4       , 0.44444444, 0.68983957, 0.83333333,
       0.65277778, 0.        , 0.        , 0.8       , 1.        ,
       0.36752137, 0.44761905, 0.8125    , 0.75      , 0.66666667,
       1.        , 0.83333333, 0.875     , 1.        , 0.74117647,
       0.55311355, 0.42857143, 0.48809524, 0.44444444, 0.6       ,
       0.73529412, 0.51515152, 0.61111111, 0.73116244, 0.30909091,
       0.59259259, 0.5       , 0.6254784 , 0.25      , 0.75      ,
       1.        , 0.75      , 1.        , 0.65925926, 0.25      ,
       0.5       , 0.625     , 1.        , 0.3       , 0.92      ,
       0.66666667, 0.        , 1.        , 0.5       , 0.548     ,
       0.33333333, 1.        , 1.        , 0.77777778, 0.67164855,
       0.6       , 0.33040936, 0.41008403, 0.69047619, 0.23529412,
       0.64583333, 0.66666667, 0.75      , 0.66666667, 0.61538462,
       0.        , 0.83333333, 0.2       , 0.65625   , 0.        ,
       0.5       , 0.67142857, 1.        , 1.        , 1.        ,
       0.76153846, 0.92857143, 0.72727273, 0.775     , 1.        ,
       0.88888889, 0.5       , 0.28571429, 1.        , 0.80769231,
       0.76041667, 1.        , 0.65847128, 0.        , 0.77272727,
       1.        , 0.81512605, 0.66666667, 0.75757576]))
rank_0 auc: 0.6581309480024333
2025-10-14 03:11:58,336 [INFO] Start training
2025-10-14 03:11:58,375 [INFO] Start training epoch 17, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [17]  [ 0/50]  eta: 1:12:19  lr: 0.000984  loss: 0.7612  time: 86.7983  data: 0.0000  max mem: 44665
Train: data epoch: [17]  [49/50]  eta: 0:01:48  lr: 0.000982  loss: 0.8261  time: 117.2998  data: 0.0000  max mem: 44665
Train: data epoch: [17] Total time: 1:30:47 (108.9487 s / it)
2025-10-14 04:42:45,809 [INFO] Averaged stats: lr: 0.000983  loss: 0.635684
2025-10-14 04:42:45,810 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:43  loss: 0.6811  acc: 0.4844  time: 9.3135  data: 0.0355  max mem: 44665
Evaluation  [16/82]  eta: 0:08:10  loss: 0.7343  acc: 0.3438  time: 7.4246  data: 0.0066  max mem: 44665
Evaluation  [32/82]  eta: 0:06:05  loss: 0.7767  acc: 0.2812  time: 7.2119  data: 0.0068  max mem: 44665
Evaluation  [48/82]  eta: 0:04:16  loss: 0.7486  acc: 0.4219  time: 7.6843  data: 0.0052  max mem: 44665
Evaluation  [64/82]  eta: 0:02:41  loss: 0.7115  acc: 0.3906  time: 12.1052  data: 0.0056  max mem: 44665
Evaluation  [80/82]  eta: 0:00:16  loss: 0.6814  acc: 0.4375  time: 7.0263  data: 0.0036  max mem: 44665
Evaluation  [81/82]  eta: 0:00:08  loss: 0.6717  acc: 0.4375  time: 6.7175  data: 0.0034  max mem: 44665
Evaluation Total time: 0:11:26 (8.3711 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.23384737968444824 uauc: 0.6344456298672047
2025-10-14 04:54:12,522 [INFO] Averaged stats: loss: 0.694036  acc: 0.428354 ***auc: 0.655583922655125 ***uauc:(0.6344456298672047, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121,
 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264
, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 44
4, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 6
28, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.64285714, 1.        , 0.72435897, 1.        ,
       1.        , 0.79591837, 0.93913043, 0.58333333, 0.5       ,
       0.5       , 0.67647059, 0.6984127 , 0.85182292, 0.125     ,
       0.66220238, 0.33333333, 0.66666667, 0.71111111, 0.84210526,
       0.63333333, 0.5       , 0.66666667, 0.        , 0.25      ,
       0.67866162, 0.76190476, 0.6       , 0.25      , 0.46666667,
       0.58333333, 0.75      , 1.        , 0.65625   , 1.        ,
       0.77394636, 0.56746032, 0.6445993 , 0.54166667, 1.        ,
       1.        , 0.33333333, 0.58230769, 1.        , 0.6       ,
       0.75      , 0.        , 0.65277778, 1.        , 0.        ,
       0.        , 0.70238095, 0.66666667, 0.84567901, 0.975     ,
       1.        , 0.625     , 0.82142857, 0.58333333, 0.54591837,
       0.23214286, 0.8       , 0.81625   , 0.68012422, 0.81318681,
       0.        , 0.25      , 0.33333333, 1.        , 0.54347826,
       0.74242424, 0.        , 0.5       , 0.52341137, 0.50784314,
       0.3125    , 0.78769841, 0.57894737, 0.75341615, 0.83333333,
       0.77777778, 0.63333333, 0.5       , 0.90564374, 0.63194444,
       0.53594771, 0.50651042, 0.44117647, 0.61403509, 0.84705882,
       0.88235294, 0.33333333, 0.975     , 0.75      , 1.        ,
       0.72916667, 0.        , 0.5       , 0.        , 0.75      ,
       0.44444444, 1.        , 0.80952381, 0.66666667, 0.5       ,
       0.44281046, 0.90625   , 0.66666667, 0.45      , 1.        ,
       0.64      , 0.66666667, 0.5       , 0.5008489 , 0.73333333,
       1.        , 0.38888889, 1.        , 0.        , 0.73333333,
       0.62054612, 0.95238095, 0.54705882, 1.        , 0.58245614,
       0.89285714, 0.66666667, 0.33333333, 0.52684407, 0.79347826,
       1.        , 0.3125    , 0.33333333, 0.6       , 0.71527778,
       1.        , 1.        , 1.        , 0.64210526, 1.        ,
       0.82857143, 0.        , 0.375     , 0.67881944, 1.        ,
       0.81632653, 0.2       , 0.44444444, 0.60962567, 0.66666667,
       0.52777778, 0.        , 0.        , 0.81111111, 1.        ,
       0.46153846, 0.52666667, 0.6875    , 0.75      , 1.        ,
       1.        , 0.75      , 0.9375    , 1.        , 0.88627451,
       0.53724054, 0.51785714, 0.58333333, 0.33333333, 0.53333333,
       0.73529412, 0.58585859, 0.55555556, 0.74088098, 0.59090909,
       0.62962963, 0.5       , 0.57353745, 0.375     , 0.5       ,
       1.        , 0.5       , 1.        , 0.62222222, 0.43181818,
       0.25      , 0.54166667, 1.        , 0.3       , 0.68      ,
       0.33333333, 0.        , 0.72727273, 0.5       , 0.612     ,
       0.66666667, 0.33333333, 0.33333333, 0.77777778, 0.69368961,
       0.56      , 0.33040936, 0.37731092, 0.64285714, 0.30882353,
       0.72916667, 0.69444444, 1.        , 0.66666667, 0.42307692,
       0.        , 0.83333333, 0.2       , 0.5625    , 0.        ,
       0.5       , 0.62857143, 1.        , 1.        , 1.        ,
       0.76410256, 0.93877551, 0.63636364, 0.825     , 1.        ,
       0.77777778, 0.5       , 0.71428571, 1.        , 0.83333333,
       0.81597222, 1.        , 0.63007141, 0.66666667, 0.6969697 ,
       1.        , 0.67226891, 0.73333333, 0.76704545]))
rank_0 auc: 0.655583922655125
2025-10-14 04:54:12,556 [INFO] Start training
2025-10-14 04:54:12,596 [INFO] Start training epoch 18, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [18]  [ 0/50]  eta: 1:07:21  lr: 0.000982  loss: 0.4953  time: 80.8327  data: 0.0000  max mem: 44665
Train: data epoch: [18]  [49/50]  eta: 0:01:48  lr: 0.000980  loss: 0.6583  time: 97.0753  data: 0.0000  max mem: 44665
Train: data epoch: [18] Total time: 1:30:39 (108.7944 s / it)
2025-10-14 06:24:52,315 [INFO] Averaged stats: lr: 0.000981  loss: 0.649028
2025-10-14 06:24:52,317 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:17  loss: 0.6253  acc: 0.2812  time: 6.0712  data: 0.0238  max mem: 44665
Evaluation  [16/82]  eta: 0:07:36  loss: 0.6990  acc: 0.2031  time: 6.9109  data: 0.0055  max mem: 44665
Evaluation  [32/82]  eta: 0:05:42  loss: 0.6867  acc: 0.1094  time: 6.9292  data: 0.0048  max mem: 44665
Evaluation  [48/82]  eta: 0:04:14  loss: 0.6723  acc: 0.2031  time: 8.2457  data: 0.0048  max mem: 44665
Evaluation  [64/82]  eta: 0:02:16  loss: 0.6420  acc: 0.2031  time: 7.8034  data: 0.0046  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6524  acc: 0.2656  time: 7.4334  data: 0.0039  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.5595  acc: 0.3125  time: 7.1294  data: 0.0038  max mem: 44665
Evaluation Total time: 0:10:04 (7.3771 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.18096137046813965 uauc: 0.6448526718045986
2025-10-14 06:34:57,441 [INFO] Averaged stats: loss: 0.645210  acc: 0.264863 ***auc: 0.6726374923734755 ***uauc:(0.6448526718045986, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.76785714, 1.        , 0.68269231, 1.        ,
       1.        , 0.44897959, 0.94782609, 0.51190476, 0.625     ,
       0.625     , 0.86134454, 0.6031746 , 0.86059028, 0.125     ,
       0.56994048, 0.66666667, 0.5       , 0.81111111, 0.86315789,
       0.72777778, 0.5       , 0.8       , 0.        , 0.25      ,
       0.67171717, 0.73809524, 0.66666667, 0.375     , 0.4       ,
       0.69444444, 0.83333333, 1.        , 0.578125  , 1.        ,
       0.75542784, 0.75      , 0.67944251, 0.63888889, 1.        ,
       1.        , 0.        , 0.57153846, 1.        , 0.5       ,
       1.        , 0.        , 0.64583333, 1.        , 0.        ,
       0.        , 0.80952381, 0.66666667, 0.87037037, 0.975     ,
       0.66666667, 0.75      , 0.80357143, 0.56944444, 0.6122449 ,
       0.17857143, 1.        , 0.80125   , 0.6242236 , 0.81043956,
       0.5       , 0.5       , 0.33333333, 1.        , 0.56521739,
       0.84848485, 0.        , 0.5       , 0.56075808, 0.56666667,
       0.4375    , 0.83134921, 0.51842105, 0.76086957, 0.83333333,
       0.66666667, 0.59259259, 0.28571429, 0.9047619 , 0.65972222,
       0.61339869, 0.50390625, 0.44852941, 0.69298246, 0.91764706,
       0.8907563 , 0.31746032, 0.9       , 0.75      , 1.        ,
       0.6875    , 0.        , 0.3       , 0.        , 0.625     ,
       0.69444444, 1.        , 0.80952381, 0.77777778, 0.5       ,
       0.53159041, 0.71875   , 0.83333333, 0.3       , 1.        ,
       0.54      , 0.74074074, 0.5       , 0.44821732, 0.6       ,
       1.        , 0.44444444, 1.        , 1.        , 0.53333333,
       0.67515818, 0.95238095, 0.57647059, 1.        , 0.61052632,
       0.96428571, 0.83333333, 0.33333333, 0.55088702, 0.79347826,
       1.        , 0.41666667, 0.66666667, 0.8       , 0.72916667,
       1.        , 0.5       , 0.75      , 0.69473684, 1.        ,
       0.57142857, 0.        , 0.25      , 0.72222222, 1.        ,
       0.83673469, 0.5       , 0.44444444, 0.58823529, 0.83333333,
       0.58333333, 0.        , 0.5       , 0.8       , 1.        ,
       0.39316239, 0.54761905, 0.8125    , 0.58333333, 1.        ,
       1.        , 0.91666667, 0.875     , 0.5       , 0.82745098,
       0.48840049, 0.48214286, 0.52380952, 0.33333333, 0.43333333,
       0.69852941, 0.56565657, 0.5       , 0.75823552, 0.54545455,
       0.62962963, 0.25      , 0.56205577, 1.        , 0.75      ,
       0.5       , 0.33333333, 1.        , 0.57777778, 0.25      ,
       0.25      , 0.51388889, 1.        , 0.3       , 0.7       ,
       0.66666667, 0.        , 0.90909091, 1.        , 0.6       ,
       0.66666667, 0.        , 1.        , 0.44444444, 0.77943841,
       0.52      , 0.34210526, 0.38571429, 0.61904762, 0.41176471,
       0.77083333, 0.69444444, 1.        , 0.66666667, 0.65384615,
       0.        , 1.        , 0.33333333, 0.5625    , 0.        ,
       0.5       , 0.62857143, 1.        , 1.        , 1.        ,
       0.73076923, 0.96938776, 0.72727273, 0.85      , 0.        ,
       0.83333333, 0.33333333, 0.71428571, 1.        , 0.83333333,
       0.83854167, 0.91666667, 0.6441902 , 0.66666667, 0.57575758,
       0.9       , 0.73109244, 0.66666667, 0.78598485]))
rank_0 auc: 0.6726374923734755
2025-10-14 06:34:57,469 [INFO] Start training
2025-10-14 06:34:57,499 [INFO] Start training epoch 19, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [19]  [ 0/50]  eta: 1:06:45  lr: 0.000980  loss: 0.7403  time: 80.1026  data: 0.0000  max mem: 44665
Train: data epoch: [19]  [49/50]  eta: 0:02:04  lr: 0.000978  loss: 0.4332  time: 134.5791  data: 0.0000  max mem: 44665
Train: data epoch: [19] Total time: 1:43:26 (124.1283 s / it)
2025-10-14 08:18:23,916 [INFO] Averaged stats: lr: 0.000979  loss: 0.623840
2025-10-14 08:18:23,918 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:15  loss: 0.6244  acc: 0.4062  time: 5.3144  data: 0.0125  max mem: 44665
Evaluation  [16/82]  eta: 0:07:11  loss: 0.7564  acc: 0.3125  time: 6.5394  data: 0.0034  max mem: 44665
Evaluation  [32/82]  eta: 0:05:41  loss: 0.7584  acc: 0.2188  time: 7.1226  data: 0.0028  max mem: 44665
Evaluation  [48/82]  eta: 0:04:03  loss: 0.7321  acc: 0.3438  time: 7.4905  data: 0.0025  max mem: 44665
Evaluation  [64/82]  eta: 0:02:19  loss: 0.6738  acc: 0.2812  time: 8.9572  data: 0.0026  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6691  acc: 0.3750  time: 6.9690  data: 0.0026  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6360  acc: 0.3125  time: 6.6810  data: 0.0025  max mem: 44665
Evaluation Total time: 0:10:04 (7.3694 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0773622989654541 uauc: 0.643145374687918
2025-10-14 08:28:28,307 [INFO] Averaged stats: loss: 0.672036  acc: 0.350991 ***auc: 0.6647543126892154 ***uauc:(0.643145374687918, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121,
 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264
, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 44
4, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 6
28, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.74107143, 1.        , 0.57051282, 0.66666667,
       0.8       , 0.46938776, 0.94782609, 0.46428571, 0.75      ,
       0.66666667, 0.82352941, 0.6031746 , 0.82395833, 0.125     ,
       0.69047619, 0.66666667, 0.33333333, 0.72222222, 0.78947368,
       0.63055556, 0.5       , 0.53333333, 0.        , 0.25      ,
       0.68118687, 0.88095238, 0.6       , 0.5       , 0.53333333,
       0.58333333, 1.        , 1.        , 0.703125  , 1.        ,
       0.71711367, 0.78174603, 0.70557491, 0.52777778, 1.        ,
       1.        , 0.        , 0.57346154, 0.5       , 0.2       ,
       0.75      , 0.33333333, 0.57638889, 1.        , 0.        ,
       1.        , 0.82142857, 0.58333333, 0.90123457, 0.9       ,
       1.        , 0.625     , 0.73214286, 0.59722222, 0.59693878,
       0.17857143, 1.        , 0.84583333, 0.66149068, 0.82692308,
       0.5       , 0.5       , 0.66666667, 1.        , 0.53913043,
       0.81818182, 0.        , 0.625     , 0.54682274, 0.52352941,
       0.3125    , 0.86309524, 0.52631579, 0.70993789, 0.79166667,
       0.61111111, 0.44814815, 0.78571429, 0.91975309, 0.79861111,
       0.61013072, 0.49348958, 0.66176471, 0.68421053, 0.92941176,
       0.86554622, 0.36507937, 0.825     , 0.875     , 1.        ,
       0.60416667, 0.        , 0.5       , 0.33333333, 0.625     ,
       0.54166667, 1.        , 0.80952381, 0.66666667, 1.        ,
       0.55664488, 0.96875   , 0.83333333, 0.3       , 1.        ,
       0.52      , 0.77777778, 0.5       , 0.52801358, 0.8       ,
       1.        , 0.5       , 0.66666667, 1.        , 0.86666667,
       0.68531469, 0.95238095, 0.62156863, 1.        , 0.58947368,
       0.96428571, 0.66666667, 0.5       , 0.59010271, 0.7173913 ,
       1.        , 0.39583333, 0.66666667, 1.        , 0.68055556,
       1.        , 0.        , 0.875     , 0.76842105, 1.        ,
       0.48571429, 0.        , 0.25      , 0.73263889, 1.        ,
       0.69387755, 0.5       , 0.4       , 0.69518717, 0.83333333,
       0.625     , 0.        , 0.25      , 0.92222222, 1.        ,
       0.39316239, 0.5452381 , 0.875     , 0.66666667, 1.        ,
       1.        , 0.83333333, 0.8125    , 1.        , 0.80784314,
       0.51892552, 0.48214286, 0.53571429, 0.55555556, 0.66666667,
       0.74264706, 0.61616162, 0.55555556, 0.74687618, 0.50909091,
       0.66666667, 0.5       , 0.57299071, 0.75      , 1.        ,
       0.5       , 0.41666667, 0.66666667, 0.51851852, 0.29545455,
       0.25      , 0.65277778, 0.25      , 0.5       , 0.72      ,
       0.66666667, 0.        , 0.90909091, 0.5       , 0.568     ,
       0.        , 0.66666667, 1.        , 0.44444444, 0.78245773,
       0.52      , 0.35672515, 0.41596639, 0.66666667, 0.11764706,
       0.83333333, 0.63888889, 1.        , 0.66666667, 0.80769231,
       0.        , 0.83333333, 0.26666667, 0.65625   , 0.        ,
       0.5       , 0.52857143, 1.        , 0.        , 1.        ,
       0.7025641 , 1.        , 0.72727273, 0.825     , 0.        ,
       0.77777778, 0.66666667, 0.28571429, 1.        , 0.82051282,
       0.83333333, 0.91666667, 0.63534567, 0.        , 0.66666667,
       0.93333333, 0.7394958 , 0.66666667, 0.80492424]))
rank_0 auc: 0.6647543126892154
2025-10-14 08:28:28,319 [INFO] Start training
2025-10-14 08:28:28,330 [INFO] Start training epoch 20, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [20]  [ 0/50]  eta: 1:10:12  lr: 0.000977  loss: 0.6190  time: 84.2570  data: 0.0000  max mem: 44665
Train: data epoch: [20]  [49/50]  eta: 0:02:03  lr: 0.000975  loss: 0.7469  time: 120.0505  data: 0.0000  max mem: 44665
Train: data epoch: [20] Total time: 1:43:04 (123.6999 s / it)
2025-10-14 10:11:33,324 [INFO] Averaged stats: lr: 0.000976  loss: 0.651405
2025-10-14 10:11:33,326 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:25  loss: 0.6545  acc: 0.2031  time: 9.0953  data: 0.0301  max mem: 44665
Evaluation  [16/82]  eta: 0:08:06  loss: 0.7410  acc: 0.1406  time: 7.3697  data: 0.0043  max mem: 44665
Evaluation  [32/82]  eta: 0:06:08  loss: 0.6369  acc: 0.1094  time: 7.4427  data: 0.0026  max mem: 44665
Evaluation  [48/82]  eta: 0:04:18  loss: 0.6414  acc: 0.2031  time: 7.6894  data: 0.0026  max mem: 44665
Evaluation  [64/82]  eta: 0:02:18  loss: 0.6600  acc: 0.1719  time: 7.9064  data: 0.0027  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6733  acc: 0.2188  time: 6.8360  data: 0.0029  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.7388  acc: 0.1250  time: 6.5295  data: 0.0027  max mem: 44665
Evaluation Total time: 0:10:02 (7.3429 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07601165771484375 uauc: 0.6375986844208533
2025-10-14 10:21:35,537 [INFO] Averaged stats: loss: 0.650493  acc: 0.190739 ***auc: 0.6648135579227582 ***uauc:(0.6375986844208533, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.74107143, 1.        , 0.6474359 , 0.66666667,
       1.        , 0.57142857, 0.93043478, 0.44047619, 0.75      ,
       0.625     , 0.78991597, 0.66666667, 0.79930556, 0.375     ,
       0.63839286, 0.66666667, 0.25      , 0.61111111, 0.78421053,
       0.73888889, 0.        , 0.66666667, 1.        , 0.5       ,
       0.67234848, 0.78571429, 0.73333333, 0.5       , 0.46666667,
       0.83333333, 1.        , 0.        , 0.59375   , 1.        ,
       0.76213282, 0.77380952, 0.61149826, 0.77777778, 1.        ,
       0.75      , 0.        , 0.66769231, 0.5       , 0.9       ,
       0.75      , 0.33333333, 0.61805556, 1.        , 0.        ,
       1.        , 0.73214286, 0.66666667, 0.91358025, 0.775     ,
       0.66666667, 0.5       , 0.82142857, 0.55555556, 0.40816327,
       0.25      , 1.        , 0.86      , 0.60248447, 0.82692308,
       0.5       , 0.25      , 0.33333333, 1.        , 0.52608696,
       0.72727273, 0.        , 0.75      , 0.50613155, 0.58431373,
       0.3125    , 0.81944444, 0.47894737, 0.78944099, 0.66666667,
       0.88888889, 0.72222222, 0.5       , 0.92416226, 0.68402778,
       0.61045752, 0.46875   , 0.53431373, 0.6754386 , 0.95294118,
       0.82352941, 0.31746032, 0.9       , 0.875     , 1.        ,
       0.66666667, 0.        , 0.5       , 0.66666667, 0.5       ,
       0.66666667, 1.        , 0.85714286, 0.66666667, 0.5       ,
       0.50326797, 0.84375   , 0.66666667, 0.25      , 1.        ,
       0.64      , 0.72222222, 0.66666667, 0.4286927 , 0.6       ,
       1.        , 0.38888889, 0.66666667, 1.        , 0.53333333,
       0.63569764, 0.9047619 , 0.52156863, 1.        , 0.56491228,
       0.96428571, 0.66666667, 0.5       , 0.57563025, 0.7826087 ,
       1.        , 0.5       , 1.        , 0.8       , 0.70833333,
       1.        , 0.        , 0.875     , 0.63684211, 1.        ,
       0.6       , 0.        , 0.25      , 0.71354167, 1.        ,
       0.69387755, 0.5       , 0.37777778, 0.72727273, 0.66666667,
       0.51388889, 0.        , 0.5       , 0.92222222, 1.        ,
       0.38461538, 0.54857143, 0.8125    , 0.5       , 1.        ,
       1.        , 0.875     , 0.875     , 1.        , 0.82745098,
       0.52380952, 0.35714286, 0.39285714, 0.33333333, 0.4       ,
       0.60294118, 0.62626263, 0.66666667, 0.72434684, 0.56363636,
       0.59259259, 0.25      , 0.49863313, 0.875     , 1.        ,
       0.5       , 0.33333333, 0.66666667, 0.62222222, 0.47727273,
       0.75      , 0.58333333, 0.75      , 0.4       , 0.9       ,
       0.66666667, 0.        , 0.90909091, 0.5       , 0.636     ,
       0.66666667, 0.33333333, 1.        , 0.55555556, 0.72252415,
       0.52      , 0.36842105, 0.36722689, 0.42857143, 0.32352941,
       0.70833333, 0.66666667, 1.        , 0.66666667, 0.76923077,
       0.        , 1.        , 0.2       , 0.40625   , 0.        ,
       0.        , 0.7       , 1.        , 0.        , 1.        ,
       0.71794872, 1.        , 0.75757576, 0.775     , 1.        ,
       0.77777778, 0.16666667, 0.28571429, 1.        , 0.73076923,
       0.82986111, 0.66666667, 0.6176566 , 0.        , 0.60606061,
       0.96666667, 0.6302521 , 0.66666667, 0.73863636]))
rank_0 auc: 0.6648135579227582
2025-10-14 10:21:35,549 [INFO] Start training
2025-10-14 10:21:35,561 [INFO] Start training epoch 21, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [21]  [ 0/50]  eta: 1:15:48  lr: 0.000975  loss: 0.6096  time: 90.9763  data: 0.0000  max mem: 44665
Train: data epoch: [21]  [49/50]  eta: 0:01:59  lr: 0.000973  loss: 0.6792  time: 122.6488  data: 0.0000  max mem: 44665
Train: data epoch: [21] Total time: 1:39:25 (119.3030 s / it)
2025-10-14 12:01:00,708 [INFO] Averaged stats: lr: 0.000974  loss: 0.613323
2025-10-14 12:01:00,710 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:19  loss: 0.6792  acc: 0.2031  time: 5.3542  data: 0.0122  max mem: 44665
Evaluation  [16/82]  eta: 0:07:17  loss: 0.6869  acc: 0.1719  time: 6.6285  data: 0.0032  max mem: 44665
Evaluation  [32/82]  eta: 0:05:41  loss: 0.6468  acc: 0.1250  time: 7.1225  data: 0.0024  max mem: 44665
Evaluation  [48/82]  eta: 0:04:03  loss: 0.6503  acc: 0.2656  time: 7.4484  data: 0.0027  max mem: 44665
Evaluation  [64/82]  eta: 0:02:10  loss: 0.6801  acc: 0.1719  time: 7.3213  data: 0.0026  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6333  acc: 0.2031  time: 7.2318  data: 0.0025  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6019  acc: 0.2500  time: 6.9260  data: 0.0023  max mem: 44665
Evaluation Total time: 0:09:36 (7.0318 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07530426979064941 uauc: 0.6523556058856899
2025-10-14 12:10:37,414 [INFO] Averaged stats: loss: 0.648721  acc: 0.239901 ***auc: 0.667799799813474 ***uauc:(0.6523556058856899, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121,
 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264
, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 44
4, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 6
28, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.78571429, 1.        , 0.56410256, 1.        ,
       0.8       , 0.51020408, 0.9826087 , 0.5       , 0.75      ,
       0.75      , 0.82352941, 0.71428571, 0.82725694, 0.125     ,
       0.58779762, 0.66666667, 0.33333333, 0.64444444, 0.88421053,
       0.78888889, 0.        , 0.66666667, 1.        , 0.5       ,
       0.66887626, 0.73809524, 0.73333333, 0.5625    , 0.33333333,
       0.59722222, 0.91666667, 0.        , 0.640625  , 1.        ,
       0.74489144, 0.6984127 , 0.71167247, 0.61111111, 1.        ,
       0.75      , 0.33333333, 0.64346154, 1.        , 0.8       ,
       0.75      , 1.        , 0.63194444, 1.        , 0.        ,
       1.        , 0.75595238, 0.58333333, 0.87037037, 0.85      ,
       0.83333333, 0.5       , 0.83928571, 0.5       , 0.65816327,
       0.35714286, 0.9       , 0.84083333, 0.58074534, 0.89835165,
       0.        , 0.5       , 0.5       , 1.        , 0.46956522,
       0.87878788, 0.        , 0.25      , 0.58695652, 0.59019608,
       0.375     , 0.86904762, 0.55263158, 0.75776398, 0.83333333,
       0.72222222, 0.73333333, 0.64285714, 0.84126984, 0.88888889,
       0.65686275, 0.53710938, 0.48529412, 0.75438596, 0.90588235,
       0.91596639, 0.20634921, 0.9       , 0.875     , 1.        ,
       0.75      , 0.        , 0.7       , 0.66666667, 0.625     ,
       0.66666667, 1.        , 0.85714286, 0.77777778, 0.5       ,
       0.51851852, 0.609375  , 0.83333333, 0.5       , 1.        ,
       0.68      , 0.81481481, 0.66666667, 0.44821732, 0.6       ,
       1.        , 0.41666667, 1.        , 1.        , 0.4       ,
       0.66283716, 0.85714286, 0.50392157, 1.        , 0.63859649,
       0.96428571, 0.83333333, 0.5       , 0.51027077, 0.80434783,
       1.        , 0.64583333, 0.66666667, 1.        , 0.72916667,
       1.        , 0.        , 0.625     , 0.70526316, 1.        ,
       0.6       , 0.        , 0.5       , 0.640625  , 1.        ,
       0.87755102, 0.5       , 0.4       , 0.63636364, 0.83333333,
       0.48611111, 0.        , 0.        , 0.82222222, 1.        ,
       0.47863248, 0.4952381 , 0.5       , 0.41666667, 1.        ,
       1.        , 0.95833333, 0.8125    , 0.5       , 0.76470588,
       0.51648352, 0.42857143, 0.42857143, 0.44444444, 0.3       ,
       0.77205882, 0.64646465, 0.38888889, 0.69399217, 0.41818182,
       0.74074074, 0.75      , 0.58857299, 0.75      , 0.5       ,
       0.5       , 0.58333333, 1.        , 0.53333333, 0.5       ,
       1.        , 0.56944444, 1.        , 0.4       , 0.84      ,
       0.66666667, 0.        , 0.90909091, 0.5       , 0.56      ,
       0.33333333, 0.66666667, 0.33333333, 0.22222222, 0.7504529 ,
       0.48      , 0.44005848, 0.43235294, 0.66666667, 0.5       ,
       0.79166667, 0.69444444, 0.5       , 0.33333333, 0.59615385,
       0.        , 1.        , 0.2       , 0.5       , 0.        ,
       0.5       , 0.67142857, 1.        , 1.        , 1.        ,
       0.78461538, 1.        , 0.72727273, 0.825     , 1.        ,
       0.88888889, 0.        , 0.14285714, 1.        , 0.80769231,
       0.70486111, 1.        , 0.58187277, 0.33333333, 0.72727273,
       0.93333333, 0.81512605, 0.66666667, 0.76704545]))
rank_0 auc: 0.667799799813474
2025-10-14 12:10:37,426 [INFO] Start training
2025-10-14 12:10:37,437 [INFO] Start training epoch 22, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [22]  [ 0/50]  eta: 1:10:18  lr: 0.000973  loss: 0.5341  time: 84.3750  data: 0.0000  max mem: 44665
Train: data epoch: [22]  [49/50]  eta: 0:01:58  lr: 0.000970  loss: 0.5597  time: 129.8150  data: 0.0000  max mem: 44665
Train: data epoch: [22] Total time: 1:39:07 (118.9437 s / it)
2025-10-14 13:49:44,623 [INFO] Averaged stats: lr: 0.000972  loss: 0.621350
2025-10-14 13:49:44,625 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:41  loss: 0.6825  acc: 0.3750  time: 5.6322  data: 0.0104  max mem: 44665
Evaluation  [16/82]  eta: 0:07:15  loss: 0.7172  acc: 0.3438  time: 6.6047  data: 0.0030  max mem: 44665
Evaluation  [32/82]  eta: 0:05:46  loss: 0.7384  acc: 0.2500  time: 7.3241  data: 0.0028  max mem: 44665
Evaluation  [48/82]  eta: 0:04:06  loss: 0.7126  acc: 0.3125  time: 7.5293  data: 0.0028  max mem: 44665
Evaluation  [64/82]  eta: 0:02:10  loss: 0.7182  acc: 0.2812  time: 7.2336  data: 0.0028  max mem: 44665
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6757  acc: 0.3906  time: 7.1454  data: 0.0028  max mem: 44665
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6051  acc: 0.3125  time: 6.8771  data: 0.0027  max mem: 44665
Evaluation Total time: 0:09:38 (7.0599 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07492637634277344 uauc: 0.6189555247564341
2025-10-14 13:59:23,633 [INFO] Averaged stats: loss: 0.678970  acc: 0.335366 ***auc: 0.6479745480100951 ***uauc:(0.6189555247564341, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.74107143, 0.83333333, 0.49358974, 0.66666667,
       0.8       , 0.44897959, 0.9826087 , 0.61904762, 0.75      ,
       0.79166667, 0.70168067, 0.61904762, 0.83142361, 0.25      ,
       0.60565476, 0.66666667, 0.        , 0.66666667, 0.84210526,
       0.71111111, 0.        , 0.6       , 0.        , 0.25      ,
       0.62973485, 0.80952381, 0.66666667, 0.5625    , 0.46666667,
       0.51388889, 1.        , 0.        , 0.625     , 1.        ,
       0.69987229, 0.6984127 , 0.61324042, 0.70833333, 1.        ,
       0.75      , 0.        , 0.61230769, 1.        , 0.8       ,
       0.75      , 1.        , 0.59722222, 1.        , 0.        ,
       0.        , 0.81547619, 0.66666667, 0.82098765, 0.85      ,
       1.        , 0.5       , 0.91071429, 0.30555556, 0.68367347,
       0.17857143, 1.        , 0.78291667, 0.58385093, 0.83928571,
       0.5       , 0.        , 0.5       , 1.        , 0.4       ,
       0.59090909, 0.        , 0.125     , 0.50278707, 0.63333333,
       0.3125    , 0.76587302, 0.49473684, 0.73478261, 0.75      ,
       0.5       , 0.76666667, 0.5       , 0.85141093, 0.73611111,
       0.53823529, 0.41796875, 0.56862745, 0.69298246, 0.95294118,
       0.85714286, 0.25396825, 0.9       , 0.75      , 1.        ,
       0.79166667, 0.        , 0.8       , 0.66666667, 0.5       ,
       0.66666667, 1.        , 0.80952381, 0.88888889, 0.5       ,
       0.58115468, 0.78125   , 0.66666667, 0.45      , 1.        ,
       0.66      , 0.72222222, 0.5       , 0.41595925, 0.53333333,
       1.        , 0.22222222, 1.        , 1.        , 0.53333333,
       0.63569764, 0.85714286, 0.48431373, 1.        , 0.56842105,
       0.89285714, 1.        , 0.66666667, 0.5070028 , 0.88043478,
       1.        , 0.875     , 0.        , 1.        , 0.57638889,
       1.        , 0.        , 0.75      , 0.67368421, 1.        ,
       0.57142857, 0.        , 0.5       , 0.609375  , 1.        ,
       0.59183673, 0.6       , 0.37777778, 0.82352941, 1.        ,
       0.47222222, 0.        , 0.25      , 0.78888889, 1.        ,
       0.41025641, 0.45857143, 0.5       , 0.66666667, 0.66666667,
       1.        , 0.95833333, 0.5625    , 1.        , 0.65490196,
       0.51282051, 0.39285714, 0.51190476, 0.        , 0.36666667,
       0.625     , 0.64646465, 0.5       , 0.68534646, 0.52727273,
       0.62962963, 0.25      , 0.51722253, 0.875     , 0.75      ,
       0.5       , 0.5       , 1.        , 0.51111111, 0.34090909,
       0.75      , 0.51388889, 1.        , 0.4       , 0.74      ,
       0.        , 0.        , 1.        , 1.        , 0.532     ,
       0.33333333, 0.33333333, 0.66666667, 0.66666667, 0.69474638,
       0.6       , 0.39766082, 0.41218487, 0.38095238, 0.52941176,
       0.72916667, 0.75      , 0.5       , 0.66666667, 0.32692308,
       0.        , 0.83333333, 0.33333333, 0.59375   , 0.25      ,
       0.        , 0.65714286, 1.        , 0.        , 1.        ,
       0.77179487, 0.98979592, 0.63636364, 0.7       , 1.        ,
       0.88888889, 0.66666667, 0.14285714, 1.        , 0.79487179,
       0.73958333, 0.58333333, 0.59274586, 0.33333333, 0.68181818,
       0.93333333, 0.8487395 , 0.73333333, 0.73106061]))
rank_0 auc: 0.6479745480100951
2025-10-14 13:59:23,646 [INFO] Start training
2025-10-14 13:59:23,657 [INFO] Start training epoch 23, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [23]  [ 0/50]  eta: 0:57:35  lr: 0.000970  loss: 0.6121  time: 69.1082  data: 0.0000  max mem: 44665
Train: data epoch: [23]  [49/50]  eta: 0:02:09  lr: 0.000968  loss: 0.5763  time: 129.8576  data: 0.0000  max mem: 44665
Train: data epoch: [23] Total time: 1:47:31 (129.0376 s / it)
2025-10-14 15:46:55,536 [INFO] Averaged stats: lr: 0.000969  loss: 0.650073
2025-10-14 15:46:55,538 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:07:31  loss: 0.6508  acc: 0.1406  time: 5.5018  data: 0.0091  max mem: 44665
Evaluation  [16/82]  eta: 0:07:03  loss: 0.6917  acc: 0.1406  time: 6.4141  data: 0.0027  max mem: 44665
Evaluation  [32/82]  eta: 0:05:33  loss: 0.6519  acc: 0.1094  time: 6.9222  data: 0.0026  max mem: 44665
Evaluation  [48/82]  eta: 0:03:59  loss: 0.6198  acc: 0.2031  time: 7.4004  data: 0.0027  max mem: 44665
Evaluation  [64/82]  eta: 0:02:07  loss: 0.6756  acc: 0.1406  time: 7.1608  data: 0.0025  max mem: 44665
Evaluation  [80/82]  eta: 0:00:13  loss: 0.6450  acc: 0.1875  time: 6.8892  data: 0.0027  max mem: 44665
Evaluation  [81/82]  eta: 0:00:06  loss: 0.5866  acc: 0.1250  time: 6.6011  data: 0.0026  max mem: 44665
Evaluation Total time: 0:09:21 (6.8480 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0788884162902832 uauc: 0.6353794377553993
2025-10-14 15:56:17,168 [INFO] Averaged stats: loss: 0.651196  acc: 0.158727 ***auc: 0.6711932598821124 ***uauc:(0.6353794377553993, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.71428571, 1.        , 0.48076923, 0.83333333,
       0.6       , 0.32653061, 0.94782609, 0.52380952, 0.75      ,
       0.6875    , 0.78571429, 0.66666667, 0.85225694, 0.25      ,
       0.65178571, 0.66666667, 0.16666667, 0.74444444, 0.72631579,
       0.70555556, 0.5       , 0.6       , 1.        , 0.25      ,
       0.65909091, 0.92857143, 0.73333333, 0.625     , 0.4       ,
       0.80555556, 0.91666667, 0.        , 0.703125  , 0.5       ,
       0.72222222, 0.65873016, 0.66027875, 0.875     , 1.        ,
       0.625     , 0.33333333, 0.65615385, 1.        , 0.8       ,
       0.75      , 0.66666667, 0.61111111, 1.        , 0.        ,
       1.        , 0.72619048, 0.58333333, 0.84567901, 0.9       ,
       0.66666667, 0.5       , 0.76785714, 0.51388889, 0.67857143,
       0.28571429, 1.        , 0.89      , 0.6568323 , 0.81043956,
       0.5       , 0.        , 0.66666667, 1.        , 0.49565217,
       0.65151515, 0.        , 0.25      , 0.57525084, 0.49215686,
       0.5       , 0.8531746 , 0.47368421, 0.78944099, 0.75      ,
       0.55555556, 0.74444444, 0.78571429, 0.87566138, 0.71527778,
       0.60653595, 0.49609375, 0.50980392, 0.80701754, 0.97647059,
       0.89915966, 0.25396825, 0.9       , 0.875     , 1.        ,
       0.79166667, 0.        , 0.6       , 0.66666667, 0.625     ,
       0.61111111, 1.        , 0.85714286, 0.55555556, 0.5       ,
       0.51851852, 0.75      , 0.66666667, 0.3       , 1.        ,
       0.62      , 0.75925926, 0.66666667, 0.43123939, 0.8       ,
       1.        , 0.27777778, 0.66666667, 1.        , 0.46666667,
       0.67232767, 0.57142857, 0.55686275, 1.        , 0.6245614 ,
       0.96428571, 0.83333333, 0.33333333, 0.52661064, 0.76086957,
       1.        , 0.625     , 0.        , 0.8       , 0.59375   ,
       1.        , 0.        , 1.        , 0.73684211, 1.        ,
       0.28571429, 0.        , 0.5       , 0.62847222, 1.        ,
       0.65306122, 0.7       , 0.35555556, 0.65240642, 0.83333333,
       0.56944444, 0.        , 0.25      , 0.84444444, 1.        ,
       0.4017094 , 0.53904762, 0.5625    , 0.66666667, 0.66666667,
       1.        , 0.875     , 0.875     , 0.5       , 0.77647059,
       0.57631258, 0.42857143, 0.35714286, 0.33333333, 0.3       ,
       0.57352941, 0.73737374, 0.55555556, 0.70175439, 0.47272727,
       0.59259259, 0.25      , 0.50027337, 1.        , 0.75      ,
       1.        , 0.5       , 1.        , 0.62962963, 0.68181818,
       0.75      , 0.44444444, 1.        , 0.6       , 0.86      ,
       0.66666667, 0.        , 0.72727273, 0.5       , 0.636     ,
       0.33333333, 0.33333333, 1.        , 0.44444444, 0.7089372 ,
       0.56      , 0.37719298, 0.37478992, 0.42857143, 0.5       ,
       0.66666667, 0.80555556, 0.25      , 0.33333333, 0.59615385,
       0.        , 0.83333333, 0.33333333, 0.53125   , 0.5       ,
       0.5       , 0.7       , 1.        , 0.        , 1.        ,
       0.77179487, 0.94897959, 0.78787879, 0.825     , 1.        ,
       0.83333333, 0.16666667, 0.14285714, 1.        , 0.76923077,
       0.77083333, 0.66666667, 0.58357676, 0.        , 0.81818182,
       0.86666667, 0.84453782, 0.8       , 0.77556818]))
rank_0 auc: 0.6711932598821124
2025-10-14 15:56:17,180 [INFO] Start training
2025-10-14 15:56:17,191 [INFO] Start training epoch 24, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [24]  [ 0/50]  eta: 1:30:45  lr: 0.000968  loss: 0.7872  time: 108.9064  data: 0.0000  max mem: 44665
Train: data epoch: [24]  [49/50]  eta: 0:02:15  lr: 0.000965  loss: 0.6223  time: 124.8736  data: 0.0000  max mem: 45296
Train: data epoch: [24] Total time: 1:52:31 (135.0351 s / it)
2025-10-14 17:48:48,949 [INFO] Averaged stats: lr: 0.000966  loss: 0.632577
2025-10-14 17:48:48,950 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:10:49  loss: 0.6433  acc: 0.3594  time: 7.9194  data: 0.0205  max mem: 45296
Evaluation  [16/82]  eta: 0:07:33  loss: 0.6837  acc: 0.3281  time: 6.8752  data: 0.0054  max mem: 45296
Evaluation  [32/82]  eta: 0:05:36  loss: 0.7385  acc: 0.1719  time: 6.6414  data: 0.0042  max mem: 45296
Evaluation  [48/82]  eta: 0:04:05  loss: 0.7083  acc: 0.3281  time: 7.6625  data: 0.0042  max mem: 45296
Evaluation  [64/82]  eta: 0:02:16  loss: 0.6545  acc: 0.2812  time: 8.3720  data: 0.0041  max mem: 45296
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6589  acc: 0.3594  time: 6.7392  data: 0.0043  max mem: 45296
Evaluation  [81/82]  eta: 0:00:07  loss: 0.5935  acc: 0.3125  time: 6.4667  data: 0.0040  max mem: 45296
Evaluation Total time: 0:09:53 (7.2372 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.20068764686584473 uauc: 0.6524300277134268
2025-10-14 17:58:42,644 [INFO] Averaged stats: loss: 0.660381  acc: 0.330221 ***auc: 0.6697130941914577 ***uauc:(0.6524300277134268, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.80357143, 1.        , 0.44871795, 0.83333333,
       1.        , 0.36734694, 0.9826087 , 0.4047619 , 0.75      ,
       0.60416667, 0.85714286, 0.61904762, 0.8375    , 0.125     ,
       0.60565476, 1.        , 0.5       , 0.76666667, 0.78947368,
       0.8       , 0.5       , 0.86666667, 1.        , 0.25      ,
       0.60984848, 0.88095238, 0.8       , 0.375     , 0.46666667,
       0.58333333, 0.75      , 0.        , 0.859375  , 0.75      ,
       0.70114943, 0.67063492, 0.72299652, 0.67361111, 1.        ,
       0.625     , 0.        , 0.60692308, 1.        , 0.4       ,
       0.75      , 0.66666667, 0.50694444, 0.5       , 0.25      ,
       0.        , 0.81547619, 0.66666667, 0.7962963 , 0.95      ,
       0.66666667, 0.625     , 0.78571429, 0.5       , 0.65306122,
       0.21428571, 0.9       , 0.77416667, 0.61801242, 0.83516484,
       1.        , 0.5       , 0.5       , 1.        , 0.60434783,
       0.81818182, 0.        , 0.        , 0.51449275, 0.55882353,
       0.3125    , 0.88095238, 0.53157895, 0.72919255, 0.79166667,
       0.44444444, 0.74444444, 0.71428571, 0.90123457, 0.69444444,
       0.58333333, 0.52994792, 0.39215686, 0.76315789, 0.94117647,
       0.93277311, 0.26984127, 0.95      , 0.875     , 1.        ,
       0.72916667, 0.        , 0.6       , 0.66666667, 0.625     ,
       0.70833333, 1.        , 0.80952381, 1.        , 0.5       ,
       0.56100218, 0.671875  , 0.83333333, 0.4       , 1.        ,
       0.44      , 0.82407407, 0.33333333, 0.49235993, 0.66666667,
       1.        , 0.27777778, 0.66666667, 1.        , 0.26666667,
       0.68198468, 0.9047619 , 0.60784314, 1.        , 0.57894737,
       0.96428571, 0.83333333, 0.83333333, 0.52427638, 0.7826087 ,
       1.        , 0.60416667, 1.        , 0.8       , 0.64583333,
       1.        , 0.        , 0.75      , 0.68421053, 1.        ,
       0.31428571, 0.        , 0.5       , 0.68229167, 1.        ,
       0.79591837, 0.5       , 0.44444444, 0.62566845, 1.        ,
       0.56944444, 0.        , 0.25      , 0.88888889, 1.        ,
       0.41025641, 0.52      , 0.5625    , 1.        , 1.        ,
       1.        , 0.875     , 0.75      , 1.        , 0.78039216,
       0.56532357, 0.58928571, 0.53571429, 0.55555556, 0.43333333,
       0.75735294, 0.72727273, 0.55555556, 0.72390509, 0.50909091,
       0.66666667, 0.5       , 0.58775287, 0.5       , 1.        ,
       1.        , 0.5       , 1.        , 0.51111111, 0.65909091,
       0.75      , 0.61111111, 1.        , 0.4       , 0.84      ,
       1.        , 0.        , 1.        , 1.        , 0.544     ,
       0.66666667, 0.33333333, 0.33333333, 0.22222222, 0.77913647,
       0.56      , 0.4371345 , 0.39495798, 0.5952381 , 0.47058824,
       0.77083333, 0.75      , 0.25      , 0.66666667, 0.53846154,
       0.5       , 0.83333333, 0.33333333, 0.46875   , 0.25      ,
       0.5       , 0.64285714, 1.        , 1.        , 1.        ,
       0.71282051, 0.93877551, 0.93939394, 0.875     , 0.        ,
       0.88888889, 0.        , 0.14285714, 1.        , 0.76923077,
       0.74305556, 0.16666667, 0.57002597, 0.66666667, 0.78787879,
       0.83333333, 0.87394958, 0.53333333, 0.79356061]))
rank_0 auc: 0.6697130941914577
2025-10-14 17:58:42,675 [INFO] Start training
2025-10-14 17:58:42,724 [INFO] Start training epoch 25, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [25]  [ 0/50]  eta: 1:17:11  lr: 0.000965  loss: 0.6310  time: 92.6254  data: 0.0000  max mem: 45296
Train: data epoch: [25]  [49/50]  eta: 0:01:45  lr: 0.000962  loss: 0.5912  time: 106.3288  data: 0.0000  max mem: 45296
Train: data epoch: [25] Total time: 1:27:34 (105.0986 s / it)
2025-10-14 19:26:17,652 [INFO] Averaged stats: lr: 0.000964  loss: 0.647415
2025-10-14 19:26:17,653 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:35  loss: 0.6585  acc: 0.1719  time: 6.2822  data: 0.0329  max mem: 45296
Evaluation  [16/82]  eta: 0:07:19  loss: 0.6896  acc: 0.1250  time: 6.6520  data: 0.0066  max mem: 45296
Evaluation  [32/82]  eta: 0:05:41  loss: 0.6885  acc: 0.1250  time: 7.0795  data: 0.0054  max mem: 45296
Evaluation  [48/82]  eta: 0:04:00  loss: 0.6849  acc: 0.1406  time: 7.2570  data: 0.0041  max mem: 45296
Evaluation  [64/82]  eta: 0:02:07  loss: 0.6844  acc: 0.1250  time: 7.0577  data: 0.0049  max mem: 45296
Evaluation  [80/82]  eta: 0:00:13  loss: 0.6299  acc: 0.2188  time: 6.8188  data: 0.0044  max mem: 45296
Evaluation  [81/82]  eta: 0:00:06  loss: 0.6273  acc: 0.1875  time: 6.5484  data: 0.0042  max mem: 45296
Evaluation Total time: 0:09:22 (6.8555 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.2338862419128418 uauc: 0.6098090154750763
2025-10-14 19:35:40,062 [INFO] Averaged stats: loss: 0.662346  acc: 0.182927 ***auc: 0.6442534573454364 ***uauc:(0.6098090154750763, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.        , 0.70535714, 1.        , 0.61538462, 0.66666667,
       1.        , 0.53061224, 0.87826087, 0.45238095, 0.5       ,
       0.58333333, 0.64705882, 0.74603175, 0.80477431, 0.        ,
       0.59077381, 1.        , 0.75      , 0.76666667, 0.81052632,
       0.71944444, 0.5       , 0.26666667, 1.        , 0.25      ,
       0.61994949, 0.85714286, 0.86666667, 0.5       , 0.4       ,
       0.65277778, 0.75      , 0.        , 0.78125   , 1.        ,
       0.63282248, 0.68650794, 0.63763066, 0.59722222, 1.        ,
       0.5       , 0.33333333, 0.62615385, 1.        , 0.5       ,
       0.5       , 0.33333333, 0.61805556, 0.5       , 0.25      ,
       1.        , 0.73214286, 0.58333333, 0.68518519, 0.9       ,
       0.83333333, 0.75      , 0.89285714, 0.43055556, 0.48979592,
       0.28571429, 1.        , 0.78583333, 0.66770186, 0.83241758,
       1.        , 0.5       , 0.5       , 1.        , 0.54782609,
       0.60606061, 0.        , 0.        , 0.48885173, 0.60098039,
       0.1875    , 0.82142857, 0.49473684, 0.69254658, 0.875     ,
       0.38888889, 0.84814815, 0.57142857, 0.8866843 , 0.49305556,
       0.57973856, 0.51692708, 0.51470588, 0.75438596, 0.87058824,
       0.80672269, 0.49206349, 0.925     , 0.5       , 1.        ,
       0.72916667, 0.        , 0.4       , 0.66666667, 1.        ,
       0.47222222, 1.        , 0.85714286, 0.55555556, 1.        ,
       0.46949891, 0.578125  , 0.66666667, 0.45      , 1.        ,
       0.48      , 0.82407407, 0.5       , 0.49405772, 0.53333333,
       1.        , 0.19444444, 1.        , 1.        , 0.13333333,
       0.5977356 , 0.85714286, 0.57058824, 0.        , 0.58245614,
       0.92857143, 0.83333333, 1.        , 0.56232493, 0.70652174,
       1.        , 0.58333333, 0.        , 0.6       , 0.6875    ,
       1.        , 0.        , 0.75      , 0.71578947, 0.        ,
       0.31428571, 0.        , 0.375     , 0.6875    , 1.        ,
       0.3877551 , 0.5       , 0.4       , 0.67914439, 1.        ,
       0.70138889, 0.        , 0.25      , 0.9       , 0.        ,
       0.39316239, 0.45714286, 0.4375    , 0.91666667, 0.66666667,
       1.        , 1.        , 0.5625    , 1.        , 0.68627451,
       0.58363858, 0.41071429, 0.63095238, 0.77777778, 0.5       ,
       0.76470588, 0.60606061, 0.61111111, 0.66515209, 0.38181818,
       0.55555556, 0.5       , 0.51011482, 0.75      , 0.75      ,
       0.5       , 0.41666667, 0.66666667, 0.59259259, 0.59090909,
       0.75      , 0.66666667, 1.        , 0.2       , 0.82      ,
       0.33333333, 0.        , 1.        , 0.5       , 0.588     ,
       0.33333333, 0.66666667, 0.66666667, 0.11111111, 0.71512681,
       0.32      , 0.29532164, 0.42563025, 0.83333333, 0.52941176,
       0.64583333, 0.61111111, 0.75      , 0.33333333, 0.42307692,
       0.5       , 0.66666667, 0.66666667, 0.53125   , 0.5       ,
       0.5       , 0.67142857, 0.        , 0.        , 1.        ,
       0.68717949, 0.85714286, 0.81818182, 0.725     , 1.        ,
       0.83333333, 0.5       , 0.28571429, 1.        , 0.65384615,
       0.78819444, 0.66666667, 0.55160662, 0.        , 0.53030303,
       0.86666667, 0.57983193, 0.73333333, 0.79545455]))
rank_0 auc: 0.6442534573454364
2025-10-14 19:35:40,104 [INFO] Start training
2025-10-14 19:35:40,143 [INFO] Start training epoch 26, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [26]  [ 0/50]  eta: 1:14:24  lr: 0.000962  loss: 0.5905  time: 89.2824  data: 0.0000  max mem: 45296
Train: data epoch: [26]  [49/50]  eta: 0:01:50  lr: 0.000959  loss: 1.1200  time: 121.7616  data: 0.0000  max mem: 45296
Train: data epoch: [26] Total time: 1:32:02 (110.4542 s / it)
2025-10-14 21:07:42,854 [INFO] Averaged stats: lr: 0.000961  loss: 0.654969
2025-10-14 21:07:42,857 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:16  loss: 0.6907  acc: 0.0000  time: 6.0492  data: 0.0167  max mem: 45296
Evaluation  [16/82]  eta: 0:07:41  loss: 0.6846  acc: 0.0000  time: 6.9874  data: 0.0059  max mem: 45296
Evaluation  [32/82]  eta: 0:05:47  loss: 0.7119  acc: 0.0000  time: 7.0626  data: 0.0050  max mem: 45296
Evaluation  [48/82]  eta: 0:05:34  loss: 0.6937  acc: 0.0000  time: 13.8368  data: 0.0048  max mem: 45296
Evaluation  [64/82]  eta: 0:02:45  loss: 0.6882  acc: 0.0000  time: 7.2821  data: 0.0040  max mem: 45296
Evaluation  [80/82]  eta: 0:00:17  loss: 0.7116  acc: 0.0000  time: 7.2910  data: 0.0049  max mem: 45296
Evaluation  [81/82]  eta: 0:00:08  loss: 0.7207  acc: 0.0000  time: 6.9941  data: 0.0047  max mem: 45296
Evaluation Total time: 0:11:46 (8.6118 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.2354261875152588 uauc: 0.4803678183036997
2025-10-14 21:19:29,291 [INFO] Averaged stats: loss: 0.694767  acc: 0.000000 ***auc: 0.5140313203861958 ***uauc:(0.4803678183036997, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.5625    , 0.16666667, 0.60897436, 0.16666667,
       1.        , 0.18367347, 0.46956522, 0.41666667, 0.75      ,
       0.58333333, 0.31092437, 0.38095238, 0.53125   , 0.75      ,
       0.40029762, 0.        , 0.25      , 0.82777778, 0.4       ,
       0.525     , 0.5       , 0.13333333, 0.        , 0.5       ,
       0.47159091, 0.57142857, 0.46666667, 0.5       , 0.73333333,
       0.66666667, 0.16666667, 0.        , 0.296875  , 0.5       ,
       0.47318008, 0.42460317, 0.61759582, 0.47222222, 1.        ,
       0.875     , 0.33333333, 0.43653846, 0.5       , 0.        ,
       0.        , 0.33333333, 0.5       , 0.5       , 0.5       ,
       0.        , 0.33333333, 0.25      , 0.34567901, 0.375     ,
       0.        , 0.75      , 0.53571429, 0.18055556, 0.56632653,
       0.28571429, 0.2       , 0.41833333, 0.51242236, 0.71978022,
       1.        , 0.25      , 0.83333333, 0.        , 0.54782609,
       0.74242424, 0.66666667, 0.875     , 0.37040134, 0.5372549 ,
       0.5625    , 0.58531746, 0.49210526, 0.60496894, 0.5       ,
       0.44444444, 0.5962963 , 0.5       , 0.67063492, 0.25694444,
       0.43071895, 0.47786458, 0.5       , 0.30701754, 0.54117647,
       0.48739496, 0.44444444, 0.625     , 0.25      , 0.        ,
       0.52083333, 0.        , 1.        , 1.        , 0.75      ,
       0.33333333, 0.5       , 0.85714286, 0.33333333, 1.        ,
       0.39433551, 0.53125   , 0.33333333, 0.05      , 1.        ,
       0.54      , 0.67592593, 0.5       , 0.55008489, 0.33333333,
       0.        , 0.30555556, 0.33333333, 0.        , 0.6       ,
       0.51465201, 0.23809524, 0.39607843, 1.        , 0.38245614,
       0.60714286, 0.5       , 1.        , 0.47735761, 0.7173913 ,
       0.        , 0.83333333, 0.        , 0.6       , 0.5       ,
       1.        , 0.        , 1.        , 0.28421053, 0.        ,
       0.51428571, 0.5       , 0.625     , 0.58854167, 1.        ,
       0.48979592, 1.        , 0.53333333, 0.38502674, 0.83333333,
       0.61111111, 0.        , 1.        , 0.57777778, 1.        ,
       0.33333333, 0.47190476, 0.625     , 0.75      , 0.        ,
       0.5       , 0.66666667, 0.5       , 1.        , 0.58823529,
       0.65323565, 0.28571429, 0.69047619, 0.        , 0.43333333,
       0.59558824, 0.48484848, 0.38888889, 0.5111069 , 0.10909091,
       0.48148148, 0.25      , 0.51284855, 0.125     , 0.25      ,
       0.        , 0.66666667, 0.66666667, 0.54074074, 0.15909091,
       0.25      , 0.72222222, 0.5       , 0.2       , 0.8       ,
       0.66666667, 0.        , 0.81818182, 0.5       , 0.504     ,
       1.        , 0.        , 0.66666667, 0.66666667, 0.49003623,
       0.76      , 0.38888889, 0.44705882, 0.42857143, 0.35294118,
       0.14583333, 0.61111111, 0.25      , 0.66666667, 0.53846154,
       1.        , 0.33333333, 0.6       , 0.09375   , 0.25      ,
       1.        , 0.22857143, 0.        , 0.        , 1.        ,
       0.63846154, 0.51020408, 0.54545455, 0.15      , 0.        ,
       0.38888889, 0.5       , 0.71428571, 1.        , 0.42307692,
       0.24305556, 0.5       , 0.40701071, 1.        , 0.15151515,
       0.63333333, 0.60504202, 0.46666667, 0.33143939]))
rank_0 auc: 0.5140313203861958
2025-10-14 21:19:29,323 [INFO] Start training
2025-10-14 21:19:29,371 [INFO] Start training epoch 27, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [27]  [ 0/50]  eta: 1:04:49  lr: 0.000959  loss: 0.6648  time: 77.7876  data: 0.0000  max mem: 45296
Train: data epoch: [27]  [49/50]  eta: 0:01:59  lr: 0.000956  loss: 0.7364  time: 119.0150  data: 0.0000  max mem: 45296
Train: data epoch: [27] Total time: 1:39:34 (119.4971 s / it)
2025-10-14 22:59:04,229 [INFO] Averaged stats: lr: 0.000958  loss: 1.006134
2025-10-14 22:59:04,231 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:20  loss: 0.8575  acc: 0.0000  time: 6.1048  data: 0.0179  max mem: 45296
Evaluation  [16/82]  eta: 0:07:26  loss: 0.7988  acc: 0.0000  time: 6.7620  data: 0.0049  max mem: 45296
Evaluation  [32/82]  eta: 0:05:46  loss: 0.6817  acc: 0.0000  time: 7.1445  data: 0.0049  max mem: 45296
Evaluation  [48/82]  eta: 0:04:11  loss: 0.7550  acc: 0.0000  time: 7.9042  data: 0.0048  max mem: 45296
Evaluation  [64/82]  eta: 0:02:33  loss: 0.7844  acc: 0.0000  time: 11.0896  data: 0.0041  max mem: 45296
Evaluation  [80/82]  eta: 0:00:17  loss: 0.8575  acc: 0.0000  time: 10.2739  data: 0.0044  max mem: 45296
Evaluation  [81/82]  eta: 0:00:08  loss: 0.7985  acc: 0.0000  time: 9.8500  data: 0.0042  max mem: 45296
Evaluation Total time: 0:11:46 (8.6214 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.25275731086730957 uauc: 0.49124926869678726
2025-10-14 23:10:51,481 [INFO] Averaged stats: loss: 0.827219  acc: 0.000000 ***auc: 0.4818899645256172 ***uauc:(0.49124926869678726, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 12
1, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 2
64, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441,
444, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
 628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.59375   , 0.16666667, 0.27564103, 0.        ,
       1.        , 0.32653061, 0.27391304, 0.50595238, 0.625     ,
       0.38541667, 0.55672269, 0.52380952, 0.37083333, 0.625     ,
       0.5639881 , 0.        , 0.58333333, 0.52777778, 0.51578947,
       0.41944444, 0.        , 0.93333333, 0.        , 0.        ,
       0.53472222, 0.48809524, 0.23333333, 0.0625    , 0.8       ,
       0.625     , 0.54166667, 0.5       , 0.6484375 , 0.5       ,
       0.52394636, 0.24404762, 0.57317073, 0.70833333, 0.        ,
       0.8125    , 0.        , 0.47153846, 1.        , 0.        ,
       1.        , 1.        , 0.29166667, 0.5       , 0.625     ,
       1.        , 0.57738095, 0.5       , 0.48148148, 0.175     ,
       0.        , 0.75      , 0.33035714, 0.45833333, 0.62755102,
       0.58928571, 0.15      , 0.58458333, 0.41770186, 0.65934066,
       0.25      , 0.5       , 0.41666667, 0.75      , 0.62826087,
       0.89393939, 0.33333333, 0.875     , 0.50195095, 0.55098039,
       0.34375   , 0.60218254, 0.63157895, 0.45124224, 0.4375    ,
       0.33333333, 0.26666667, 0.17857143, 0.32010582, 0.25694444,
       0.54019608, 0.48242188, 0.5       , 0.39035088, 0.62352941,
       0.6512605 , 0.42857143, 0.5875    , 0.        , 1.        ,
       0.86458333, 0.5       , 0.95      , 0.33333333, 0.625     ,
       0.54166667, 0.5       , 0.73809524, 0.72222222, 0.75      ,
       0.4204793 , 0.71875   , 0.5       , 0.225     , 0.5       ,
       0.63      , 0.83796296, 0.5       , 0.57979626, 0.3       ,
       0.125     , 0.33333333, 0.66666667, 0.        , 0.23333333,
       0.42407592, 0.54761905, 0.47647059, 1.        , 0.57368421,
       0.58928571, 0.91666667, 0.16666667, 0.63001867, 0.44565217,
       0.        , 0.10416667, 0.33333333, 0.2       , 0.47916667,
       0.        , 0.75      , 0.3125    , 0.54736842, 0.5       ,
       0.47142857, 0.        , 0.5       , 0.3984375 , 0.16666667,
       0.41836735, 0.35      , 0.78888889, 0.47326203, 0.16666667,
       0.71527778, 0.5       , 0.        , 0.43333333, 0.5       ,
       0.72222222, 0.41714286, 0.3125    , 0.33333333, 0.16666667,
       1.        , 0.60416667, 0.5625    , 1.        , 0.54117647,
       0.54700855, 0.5       , 0.60119048, 0.55555556, 0.75      ,
       0.66544118, 0.55050505, 0.88888889, 0.50643696, 0.37272727,
       0.59259259, 1.        , 0.46008748, 0.5       , 0.        ,
       1.        , 0.58333333, 0.83333333, 0.77777778, 0.        ,
       0.25      , 0.52083333, 1.        , 0.25      , 0.73      ,
       0.66666667, 0.        , 0.09090909, 1.        , 0.39      ,
       0.5       , 0.16666667, 0.16666667, 0.27777778, 0.52038043,
       0.48      , 0.42836257, 0.42394958, 0.55952381, 0.48529412,
       0.51041667, 0.47222222, 0.875     , 0.83333333, 0.41346154,
       1.        , 0.16666667, 0.76666667, 0.578125  , 0.5       ,
       0.5       , 0.48571429, 0.        , 1.        , 1.        ,
       0.53461538, 0.42857143, 0.66666667, 0.3125    , 0.        ,
       0.38888889, 0.33333333, 1.        , 1.        , 0.71153846,
       0.27951389, 0.45833333, 0.53570269, 0.16666667, 0.53030303,
       0.48333333, 0.52521008, 0.46666667, 0.27556818]))
rank_0 auc: 0.4818899645256172
2025-10-14 23:10:51,509 [INFO] Start training
2025-10-14 23:10:51,547 [INFO] Start training epoch 28, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [28]  [ 0/50]  eta: 1:32:16  lr: 0.000956  loss: 0.9743  time: 110.7282  data: 0.0000  max mem: 45296
Train: data epoch: [28]  [49/50]  eta: 0:02:03  lr: 0.000953  loss: 0.6879  time: 130.1941  data: 0.0000  max mem: 45296
Train: data epoch: [28] Total time: 1:42:43 (123.2736 s / it)
2025-10-15 00:53:35,226 [INFO] Averaged stats: lr: 0.000955  loss: 0.722473
2025-10-15 00:53:35,229 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:08:15  loss: 0.6859  acc: 0.0000  time: 6.0399  data: 0.0167  max mem: 45296
Evaluation  [16/82]  eta: 0:07:38  loss: 0.7052  acc: 0.0000  time: 6.9504  data: 0.0053  max mem: 45296
Evaluation  [32/82]  eta: 0:09:17  loss: 0.7442  acc: 0.0000  time: 13.9810  data: 0.0045  max mem: 45296
Evaluation  [48/82]  eta: 0:05:48  loss: 0.7199  acc: 0.0000  time: 7.9466  data: 0.0039  max mem: 45296
Evaluation  [64/82]  eta: 0:02:55  loss: 0.7101  acc: 0.0000  time: 8.0300  data: 0.0044  max mem: 45296
Evaluation  [80/82]  eta: 0:00:18  loss: 0.6859  acc: 0.0000  time: 7.1516  data: 0.0040  max mem: 45296
Evaluation  [81/82]  eta: 0:00:09  loss: 0.7049  acc: 0.0000  time: 6.8428  data: 0.0039  max mem: 45296
Evaluation Total time: 0:12:19 (9.0199 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.27481627464294434 uauc: 0.4936014113207428
2025-10-15 01:05:55,177 [INFO] Averaged stats: loss: 0.695801  acc: 0.000000 ***auc: 0.4928900522798353 ***uauc:(0.4936014113207428, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.56696429, 0.25      , 0.51602564, 0.16666667,
       1.        , 0.35714286, 0.32608696, 0.30952381, 0.5       ,
       0.51041667, 0.68487395, 0.41269841, 0.37534722, 0.5       ,
       0.546875  , 0.        , 0.83333333, 0.58888889, 0.61052632,
       0.46111111, 0.        , 0.53333333, 0.        , 0.        ,
       0.5167298 , 0.36904762, 0.53333333, 0.28125   , 0.8       ,
       0.55555556, 0.20833333, 0.        , 0.5703125 , 0.        ,
       0.50446999, 0.46031746, 0.50696864, 0.33333333, 0.5       ,
       0.5       , 0.        , 0.56769231, 0.        , 0.        ,
       0.75      , 1.        , 0.51041667, 0.5       , 0.875     ,
       1.        , 0.50297619, 0.25      , 0.34259259, 0.425     ,
       0.41666667, 0.9375    , 0.24107143, 0.59722222, 0.47193878,
       0.32142857, 0.        , 0.49333333, 0.41149068, 0.64010989,
       0.        , 0.75      , 0.33333333, 0.5       , 0.63478261,
       0.62878788, 0.        , 0.5       , 0.56243032, 0.60098039,
       0.5625    , 0.53571429, 0.55      , 0.50093168, 0.5625    ,
       0.52777778, 0.48518519, 0.32142857, 0.31790123, 0.33333333,
       0.54052288, 0.40039062, 0.30392157, 0.32017544, 0.67647059,
       0.49579832, 0.19047619, 0.35      , 0.5625    , 1.        ,
       0.78125   , 1.        , 0.75      , 0.        , 0.5       ,
       0.66666667, 0.        , 0.66666667, 0.27777778, 0.        ,
       0.45533769, 0.625     , 0.41666667, 0.125     , 1.        ,
       0.65      , 0.5       , 0.66666667, 0.56281834, 0.        ,
       0.25      , 0.47222222, 0.66666667, 1.        , 0.2       ,
       0.38195138, 0.85714286, 0.46176471, 1.        , 0.53859649,
       0.35714286, 0.25      , 0.33333333, 0.56442577, 0.50543478,
       1.        , 0.36458333, 1.        , 0.4       , 0.55902778,
       1.        , 1.        , 0.1875    , 0.61578947, 1.        ,
       0.54285714, 0.        , 0.5       , 0.43923611, 0.16666667,
       0.25510204, 0.5       , 0.52222222, 0.45989305, 0.83333333,
       0.64583333, 0.5       , 0.        , 0.54444444, 0.        ,
       0.55555556, 0.41380952, 0.6875    , 0.58333333, 0.        ,
       0.75      , 0.70833333, 0.3125    , 1.        , 0.40392157,
       0.55982906, 0.59821429, 0.50595238, 0.27777778, 0.66666667,
       0.45588235, 0.43939394, 0.94444444, 0.51861669, 0.36363636,
       0.44444444, 0.625     , 0.43630399, 0.8125    , 0.5       ,
       1.        , 0.45833333, 0.33333333, 0.64444444, 0.02272727,
       0.75      , 0.41666667, 1.        , 0.2       , 0.82      ,
       0.66666667, 0.        , 0.40909091, 1.        , 0.432     ,
       0.33333333, 0.5       , 1.        , 0.22222222, 0.51675725,
       0.82      , 0.46929825, 0.43529412, 0.61904762, 0.57352941,
       0.39583333, 0.58333333, 0.25      , 0.5       , 0.33653846,
       0.75      , 0.58333333, 0.93333333, 0.328125  , 0.625     ,
       0.25      , 0.43571429, 1.        , 0.        , 1.        ,
       0.55897436, 0.46938776, 0.63636364, 0.4125    , 1.        ,
       0.33333333, 0.33333333, 0.85714286, 0.        , 0.78205128,
       0.44618056, 0.83333333, 0.5588283 , 0.        , 0.34848485,
       0.63333333, 0.63445378, 0.6       , 0.34469697]))
rank_0 auc: 0.4928900522798353
2025-10-15 01:05:55,209 [INFO] Start training
2025-10-15 01:05:55,248 [INFO] Start training epoch 29, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [29]  [ 0/50]  eta: 1:03:43  lr: 0.000953  loss: 0.7441  time: 76.4735  data: 0.0000  max mem: 45296
Train: data epoch: [29]  [49/50]  eta: 0:01:59  lr: 0.000950  loss: 0.6548  time: 114.3883  data: 0.0000  max mem: 45296
Train: data epoch: [29] Total time: 1:39:40 (119.6039 s / it)
2025-10-15 02:45:35,442 [INFO] Averaged stats: lr: 0.000952  loss: 0.744552
2025-10-15 02:45:35,450 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:11:22  loss: 0.6870  acc: 0.0000  time: 8.3222  data: 0.0213  max mem: 45296
Evaluation  [16/82]  eta: 0:07:31  loss: 0.6956  acc: 0.0000  time: 6.8453  data: 0.0055  max mem: 45296
Evaluation  [32/82]  eta: 0:05:46  loss: 0.7128  acc: 0.0000  time: 7.0758  data: 0.0044  max mem: 45296
Evaluation  [48/82]  eta: 0:04:09  loss: 0.7020  acc: 0.0000  time: 7.7828  data: 0.0048  max mem: 45296
Evaluation  [64/82]  eta: 0:02:11  loss: 0.6978  acc: 0.0000  time: 7.2033  data: 0.0042  max mem: 45296
Evaluation  [80/82]  eta: 0:00:14  loss: 0.6870  acc: 0.0000  time: 7.2262  data: 0.0033  max mem: 45296
Evaluation  [81/82]  eta: 0:00:07  loss: 0.6957  acc: 0.0000  time: 6.9305  data: 0.0031  max mem: 45296
Evaluation Total time: 0:09:42 (7.0990 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.16019439697265625 uauc: 0.4907525083946371
2025-10-15 02:55:17,772 [INFO] Averaged stats: loss: 0.691362  acc: 0.000000 ***auc: 0.49128538250816406 ***uauc:(0.4907525083946371, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 12
1, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 2
64, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441,
444, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
 628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([0.5       , 0.47321429, 0.5       , 0.50320513, 1.        ,
       0.3       , 0.66326531, 0.46086957, 0.70238095, 0.5625    ,
       0.51041667, 0.6197479 , 0.30952381, 0.52899306, 0.375     ,
       0.57738095, 0.        , 0.70833333, 0.61111111, 0.38421053,
       0.44583333, 0.        , 0.4       , 0.        , 0.5       ,
       0.44381313, 0.25      , 0.33333333, 0.28125   , 0.53333333,
       0.38888889, 0.29166667, 1.        , 0.609375  , 0.875     ,
       0.58045977, 0.48214286, 0.41027875, 0.35416667, 0.75      ,
       0.375     , 0.        , 0.60230769, 0.        , 0.25      ,
       0.25      , 1.        , 0.39930556, 0.5       , 1.        ,
       0.        , 0.71130952, 0.33333333, 0.4845679 , 0.55      ,
       0.5       , 1.        , 0.25892857, 0.64583333, 0.50510204,
       0.5       , 0.55      , 0.48375   , 0.47515528, 0.53708791,
       0.        , 1.        , 0.66666667, 1.        , 0.60869565,
       0.59090909, 1.        , 0.25      , 0.61928651, 0.6372549 ,
       0.59375   , 0.52678571, 0.46578947, 0.50403727, 0.29166667,
       0.88888889, 0.33148148, 0.42857143, 0.25749559, 0.55208333,
       0.53039216, 0.64908854, 0.67892157, 0.46929825, 0.66470588,
       0.46638655, 0.61111111, 0.45      , 0.25      , 0.5       ,
       0.48958333, 1.        , 0.45      , 0.33333333, 0.5       ,
       0.78472222, 0.5       , 0.66666667, 0.5       , 0.5       ,
       0.44117647, 0.421875  , 0.08333333, 0.175     , 0.        ,
       0.57      , 0.29166667, 0.33333333, 0.5237691 , 0.13333333,
       0.75      , 0.44444444, 0.66666667, 1.        , 0.36666667,
       0.41425241, 0.85714286, 0.48627451, 1.        , 0.69649123,
       0.82142857, 0.66666667, 0.16666667, 0.49836601, 0.20652174,
       0.        , 0.60416667, 1.        , 0.6       , 0.47916667,
       0.        , 0.5       , 0.        , 0.62105263, 1.        ,
       0.6       , 0.        , 1.        , 0.54253472, 1.        ,
       0.31632653, 0.2       , 0.38888889, 0.47593583, 0.66666667,
       0.56944444, 0.        , 0.75      , 0.48333333, 0.        ,
       0.67948718, 0.44761905, 0.65625   , 0.91666667, 0.33333333,
       0.        , 0.39583333, 0.1875    , 0.75      , 0.25882353,
       0.43589744, 0.55357143, 0.27380952, 0.11111111, 0.71666667,
       0.49632353, 0.37373737, 0.66666667, 0.41417392, 0.10909091,
       0.51851852, 0.        , 0.61509021, 0.75      , 0.625     ,
       0.5       , 0.29166667, 1.        , 0.43703704, 0.07954545,
       0.        , 0.40277778, 0.25      , 0.35      , 0.75      ,
       0.33333333, 0.        , 0.45454545, 1.        , 0.486     ,
       0.        , 0.        , 1.        , 0.77777778, 0.42330918,
       0.84      , 0.55847953, 0.46302521, 0.61904762, 0.57352941,
       0.38541667, 0.88888889, 0.75      , 0.33333333, 0.51923077,
       1.        , 0.33333333, 0.46666667, 0.1875    , 0.75      ,
       0.        , 0.56428571, 0.        , 0.        , 1.        ,
       0.42179487, 0.45918367, 0.65151515, 0.65      , 0.        ,
       0.72222222, 0.66666667, 0.78571429, 0.5       , 0.58974359,
       0.41666667, 0.91666667, 0.42956832, 0.66666667, 0.29545455,
       0.66666667, 0.71008403, 0.13333333, 0.40246212]))
rank_0 auc: 0.49128538250816406
2025-10-15 02:55:17,808 [INFO] Start training
2025-10-15 02:55:17,848 [INFO] Start training epoch 30, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [30]  [ 0/50]  eta: 0:59:29  lr: 0.000950  loss: 0.6698  time: 71.3835  data: 0.0000  max mem: 45296
Train: data epoch: [30]  [49/50]  eta: 0:01:52  lr: 0.000947  loss: 0.6912  time: 110.4495  data: 0.0000  max mem: 45296
Train: data epoch: [30] Total time: 1:33:41 (112.4288 s / it)
2025-10-15 04:28:59,288 [INFO] Averaged stats: lr: 0.000948  loss: 0.703579
2025-10-15 04:28:59,290 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:12:32  loss: 0.7131  acc: 0.5625  time: 9.1718  data: 0.0194  max mem: 45296
Evaluation  [16/82]  eta: 0:08:54  loss: 0.7592  acc: 0.5000  time: 8.1006  data: 0.0046  max mem: 45296
Evaluation  [32/82]  eta: 0:06:27  loss: 0.8508  acc: 0.3750  time: 7.3884  data: 0.0036  max mem: 45296
Evaluation  [48/82]  eta: 0:04:28  loss: 0.7934  acc: 0.4531  time: 7.8011  data: 0.0038  max mem: 45296
Evaluation  [64/82]  eta: 0:02:20  loss: 0.7705  acc: 0.4844  time: 7.3880  data: 0.0041  max mem: 45296
Evaluation  [80/82]  eta: 0:00:15  loss: 0.7133  acc: 0.5625  time: 7.9244  data: 0.0057  max mem: 45296
Evaluation  [81/82]  eta: 0:00:07  loss: 0.7589  acc: 0.5000  time: 7.6107  data: 0.0055  max mem: 45296
Evaluation Total time: 0:10:26 (7.6416 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.19681644439697266 uauc: 0.4909667417268372
2025-10-15 04:39:26,120 [INFO] Averaged stats: loss: 0.736820  acc: 0.530297 ***auc: 0.49978143111835843 ***uauc:(0.4909667417268372, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 12
1, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 2
64, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441,
444, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
 628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.60714286, 0.33333333, 0.58333333, 0.16666667,
       0.3       , 0.67346939, 0.47391304, 0.52380952, 0.3125    ,
       0.70833333, 0.75210084, 0.46031746, 0.5625    , 0.4375    ,
       0.671875  , 0.5       , 0.5       , 0.51111111, 0.45263158,
       0.54722222, 0.5       , 0.33333333, 1.        , 0.5       ,
       0.41319444, 0.20238095, 0.36666667, 0.3125    , 0.6       ,
       0.39583333, 0.33333333, 0.5       , 0.484375  , 0.5       ,
       0.59323116, 0.33531746, 0.41114983, 0.39583333, 0.75      ,
       0.375     , 0.        , 0.61615385, 0.        , 0.        ,
       0.        , 1.        , 0.47569444, 0.25      , 0.5       ,
       0.        , 0.6547619 , 0.54166667, 0.46604938, 0.575     ,
       0.41666667, 0.625     , 0.09821429, 0.64583333, 0.39030612,
       0.48214286, 0.35      , 0.49083333, 0.44099379, 0.45467033,
       0.        , 0.5       , 0.66666667, 1.        , 0.57826087,
       0.49242424, 0.5       , 0.1875    , 0.6025641 , 0.64019608,
       0.59375   , 0.54265873, 0.46052632, 0.52639752, 0.45833333,
       0.75      , 0.4537037 , 0.07142857, 0.33289242, 0.47222222,
       0.56928105, 0.55598958, 0.55392157, 0.45614035, 0.64117647,
       0.48739496, 0.57936508, 0.2875    , 0.625     , 1.        ,
       0.67708333, 1.        , 0.5       , 0.        , 0.6875    ,
       0.77083333, 0.25      , 0.45238095, 0.        , 1.        ,
       0.46840959, 0.4375    , 0.58333333, 0.275     , 0.        ,
       0.72      , 0.28703704, 0.5       , 0.5713073 , 0.13333333,
       0.75      , 0.36111111, 0.66666667, 1.        , 0.1       ,
       0.41541792, 0.85714286, 0.46176471, 0.5       , 0.62631579,
       0.85714286, 0.33333333, 0.16666667, 0.47455649, 0.375     ,
       0.5       , 0.71875   , 0.66666667, 0.2       , 0.56944444,
       0.75      , 0.5       , 0.375     , 0.61052632, 0.5       ,
       0.57142857, 0.5       , 0.875     , 0.57118056, 0.91666667,
       0.3877551 , 0.55      , 0.51111111, 0.44652406, 0.5       ,
       0.63888889, 0.5       , 0.375     , 0.72222222, 0.        ,
       0.67948718, 0.51857143, 0.75      , 0.70833333, 0.33333333,
       0.        , 0.64583333, 0.1875    , 1.        , 0.2627451 ,
       0.45787546, 0.625     , 0.43452381, 0.38888889, 0.63333333,
       0.49264706, 0.51010101, 0.44444444, 0.41840212, 0.16363636,
       0.62962963, 0.5       , 0.60852925, 0.75      , 1.        ,
       0.        , 0.58333333, 0.66666667, 0.38148148, 0.17045455,
       0.25      , 0.29861111, 0.625     , 0.3       , 0.81      ,
       0.5       , 0.        , 0.13636364, 0.5       , 0.516     ,
       0.        , 0.16666667, 0.66666667, 0.11111111, 0.42783816,
       0.6       , 0.61111111, 0.61176471, 0.53571429, 0.54411765,
       0.1875    , 0.65277778, 0.625     , 0.66666667, 0.51923077,
       1.        , 0.5       , 0.36666667, 0.359375  , 0.75      ,
       0.        , 0.51428571, 1.        , 0.        , 1.        ,
       0.51025641, 0.45408163, 0.51515152, 0.5125    , 0.        ,
       0.55555556, 0.33333333, 0.85714286, 0.        , 0.69230769,
       0.52083333, 1.        , 0.3504544 , 0.66666667, 0.31818182,
       0.8       , 0.56302521, 0.76666667, 0.47348485]))
rank_0 auc: 0.49978143111835843
2025-10-15 04:39:26,149 [INFO] Start training
2025-10-15 04:39:26,177 [INFO] Start training epoch 31, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
Train: data epoch: [31]  [ 0/50]  eta: 1:11:29  lr: 0.000947  loss: 0.6672  time: 85.7811  data: 0.0000  max mem: 45296
Train: data epoch: [31]  [49/50]  eta: 0:01:57  lr: 0.000943  loss: 0.9239  time: 112.5219  data: 0.0000  max mem: 45296
Train: data epoch: [31] Total time: 1:37:51 (117.4336 s / it)
2025-10-15 06:17:17,855 [INFO] Averaged stats: lr: 0.000945  loss: 0.725002
2025-10-15 06:17:17,857 [INFO] Evaluating on valid.
Evaluation  [ 0/82]  eta: 0:09:08  loss: 0.7296  acc: 0.5625  time: 6.6930  data: 0.0347  max mem: 45296
Evaluation  [16/82]  eta: 0:07:36  loss: 0.7838  acc: 0.5000  time: 6.9193  data: 0.0060  max mem: 45296
Evaluation  [32/82]  eta: 0:05:54  loss: 0.8917  acc: 0.3750  time: 7.3158  data: 0.0047  max mem: 45296
Evaluation  [48/82]  eta: 0:04:15  loss: 0.8242  acc: 0.4531  time: 7.9535  data: 0.0059  max mem: 45296
Evaluation  [64/82]  eta: 0:02:18  loss: 0.7973  acc: 0.4844  time: 8.0282  data: 0.0061  max mem: 45296
Evaluation  [80/82]  eta: 0:00:14  loss: 0.7298  acc: 0.5625  time: 7.3757  data: 0.0041  max mem: 45296
Evaluation  [81/82]  eta: 0:00:07  loss: 0.7835  acc: 0.5000  time: 6.9609  data: 0.0039  max mem: 45296
Evaluation Total time: 0:10:06 (7.4019 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.21363377571105957 uauc: 0.49707756043082335
2025-10-15 06:27:25,043 [INFO] Averaged stats: loss: 0.757495  acc: 0.530297 ***auc: 0.499158019807508 ***uauc:(0.49707756043082335, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121
, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 26
4, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 4
44, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627,
628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.59375   , 0.33333333, 0.49038462, 0.41666667,
       1.        , 0.63265306, 0.25652174, 0.6547619 , 0.125     ,
       0.53125   , 0.68697479, 0.38095238, 0.51848958, 0.75      ,
       0.70833333, 0.16666667, 0.83333333, 0.62777778, 0.48421053,
       0.55      , 0.        , 0.63333333, 0.        , 0.75      ,
       0.45833333, 0.23809524, 0.03333333, 0.34375   , 0.66666667,
       0.50694444, 0.08333333, 1.        , 0.4765625 , 0.5       ,
       0.56609195, 0.50992063, 0.31445993, 0.40277778, 0.75      ,
       0.125     , 0.        , 0.56076923, 0.        , 0.1       ,
       0.        , 1.        , 0.57638889, 0.5       , 1.        ,
       1.        , 0.64583333, 0.33333333, 0.39814815, 0.625     ,
       0.5       , 0.75      , 0.25892857, 0.61805556, 0.41836735,
       0.46428571, 0.55      , 0.4775    , 0.47360248, 0.53159341,
       1.        , 0.375     , 0.5       , 0.75      , 0.59565217,
       0.37878788, 0.83333333, 0.375     , 0.606466  , 0.60588235,
       0.75      , 0.53373016, 0.41578947, 0.53540373, 0.25      ,
       0.27777778, 0.50555556, 0.07142857, 0.33201058, 0.44791667,
       0.57222222, 0.6328125 , 0.45833333, 0.42982456, 0.58235294,
       0.42436975, 0.69047619, 0.25      , 0.5       , 0.        ,
       0.48958333, 1.        , 0.4       , 0.        , 0.5625    ,
       0.70833333, 0.5       , 0.61904762, 0.        , 0.5       ,
       0.50054466, 0.5625    , 0.16666667, 0.475     , 1.        ,
       0.5       , 0.24537037, 0.5       , 0.50933786, 0.        ,
       0.875     , 0.43055556, 0.66666667, 0.5       , 0.26666667,
       0.43489843, 0.85714286, 0.49215686, 1.        , 0.62982456,
       0.71428571, 0.33333333, 0.08333333, 0.44094304, 0.33695652,
       0.        , 0.69791667, 0.5       , 0.5       , 0.46180556,
       0.5       , 0.5       , 0.3125    , 0.66315789, 1.        ,
       0.57142857, 0.75      , 0.875     , 0.58767361, 0.75      ,
       0.29591837, 0.25      , 0.44444444, 0.43582888, 0.5       ,
       0.63888889, 1.        , 0.125     , 0.53333333, 0.5       ,
       0.69230769, 0.52952381, 0.84375   , 0.20833333, 0.33333333,
       0.        , 0.625     , 0.21875   , 0.75      , 0.22352941,
       0.47008547, 0.625     , 0.48809524, 0.05555556, 0.81666667,
       0.48529412, 0.52020202, 0.75      , 0.47261138, 0.2       ,
       0.51851852, 0.875     , 0.53444505, 0.8125    , 1.        ,
       0.75      , 0.54166667, 0.83333333, 0.45555556, 0.11363636,
       0.5       , 0.45833333, 1.        , 0.4       , 0.79      ,
       0.66666667, 0.        , 0.22727273, 0.75      , 0.448     ,
       0.5       , 0.        , 0.5       , 0.5       , 0.42225242,
       0.32      , 0.58918129, 0.61302521, 0.58333333, 0.11764706,
       0.03125   , 0.69444444, 0.        , 0.33333333, 0.55769231,
       1.        , 0.66666667, 0.13333333, 0.375     , 0.5       ,
       0.75      , 0.39285714, 1.        , 0.        , 0.5       ,
       0.50512821, 0.43367347, 0.65151515, 0.525     , 0.        ,
       0.41666667, 0.75      , 0.5       , 0.        , 0.76923077,
       0.52256944, 0.91666667, 0.35029211, 0.33333333, 0.45454545,
       0.71666667, 0.68907563, 0.53333333, 0.4157197 ]))
rank_0 auc: 0.499158019807508
2025-10-15 06:27:25,061 [INFO] Early stop. The results has not changed up to 20 epochs.
2025-10-15 06:27:25,063 [INFO] Training time 2 days, 8:24:11
