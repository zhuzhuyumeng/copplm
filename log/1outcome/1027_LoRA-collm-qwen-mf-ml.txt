W1115 14:46:38.317870 39852 site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
2025-11-15 14:46:38,338 [INFO] Building datasets...
2025-11-15 14:46:38,535 [INFO] Movie OOD datasets, max history length:10
2025-11-15 14:46:38,568 [INFO] Movie OOD datasets, max history length:10
2025-11-15 14:46:38,727 [INFO] Movie OOD datasets, max history length:10
2025-11-15 14:46:38,869 [INFO] 
=====  Running Parameters    =====
2025-11-15 14:46:38,871 [INFO] {
    "amp": true,
    "batch_size_eval": 64,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_url": "env://",
    "distributed": false,
    "evaluate": false,
    "init_lr": 0.0001,
    "iters_per_epoch": 50,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 1000,
    "min_lr": 8e-05,
    "mode": "v2",
    "num_workers": 0,
    "output_dir": "Qwen/Qwen2.5-1.5rec_log/collm",
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "rec_pretrain",
    "test_splits": [
        "test",
        "valid",
        "test_warm",
        "test_cold"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "valid"
    ],
    "warmup_lr": 1e-05,
    "warmup_steps": 200,
    "weight_decay": 0.001,
    "world_size": 1
}
2025-11-15 14:46:38,871 [INFO] 
======  Dataset Attributes  ======
2025-11-15 14:46:38,871 [INFO] 
======== amazon_ood =======
2025-11-15 14:46:38,871 [INFO] {
    "build_info": {
        "storage": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
    },
    "data_type": "default",
    "path": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
}
2025-11-15 14:46:38,871 [INFO] 
======  Model Attributes  ======
2025-11-15 14:46:38,871 [INFO] {
    "ans_type": "v2",
    "arch": "mini_gpt4rec_v2",
    "ckpt": "minigpt4/Qwen/Qwen2.5-1.5rec_log/collm/20251112212_best_tallrec/checkpoint_best.pth",
    "end_sym": "###",
    "freeze_lora": true,
    "freeze_proj": false,
    "freeze_rec": true,
    "item_num": -100,
    "llama_model": "Qwen/Qwen2-1.5B",
    "lora_config": {
        "alpha": 16,
        "dropout": 0.05,
        "r": 8,
        "target_modules": [
            "q_proj",
            "v_proj"
        ],
        "use_lora": true
    },
    "max_txt_len": 1024,
    "model_type": "pretrain_vicuna",
    "proj_drop": 0,
    "proj_mid_times": 10,
    "proj_token_num": 1,
    "prompt_path": "prompts/collm_movie.txt",
    "prompt_template": "{}",
    "rec_config": {
        "embedding_size": 256,
        "item_num": 3256,
        "pretrained_path": "collm-trained-models/my-collm-trained-models/mf_0912_ml1m_oodv2_best_model_d256lr-0.001wd0.0001.pth",
        "user_num": 839
    },
    "rec_model": "MF",
    "user_num": -100
}
2025-11-15 14:46:38,883 [INFO] freeze rec encoder
`torch_dtype` is deprecated! Use `dtype` instead!

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
binary_path: D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll
CUDA SETUP: Loading binary D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll...
Not using distributed mode
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\train data size: (33891, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\valid_small data size: (5200, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\test data size: (7331, 7)
Movie OOD datasets, max history length: 10
data dir: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\
正在计算全局流行度以进行偏见评估...
已计算 3087 个物品的流行度。总物品数为 3256。
正在将流行度数据注入评估任务...
runing MiniGPT4Rec_v2 ...... 
Loading Rec_model
### rec_encoder: MF
creat MF model, user num: 839 item num: 3256
successfully load the pretrained model......
freeze rec encoder
Loading Rec_model Done
Loading LLama model: Qwen/Qwen2-1.5B
Loading LLAMA Done
Setting Lora
Setting Lora Done
freeze lora...
type: <class 'int'> 10
Load 4 training prompts
Prompt List: 
['#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:']
Load MiniGPT4Rec Checkpoint: minigpt4/Qwen/Qwen2.5-1.5rec_log/collm/20251112212_best_tallrec/checkpoint_best.pth
loading message, msg.... 
 _IncompatibleKeys(missing_keys=['rec_encoder.user_embedding.weight', 'rec_encoder.item_embedding.weight', 'llama_model.base_model.model.model.embed_tokens.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.bias', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.bias', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.bias', 'llama_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model.base_model.model.model.norm.weight', 'llama_proj.0.weight', 'llama_proj.0.bias', 'llama_proj.2.weight', 'llama_proj.2.bias'], unexpected_keys=[])2025-11-15 14:46:40,657 [INFO] Start training
2025-11-15 14:46:40,663 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2025-11-15 14:46:40,663 [INFO] Loaded 33891 records for train split from the dataset.
2025-11-15 14:46:40,663 [INFO] Loaded 5200 records for valid split from the dataset.
2025-11-15 14:46:40,663 [INFO] Loaded 7331 records for test split from the dataset.
2025-11-15 14:46:40,668 [INFO] number of trainable parameters: 4591616
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\runners\runner_base.py:153: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
2025-11-15 14:46:40,670 [INFO] Start training epoch 0, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 14:47:07,846 [INFO] Averaged stats: lr: 0.000021  loss: 0.587929
2025-11-15 14:47:07,848 [INFO] Evaluating on valid.
2025-11-15 14:50:15,305 [INFO] Averaged stats: loss: 0.630431  acc: 0.649581 ***auc: 0.7044672539418496 ***uauc: 0.6624539416676539 ***u-nDCG: 0.8559667718855826
2025-11-15 14:50:15,311 [INFO] Saving checkpoint at epoch 0 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 14:50:15,761 [INFO] Start training
2025-11-15 14:50:15,767 [INFO] Start training epoch 1, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 14:50:42,632 [INFO] Averaged stats: lr: 0.000021  loss: 0.542305
2025-11-15 14:50:42,634 [INFO] Evaluating on valid.
2025-11-15 14:54:07,601 [INFO] Averaged stats: loss: 0.626882  acc: 0.649009 ***auc: 0.7114944959099259 ***uauc: 0.6663273140170625 ***u-nDCG: 0.8605113088932743
2025-11-15 14:54:07,607 [INFO] Saving checkpoint at epoch 1 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 14:54:08,090 [INFO] Start training
2025-11-15 14:54:08,097 [INFO] Start training epoch 2, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 14:54:34,792 [INFO] Averaged stats: lr: 0.000021  loss: 0.489454
2025-11-15 14:54:34,794 [INFO] Evaluating on valid.
2025-11-15 14:57:43,991 [INFO] Averaged stats: loss: 0.621278  acc: 0.646532 ***auc: 0.7169970268989694 ***uauc: 0.6689817638870025 ***u-nDCG: 0.8611125592519648
2025-11-15 14:57:43,997 [INFO] Saving checkpoint at epoch 2 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 14:57:44,490 [INFO] Start training
2025-11-15 14:57:44,497 [INFO] Start training epoch 3, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 14:58:11,098 [INFO] Averaged stats: lr: 0.000021  loss: 0.508297
2025-11-15 14:58:11,100 [INFO] Evaluating on valid.
2025-11-15 15:01:16,846 [INFO] Averaged stats: loss: 0.623193  acc: 0.629192 ***auc: 0.7158090783000746 ***uauc: 0.6842069992544951 ***u-nDCG: 0.8677231713929459
2025-11-15 15:01:16,852 [INFO] Start training
2025-11-15 15:01:16,859 [INFO] Start training epoch 4, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:01:43,460 [INFO] Averaged stats: lr: 0.000100  loss: 0.571549
2025-11-15 15:01:43,461 [INFO] Evaluating on valid.
2025-11-15 15:04:48,593 [INFO] Averaged stats: loss: 0.625130  acc: 0.614710 ***auc: 0.7187740869366636 ***uauc: 0.6803468834408417 ***u-nDCG: 0.8661646346225886
2025-11-15 15:04:48,599 [INFO] Saving checkpoint at epoch 4 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 15:04:49,086 [INFO] Start training
2025-11-15 15:04:49,092 [INFO] Start training epoch 5, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:05:15,884 [INFO] Averaged stats: lr: 0.000100  loss: 0.508227
2025-11-15 15:05:15,887 [INFO] Evaluating on valid.
2025-11-15 15:08:27,620 [INFO] Averaged stats: loss: 0.614391  acc: 0.626905 ***auc: 0.72105064814137 ***uauc: 0.6776534460071385 ***u-nDCG: 0.8666406508678144
2025-11-15 15:08:27,627 [INFO] Saving checkpoint at epoch 5 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 15:08:28,130 [INFO] Start training
2025-11-15 15:08:28,136 [INFO] Start training epoch 6, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:08:54,839 [INFO] Averaged stats: lr: 0.000100  loss: 0.498907
2025-11-15 15:08:54,840 [INFO] Evaluating on valid.
2025-11-15 15:12:10,474 [INFO] Averaged stats: loss: 0.620959  acc: 0.623285 ***auc: 0.7154136646235723 ***uauc: 0.6690616549049787 ***u-nDCG: 0.8610790815888205
2025-11-15 15:12:10,481 [INFO] Start training
2025-11-15 15:12:10,487 [INFO] Start training epoch 7, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:12:37,010 [INFO] Averaged stats: lr: 0.000100  loss: 0.469013
2025-11-15 15:12:37,011 [INFO] Evaluating on valid.
2025-11-15 15:15:41,271 [INFO] Averaged stats: loss: 0.626121  acc: 0.616044 ***auc: 0.7131246307381199 ***uauc: 0.6769367171222331 ***u-nDCG: 0.8648822983329004
2025-11-15 15:15:41,278 [INFO] Start training
2025-11-15 15:15:41,284 [INFO] Start training epoch 8, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:16:08,474 [INFO] Averaged stats: lr: 0.000100  loss: 0.461404
2025-11-15 15:16:08,475 [INFO] Evaluating on valid.
2025-11-15 15:19:39,175 [INFO] Averaged stats: loss: 0.644950  acc: 0.609375 ***auc: 0.7237388078106897 ***uauc: 0.6750504311165556 ***u-nDCG: 0.8641976016802029
2025-11-15 15:19:39,186 [INFO] Saving checkpoint at epoch 8 to D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\Qwen\Qwen2.5-1.5rec_log\collm\20251115144\checkpoint_best.pth.
2025-11-15 15:19:40,326 [INFO] Start training
2025-11-15 15:19:40,353 [INFO] Start training epoch 9, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:20:11,174 [INFO] Averaged stats: lr: 0.000100  loss: 0.446753
2025-11-15 15:20:11,175 [INFO] Evaluating on valid.

answer token ids: pos: 9454 neg ids: 2753
Prompt Pos Example 
#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user's preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \n#Answer: Yes or No
llama_proj.0.weight
llama_proj.0.bias
llama_proj.2.weight
llama_proj.2.bias
prompt example: <s>#Question: A user has given high ratings to the following movies: "Best in Show (2000)", "High Fidelity (2000)", "Bring It On (2000)", "28 Days (2000)", "Perfect Storm, The (2000)", "Return to Me (2000)", "Thomas Crown Affair, The (1999)". Additionally, we have information about the user's preferences encoded in the feature <unk>. Using all available information, make a prediction about whether the user would enjoy the movie titled "My Dog Skip (1999)" with the feature <unk>? Answer with "Yes" or "No". \n#Answer:
#######prmpt decoded example:  <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <s ># Question :  A  user  has  given  high  ratings  to  the  following  movies :  " Dave  ( 1 9 9 3 )",  " Add ams  Family ,  The  ( 1 9 9 1 )",  " Ham let  ( 1 9 9 0 )",  " Cour age  Under  Fire  ( 1 9 9 6 )",  " P ump  Up  the  Volume  ( 1 9 9 0 )",  " B ul worth  ( 1 9 9 8 )",  " M ight y  Aph rod ite  ( 1 9 9 5 )",  " Four  Wed dings  and  a  Funeral  ( 1 9 9 4 )",  " In  the  Name  of  the  Father  ( 1 9 9 3 )",  " S cream  ( 1 9 9 6 )".  Additionally ,  we  have  information  about  the  user 's  preferences  encoded  in  the  feature   <unk> .  Using  all  available  information ,  make  a  prediction  about  whether  the  user  would  enjoy  the  movie  titled  " Last  Sup per ,  The  ( 1 9 9 5 )"  with  the  feature   <unk> ?  Answer  with  " Yes "  or  " No ".  \ n # Answer :
Train: data epoch: [0]  [ 0/50]  eta: 0:00:40  lr: 0.000010  loss: 0.7437  time: 0.8014  data: 0.0000  max mem: 18557
Train: data epoch: [0]  [49/50]  eta: 0:00:00  lr: 0.000032  loss: 0.5420  time: 0.5369  data: 0.0000  max mem: 21199
Train: data epoch: [0] Total time: 0:00:27 (0.5435 s / it)
Evaluation  [ 0/82]  eta: 0:02:06  loss: 0.6120  acc: 0.6719  time: 1.5464  data: 0.0061  max mem: 24751
Evaluation  [16/82]  eta: 0:02:22  loss: 0.7171  acc: 0.5469  time: 2.1634  data: 0.0030  max mem: 29510
Evaluation  [32/82]  eta: 0:01:51  loss: 0.6756  acc: 0.6094  time: 2.2782  data: 0.0024  max mem: 30553
Evaluation  [48/82]  eta: 0:01:17  loss: 0.7046  acc: 0.6250  time: 2.3378  data: 0.0025  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6668  acc: 0.6250  time: 2.4136  data: 0.0028  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6325  acc: 0.6094  time: 2.2741  data: 0.0023  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4394  acc: 0.7500  time: 2.1819  data: 0.0021  max mem: 31643
Evaluation Total time: 0:03:07 (2.2849 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07877016067504883 uauc: 0.6624539416676539
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0027043819427490234 u-nDCG: 0.8559667718855826
rank_0 auc: 0.7044672539418496
Train: data epoch: [1]  [ 0/50]  eta: 0:00:24  lr: 0.000010  loss: 0.6933  time: 0.4939  data: 0.0000  max mem: 31643
Train: data epoch: [1]  [49/50]  eta: 0:00:00  lr: 0.000032  loss: 0.3142  time: 0.5314  data: 0.0000  max mem: 31643
Train: data epoch: [1] Total time: 0:00:26 (0.5373 s / it)
Evaluation  [ 0/82]  eta: 0:01:54  loss: 0.6079  acc: 0.6562  time: 1.3991  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:28  loss: 0.7258  acc: 0.5469  time: 2.2481  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:02:00  loss: 0.6374  acc: 0.6406  time: 2.5720  data: 0.0033  max mem: 31643
Evaluation  [48/82]  eta: 0:01:24  loss: 0.6991  acc: 0.6094  time: 2.6153  data: 0.0045  max mem: 31643
Evaluation  [64/82]  eta: 0:00:46  loss: 0.6553  acc: 0.6406  time: 2.6993  data: 0.0050  max mem: 31643
Evaluation  [80/82]  eta: 0:00:05  loss: 0.6496  acc: 0.5469  time: 2.4578  data: 0.0049  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4445  acc: 0.7500  time: 2.3541  data: 0.0047  max mem: 31643
Evaluation Total time: 0:03:24 (2.4984 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.08162784576416016 uauc: 0.6663273140170625
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030183792114257812 u-nDCG: 0.8605113088932743
rank_0 auc: 0.7114944959099259
Train: data epoch: [2]  [ 0/50]  eta: 0:00:25  lr: 0.000010  loss: 0.6316  time: 0.5018  data: 0.0000  max mem: 31643
Train: data epoch: [2]  [49/50]  eta: 0:00:00  lr: 0.000032  loss: 0.4205  time: 0.5341  data: 0.0000  max mem: 31643
Train: data epoch: [2] Total time: 0:00:26 (0.5339 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.6270  acc: 0.6250  time: 1.4110  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:24  loss: 0.6838  acc: 0.5625  time: 2.1913  data: 0.0026  max mem: 31643
Evaluation  [32/82]  eta: 0:01:54  loss: 0.6240  acc: 0.6719  time: 2.3890  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6753  acc: 0.5938  time: 2.3399  data: 0.0027  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.6544  acc: 0.6719  time: 2.3520  data: 0.0027  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6186  acc: 0.6562  time: 2.3241  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4233  acc: 0.7500  time: 2.2312  data: 0.0025  max mem: 31643
Evaluation Total time: 0:03:09 (2.3062 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07875514030456543 uauc: 0.6689817638870025
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030601024627685547 u-nDCG: 0.8611125592519648
rank_0 auc: 0.7169970268989694
Train: data epoch: [3]  [ 0/50]  eta: 0:00:25  lr: 0.000010  loss: 0.6234  time: 0.5137  data: 0.0000  max mem: 31643
Train: data epoch: [3]  [49/50]  eta: 0:00:00  lr: 0.000032  loss: 0.5963  time: 0.5370  data: 0.0000  max mem: 31643
Train: data epoch: [3] Total time: 0:00:26 (0.5320 s / it)
Evaluation  [ 0/82]  eta: 0:01:53  loss: 0.6328  acc: 0.6406  time: 1.3825  data: 0.0062  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.7095  acc: 0.5781  time: 2.1521  data: 0.0030  max mem: 31643
Evaluation  [32/82]  eta: 0:01:52  loss: 0.6038  acc: 0.6719  time: 2.3369  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.6540  acc: 0.5625  time: 2.3702  data: 0.0028  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6312  acc: 0.6562  time: 2.3005  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6056  acc: 0.6562  time: 2.2617  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5162  acc: 0.7500  time: 2.1710  data: 0.0025  max mem: 31643
Evaluation Total time: 0:03:05 (2.2641 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07709741592407227 uauc: 0.6842069992544951
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.003085613250732422 u-nDCG: 0.8677231713929459
rank_0 auc: 0.7158090783000746
Train: data epoch: [4]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.6055  time: 0.5014  data: 0.0000  max mem: 31643
Train: data epoch: [4]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.7297  time: 0.5392  data: 0.0000  max mem: 31643
2025-11-15 15:23:31,658 [INFO] Averaged stats: loss: 0.638762  acc: 0.608232 ***auc: 0.7196292822313746 ***uauc: 0.6761268821374764 ***u-nDCG: 0.8674303563953786
2025-11-15 15:23:31,665 [INFO] Start training
2025-11-15 15:23:31,670 [INFO] Start training epoch 10, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:23:58,137 [INFO] Averaged stats: lr: 0.000100  loss: 0.428401
2025-11-15 15:23:58,139 [INFO] Evaluating on valid.
2025-11-15 15:27:11,913 [INFO] Averaged stats: loss: 0.664180  acc: 0.658918 ***auc: 0.7069987626803731 ***uauc: 0.6725467423475943 ***u-nDCG: 0.86158277217058
2025-11-15 15:27:11,928 [INFO] Start training
2025-11-15 15:27:11,942 [INFO] Start training epoch 11, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:27:40,075 [INFO] Averaged stats: lr: 0.000100  loss: 0.403642
2025-11-15 15:27:40,076 [INFO] Evaluating on valid.
2025-11-15 15:30:47,812 [INFO] Averaged stats: loss: 0.686842  acc: 0.657965 ***auc: 0.7071533348310449 ***uauc: 0.6539615296776385 ***u-nDCG: 0.8567324242754342
2025-11-15 15:30:47,818 [INFO] Start training
2025-11-15 15:30:47,826 [INFO] Start training epoch 12, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:31:14,808 [INFO] Averaged stats: lr: 0.000100  loss: 0.415721
2025-11-15 15:31:14,809 [INFO] Evaluating on valid.
2025-11-15 15:34:19,744 [INFO] Averaged stats: loss: 0.630259  acc: 0.641197 ***auc: 0.7167606399019766 ***uauc: 0.6844975100154427 ***u-nDCG: 0.8696405253072145
2025-11-15 15:34:19,751 [INFO] Start training
2025-11-15 15:34:19,758 [INFO] Start training epoch 13, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:34:49,475 [INFO] Averaged stats: lr: 0.000100  loss: 0.470618
2025-11-15 15:34:49,480 [INFO] Evaluating on valid.
2025-11-15 15:37:58,195 [INFO] Averaged stats: loss: 0.649438  acc: 0.663491 ***auc: 0.713924589875242 ***uauc: 0.6525678902133085 ***u-nDCG: 0.8546736169631346
2025-11-15 15:37:58,201 [INFO] Start training
2025-11-15 15:37:58,207 [INFO] Start training epoch 14, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:38:24,779 [INFO] Averaged stats: lr: 0.000100  loss: 0.419414
2025-11-15 15:38:24,780 [INFO] Evaluating on valid.
2025-11-15 15:41:37,759 [INFO] Averaged stats: loss: 0.646007  acc: 0.664444 ***auc: 0.7169309513878753 ***uauc: 0.667586996694437 ***u-nDCG: 0.8596496630208124
2025-11-15 15:41:37,792 [INFO] Start training
2025-11-15 15:41:37,811 [INFO] Start training epoch 15, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:42:06,023 [INFO] Averaged stats: lr: 0.000100  loss: 0.421999
2025-11-15 15:42:06,023 [INFO] Evaluating on valid.
Train: data epoch: [4] Total time: 0:00:26 (0.5320 s / it)
Evaluation  [ 0/82]  eta: 0:02:05  loss: 0.6446  acc: 0.5781  time: 1.5345  data: 0.0045  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.6758  acc: 0.6719  time: 2.1522  data: 0.0026  max mem: 31643
Evaluation  [32/82]  eta: 0:01:50  loss: 0.6303  acc: 0.6406  time: 2.2744  data: 0.0028  max mem: 31643
Evaluation  [48/82]  eta: 0:01:16  loss: 0.6708  acc: 0.5781  time: 2.3012  data: 0.0028  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6590  acc: 0.6406  time: 2.3186  data: 0.0025  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.5891  acc: 0.6250  time: 2.2879  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4420  acc: 0.8125  time: 2.1970  data: 0.0023  max mem: 31643
Evaluation Total time: 0:03:05 (2.2566 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0762779712677002 uauc: 0.6803468834408417
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030689239501953125 u-nDCG: 0.8661646346225886
rank_0 auc: 0.7187740869366636
Train: data epoch: [5]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.8900  time: 0.5230  data: 0.0000  max mem: 31643
Train: data epoch: [5]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4644  time: 0.5344  data: 0.0000  max mem: 31643
Train: data epoch: [5] Total time: 0:00:26 (0.5358 s / it)
Evaluation  [ 0/82]  eta: 0:01:54  loss: 0.6456  acc: 0.6250  time: 1.3952  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:27  loss: 0.6781  acc: 0.5781  time: 2.2288  data: 0.0026  max mem: 31643
Evaluation  [32/82]  eta: 0:01:53  loss: 0.6389  acc: 0.6719  time: 2.3342  data: 0.0026  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6393  acc: 0.5469  time: 2.3257  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6449  acc: 0.6250  time: 2.3143  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6028  acc: 0.6406  time: 2.5306  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4989  acc: 0.7500  time: 2.4400  data: 0.0025  max mem: 31643
Evaluation Total time: 0:03:11 (2.3371 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07840871810913086 uauc: 0.6776534460071385
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030329227447509766 u-nDCG: 0.8666406508678144
rank_0 auc: 0.72105064814137
Train: data epoch: [6]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.4522  time: 0.5244  data: 0.0000  max mem: 31643
Train: data epoch: [6]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4051  time: 0.5283  data: 0.0000  max mem: 31643
Train: data epoch: [6] Total time: 0:00:26 (0.5341 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.6248  acc: 0.5938  time: 1.4075  data: 0.0057  max mem: 31643
Evaluation  [16/82]  eta: 0:02:29  loss: 0.6090  acc: 0.7188  time: 2.2712  data: 0.0038  max mem: 31643
Evaluation  [32/82]  eta: 0:02:04  loss: 0.6147  acc: 0.7031  time: 2.6867  data: 0.0047  max mem: 31643
Evaluation  [48/82]  eta: 0:01:23  loss: 0.7021  acc: 0.5469  time: 2.4129  data: 0.0031  max mem: 31643
Evaluation  [64/82]  eta: 0:00:43  loss: 0.6450  acc: 0.6250  time: 2.3149  data: 0.0027  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.5453  acc: 0.6406  time: 2.3115  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4541  acc: 0.8125  time: 2.2238  data: 0.0025  max mem: 31643
Evaluation Total time: 0:03:15 (2.3827 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.24146080017089844 uauc: 0.6690616549049787
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.001554250717163086 u-nDCG: 0.8610790815888205
rank_0 auc: 0.7154136646235723
Train: data epoch: [7]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.7777  time: 0.5062  data: 0.0000  max mem: 31643
Train: data epoch: [7]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.5189  time: 0.5309  data: 0.0000  max mem: 31643
Train: data epoch: [7] Total time: 0:00:26 (0.5305 s / it)
Evaluation  [ 0/82]  eta: 0:01:52  loss: 0.6400  acc: 0.5938  time: 1.3690  data: 0.0057  max mem: 31643
Evaluation  [16/82]  eta: 0:02:21  loss: 0.6533  acc: 0.6562  time: 2.1407  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:50  loss: 0.6565  acc: 0.6875  time: 2.2766  data: 0.0024  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.6716  acc: 0.5781  time: 2.3382  data: 0.0027  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6506  acc: 0.6250  time: 2.2803  data: 0.0027  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.5913  acc: 0.6094  time: 2.2532  data: 0.0028  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5040  acc: 0.6875  time: 2.1620  data: 0.0026  max mem: 31643
Evaluation Total time: 0:03:04 (2.2460 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07515215873718262 uauc: 0.6769367171222331
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0026025772094726562 u-nDCG: 0.8648822983329004
rank_0 auc: 0.7131246307381199
Train: data epoch: [8]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.4155  time: 0.5060  data: 0.0000  max mem: 31643
Train: data epoch: [8]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.5569  time: 0.5431  data: 0.0000  max mem: 31643
Train: data epoch: [8] Total time: 0:00:27 (0.5438 s / it)
Evaluation  [ 0/82]  eta: 0:02:05  loss: 0.6852  acc: 0.5938  time: 1.5363  data: 0.0055  max mem: 31643
Evaluation  [16/82]  eta: 0:02:29  loss: 0.7161  acc: 0.6406  time: 2.2659  data: 0.0045  max mem: 31643
Evaluation  [32/82]  eta: 0:02:02  loss: 0.5988  acc: 0.7031  time: 2.6028  data: 0.0060  max mem: 31643
Evaluation  [48/82]  eta: 0:01:26  loss: 0.6530  acc: 0.5625  time: 2.7084  data: 0.0079  max mem: 31643
Evaluation  [64/82]  eta: 0:00:46  loss: 0.6679  acc: 0.5938  time: 2.7344  data: 0.0110  max mem: 31643
Evaluation  [80/82]  eta: 0:00:05  loss: 0.6759  acc: 0.5938  time: 2.6346  data: 0.0072  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5003  acc: 0.7500  time: 2.5265  data: 0.0070  max mem: 31643
Evaluation Total time: 0:03:30 (2.5663 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.22295188903808594 uauc: 0.6750504311165556
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.01059412956237793 u-nDCG: 0.8641976016802029
rank_0 auc: 0.7237388078106897
Train: data epoch: [9]  [ 0/50]  eta: 0:00:29  lr: 0.000100  loss: 0.4898  time: 0.5927  data: 0.0000  max mem: 31643
Train: data epoch: [9]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.5391  time: 0.6117  data: 0.0000  max mem: 31643
Train: data epoch: [9] Total time: 0:00:30 (0.6164 s / it)
Evaluation  [ 0/82]  eta: 0:02:25  loss: 0.6601  acc: 0.6406  time: 1.7783  data: 0.0122  max mem: 31643
Evaluation  [16/82]  eta: 0:02:46  loss: 0.6231  acc: 0.6406  time: 2.5176  data: 0.0087  max mem: 31643
Evaluation  [32/82]  eta: 0:02:02  loss: 0.5653  acc: 0.6562  time: 2.4272  data: 0.0030  max mem: 31643
Evaluation  [48/82]  eta: 0:01:22  loss: 0.6807  acc: 0.5938  time: 2.3395  data: 0.0022  max mem: 31643
Evaluation  [64/82]  eta: 0:00:44  loss: 0.6407  acc: 0.5781  time: 2.5548  data: 0.0038  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6643  acc: 0.5938  time: 2.4732  data: 0.0033  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4851  acc: 0.7500  time: 2.3732  data: 0.0031  max mem: 31643
Evaluation Total time: 0:03:20 (2.4436 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07936334609985352 uauc: 0.6761268821374764
only one interaction users (for nDCG): 48
2025-11-15 15:45:40,424 [INFO] Averaged stats: loss: 0.657958  acc: 0.620046 ***auc: 0.6995238405640978 ***uauc: 0.6559772508635612 ***u-nDCG: 0.8557780808181812
2025-11-15 15:45:40,440 [INFO] Start training
2025-11-15 15:45:40,457 [INFO] Start training epoch 16, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:46:10,334 [INFO] Averaged stats: lr: 0.000100  loss: 0.371432
2025-11-15 15:46:10,336 [INFO] Evaluating on valid.
2025-11-15 15:49:37,214 [INFO] Averaged stats: loss: 0.693634  acc: 0.650724 ***auc: 0.7040145995697816 ***uauc: 0.6591771241422075 ***u-nDCG: 0.8566891664927219
2025-11-15 15:49:37,221 [INFO] Start training
2025-11-15 15:49:37,228 [INFO] Start training epoch 17, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:50:03,940 [INFO] Averaged stats: lr: 0.000100  loss: 0.405796
2025-11-15 15:50:03,941 [INFO] Evaluating on valid.
2025-11-15 15:53:09,407 [INFO] Averaged stats: loss: 0.670662  acc: 0.654345 ***auc: 0.7035661770001095 ***uauc: 0.6564543750265125 ***u-nDCG: 0.8509318597326229
2025-11-15 15:53:09,413 [INFO] Start training
2025-11-15 15:53:09,421 [INFO] Start training epoch 18, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:53:36,358 [INFO] Averaged stats: lr: 0.000100  loss: 0.376275
2025-11-15 15:53:36,359 [INFO] Evaluating on valid.
2025-11-15 15:56:48,445 [INFO] Averaged stats: loss: 0.688502  acc: 0.656822 ***auc: 0.7095978317729366 ***uauc: 0.6544890271703095 ***u-nDCG: 0.8501229169225354
2025-11-15 15:56:48,451 [INFO] Start training
2025-11-15 15:56:48,458 [INFO] Start training epoch 19, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 15:57:15,136 [INFO] Averaged stats: lr: 0.000100  loss: 0.305119
2025-11-15 15:57:15,140 [INFO] Evaluating on valid.
2025-11-15 16:00:24,248 [INFO] Averaged stats: loss: 0.715048  acc: 0.643674 ***auc: 0.6896795545589949 ***uauc: 0.6451046020354924 ***u-nDCG: 0.8496175386689853
2025-11-15 16:00:24,254 [INFO] Start training
2025-11-15 16:00:24,262 [INFO] Start training epoch 20, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:00:51,123 [INFO] Averaged stats: lr: 0.000100  loss: 0.341655
2025-11-15 16:00:51,124 [INFO] Evaluating on valid.
2025-11-15 16:03:57,574 [INFO] Averaged stats: loss: 0.692953  acc: 0.606517 ***auc: 0.6875597927693791 ***uauc: 0.6341560494022422 ***u-nDCG: 0.8442830790348624
2025-11-15 16:03:57,580 [INFO] Start training
2025-11-15 16:03:57,586 [INFO] Start training epoch 21, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:04:24,462 [INFO] Averaged stats: lr: 0.000100  loss: 0.314258
2025-11-15 16:04:24,464 [INFO] Evaluating on valid.
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030434131622314453 u-nDCG: 0.8674303563953786
rank_0 auc: 0.7196292822313746
Train: data epoch: [10]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.5040  time: 0.5087  data: 0.0000  max mem: 31643
Train: data epoch: [10]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4529  time: 0.5334  data: 0.0000  max mem: 31643
Train: data epoch: [10] Total time: 0:00:26 (0.5293 s / it)
Evaluation  [ 0/82]  eta: 0:01:53  loss: 0.6308  acc: 0.7031  time: 1.3802  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:26  loss: 0.7908  acc: 0.5469  time: 2.2226  data: 0.0029  max mem: 31643
Evaluation  [32/82]  eta: 0:01:53  loss: 0.6388  acc: 0.7031  time: 2.2997  data: 0.0028  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6399  acc: 0.6719  time: 2.3672  data: 0.0025  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.6517  acc: 0.6094  time: 2.3947  data: 0.0027  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7588  acc: 0.5469  time: 2.5539  data: 0.0042  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3091  acc: 0.8750  time: 2.4624  data: 0.0041  max mem: 31643
Evaluation Total time: 0:03:13 (2.3599 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.23671936988830566 uauc: 0.6725467423475943
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.009127616882324219 u-nDCG: 0.86158277217058
rank_0 auc: 0.7069987626803731
Train: data epoch: [11]  [ 0/50]  eta: 0:00:29  lr: 0.000100  loss: 0.3456  time: 0.5913  data: 0.0000  max mem: 31643
Train: data epoch: [11]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3642  time: 0.5408  data: 0.0000  max mem: 31643
Train: data epoch: [11] Total time: 0:00:28 (0.5627 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.6587  acc: 0.7188  time: 1.4080  data: 0.0060  max mem: 31643
Evaluation  [16/82]  eta: 0:02:23  loss: 0.8399  acc: 0.6094  time: 2.1692  data: 0.0029  max mem: 31643
Evaluation  [32/82]  eta: 0:01:53  loss: 0.6150  acc: 0.6875  time: 2.3624  data: 0.0025  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6926  acc: 0.6250  time: 2.3572  data: 0.0027  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6831  acc: 0.5938  time: 2.3274  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7826  acc: 0.5625  time: 2.3087  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4667  acc: 0.8125  time: 2.2163  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:07 (2.2884 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07748866081237793 uauc: 0.6539615296776385
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030679702758789062 u-nDCG: 0.8567324242754342
rank_0 auc: 0.7071533348310449
Train: data epoch: [12]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.5868  time: 0.5229  data: 0.0000  max mem: 31643
Train: data epoch: [12]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4764  time: 0.5407  data: 0.0000  max mem: 31643
Train: data epoch: [12] Total time: 0:00:26 (0.5396 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.6169  acc: 0.6719  time: 1.4094  data: 0.0061  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.6315  acc: 0.6406  time: 2.1545  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:51  loss: 0.6199  acc: 0.6875  time: 2.3085  data: 0.0026  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.7068  acc: 0.5469  time: 2.3351  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6180  acc: 0.6406  time: 2.3012  data: 0.0027  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6239  acc: 0.6250  time: 2.2711  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5082  acc: 0.6875  time: 2.1782  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:04 (2.2541 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.0819559097290039 uauc: 0.6844975100154427
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030317306518554688 u-nDCG: 0.8696405253072145
rank_0 auc: 0.7167606399019766
Train: data epoch: [13]  [ 0/50]  eta: 0:00:27  lr: 0.000100  loss: 0.4558  time: 0.5495  data: 0.0000  max mem: 31643
Train: data epoch: [13]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4116  time: 0.5935  data: 0.0000  max mem: 31643
Train: data epoch: [13] Total time: 0:00:29 (0.5943 s / it)
Evaluation  [ 0/82]  eta: 0:02:28  loss: 0.6178  acc: 0.7031  time: 1.8121  data: 0.0106  max mem: 31643
Evaluation  [16/82]  eta: 0:02:29  loss: 0.7087  acc: 0.5938  time: 2.2633  data: 0.0037  max mem: 31643
Evaluation  [32/82]  eta: 0:01:54  loss: 0.6160  acc: 0.6875  time: 2.3233  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:19  loss: 0.6885  acc: 0.6562  time: 2.3376  data: 0.0029  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.6570  acc: 0.6562  time: 2.3749  data: 0.0029  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7374  acc: 0.5625  time: 2.2649  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5537  acc: 0.7500  time: 2.1721  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:08 (2.3001 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07647156715393066 uauc: 0.6525678902133085
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.002521991729736328 u-nDCG: 0.8546736169631346
rank_0 auc: 0.713924589875242
Train: data epoch: [14]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.4001  time: 0.5066  data: 0.0000  max mem: 31643
Train: data epoch: [14]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.5568  time: 0.5259  data: 0.0000  max mem: 31643
Train: data epoch: [14] Total time: 0:00:26 (0.5314 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.6286  acc: 0.6719  time: 1.4065  data: 0.0061  max mem: 31643
Evaluation  [16/82]  eta: 0:02:28  loss: 0.6882  acc: 0.6094  time: 2.2569  data: 0.0029  max mem: 31643
Evaluation  [32/82]  eta: 0:01:55  loss: 0.6727  acc: 0.6406  time: 2.4190  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:19  loss: 0.6895  acc: 0.7031  time: 2.3643  data: 0.0025  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.6277  acc: 0.7031  time: 2.3741  data: 0.0028  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6660  acc: 0.6250  time: 2.4539  data: 0.0031  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4239  acc: 0.8125  time: 2.3659  data: 0.0031  max mem: 31643
Evaluation Total time: 0:03:12 (2.3494 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.28319430351257324 uauc: 0.667586996694437
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.021231889724731445 u-nDCG: 0.8596496630208124
rank_0 auc: 0.7169309513878753
Train: data epoch: [15]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.4211  time: 0.5273  data: 0.0000  max mem: 31643
Train: data epoch: [15]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3443  time: 0.5364  data: 0.0000  max mem: 31643
Train: data epoch: [15] Total time: 0:00:28 (0.5642 s / it)
Evaluation  [ 0/82]  eta: 0:01:57  loss: 0.6573  acc: 0.5938  time: 1.4375  data: 0.0045  max mem: 31643
Evaluation  [16/82]  eta: 0:02:48  loss: 0.7303  acc: 0.5938  time: 2.5468  data: 0.0069  max mem: 31643
Evaluation  [32/82]  eta: 0:02:12  loss: 0.5683  acc: 0.6719  time: 2.7436  data: 0.0068  max mem: 31643
Evaluation  [48/82]  eta: 0:01:30  loss: 0.6172  acc: 0.6406  time: 2.6082  data: 0.0050  max mem: 31643
Evaluation  [64/82]  eta: 0:00:47  loss: 0.6480  acc: 0.6562  time: 2.5993  data: 0.0049  max mem: 31643
2025-11-15 16:07:33,489 [INFO] Averaged stats: loss: 0.734823  acc: 0.624047 ***auc: 0.681407568749342 ***uauc: 0.6388071583881134 ***u-nDCG: 0.8470066330981613
2025-11-15 16:07:33,496 [INFO] Start training
2025-11-15 16:07:33,501 [INFO] Start training epoch 22, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:08:00,377 [INFO] Averaged stats: lr: 0.000100  loss: 0.363929
2025-11-15 16:08:00,379 [INFO] Evaluating on valid.
2025-11-15 16:11:09,526 [INFO] Averaged stats: loss: 0.733141  acc: 0.619284 ***auc: 0.6723560403930735 ***uauc: 0.637052509782979 ***u-nDCG: 0.8502172006916532
2025-11-15 16:11:09,532 [INFO] Start training
2025-11-15 16:11:09,539 [INFO] Start training epoch 23, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:11:36,384 [INFO] Averaged stats: lr: 0.000100  loss: 0.266980
2025-11-15 16:11:36,386 [INFO] Evaluating on valid.
2025-11-15 16:14:46,355 [INFO] Averaged stats: loss: 0.786479  acc: 0.645008 ***auc: 0.6938889358264242 ***uauc: 0.6385557629388454 ***u-nDCG: 0.8538759418629558
2025-11-15 16:14:46,362 [INFO] Start training
2025-11-15 16:14:46,368 [INFO] Start training epoch 24, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:15:13,008 [INFO] Averaged stats: lr: 0.000100  loss: 0.297746
2025-11-15 16:15:13,008 [INFO] Evaluating on valid.
2025-11-15 16:18:18,445 [INFO] Averaged stats: loss: 0.712758  acc: 0.633003 ***auc: 0.6811070365370849 ***uauc: 0.6475150837007034 ***u-nDCG: 0.8571191917606052
2025-11-15 16:18:18,451 [INFO] Start training
2025-11-15 16:18:18,458 [INFO] Start training epoch 25, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:18:45,042 [INFO] Averaged stats: lr: 0.000100  loss: 0.297777
2025-11-15 16:18:45,044 [INFO] Evaluating on valid.
2025-11-15 16:21:49,489 [INFO] Averaged stats: loss: 0.787937  acc: 0.583270 ***auc: 0.6393539953188841 ***uauc: 0.6242791313230324 ***u-nDCG: 0.8427284329143153
2025-11-15 16:21:49,496 [INFO] Start training
2025-11-15 16:21:49,502 [INFO] Start training epoch 26, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:22:16,189 [INFO] Averaged stats: lr: 0.000100  loss: 0.287479
2025-11-15 16:22:16,191 [INFO] Evaluating on valid.
2025-11-15 16:25:28,888 [INFO] Averaged stats: loss: 0.755444  acc: 0.631669 ***auc: 0.6809162342185324 ***uauc: 0.6530016366250594 ***u-nDCG: 0.8544369134630001
Evaluation  [80/82]  eta: 0:00:05  loss: 0.7442  acc: 0.5469  time: 2.5976  data: 0.0048  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5634  acc: 0.6875  time: 2.5030  data: 0.0046  max mem: 31643
Evaluation Total time: 0:03:34 (2.6111 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.2668032646179199 uauc: 0.6559772508635612
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.007648468017578125 u-nDCG: 0.8557780808181812
rank_0 auc: 0.6995238405640978
Train: data epoch: [16]  [ 0/50]  eta: 0:00:28  lr: 0.000100  loss: 0.6777  time: 0.5776  data: 0.0000  max mem: 31643
Train: data epoch: [16]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3036  time: 0.5959  data: 0.0000  max mem: 31643
Train: data epoch: [16] Total time: 0:00:29 (0.5976 s / it)
Evaluation  [ 0/82]  eta: 0:02:06  loss: 0.6854  acc: 0.6250  time: 1.5424  data: 0.0170  max mem: 31643
Evaluation  [16/82]  eta: 0:02:39  loss: 0.8050  acc: 0.5781  time: 2.4201  data: 0.0050  max mem: 31643
Evaluation  [32/82]  eta: 0:02:06  loss: 0.6048  acc: 0.7031  time: 2.6272  data: 0.0053  max mem: 31643
Evaluation  [48/82]  eta: 0:01:27  loss: 0.6626  acc: 0.6562  time: 2.6117  data: 0.0073  max mem: 31643
Evaluation  [64/82]  eta: 0:00:46  loss: 0.6894  acc: 0.6094  time: 2.6022  data: 0.0063  max mem: 31643
Evaluation  [80/82]  eta: 0:00:05  loss: 0.8812  acc: 0.5469  time: 2.4633  data: 0.0030  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4096  acc: 0.8125  time: 2.3544  data: 0.0029  max mem: 31643
Evaluation Total time: 0:03:26 (2.5218 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07706499099731445 uauc: 0.6591771241422075
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.002827882766723633 u-nDCG: 0.8566891664927219
rank_0 auc: 0.7040145995697816
Train: data epoch: [17]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.3293  time: 0.5048  data: 0.0000  max mem: 31643
Train: data epoch: [17]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3320  time: 0.5299  data: 0.0000  max mem: 31643
Train: data epoch: [17] Total time: 0:00:26 (0.5342 s / it)
Evaluation  [ 0/82]  eta: 0:01:54  loss: 0.6495  acc: 0.6562  time: 1.3988  data: 0.0045  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.7800  acc: 0.5938  time: 2.1623  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:52  loss: 0.6026  acc: 0.6875  time: 2.3152  data: 0.0026  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.6647  acc: 0.6250  time: 2.3142  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6495  acc: 0.6250  time: 2.3073  data: 0.0028  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7050  acc: 0.5938  time: 2.2732  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4879  acc: 0.8750  time: 2.1823  data: 0.0026  max mem: 31643
Evaluation Total time: 0:03:05 (2.2607 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07457828521728516 uauc: 0.6564543750265125
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030541419982910156 u-nDCG: 0.8509318597326229
rank_0 auc: 0.7035661770001095
Train: data epoch: [18]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.2169  time: 0.5152  data: 0.0000  max mem: 31643
Train: data epoch: [18]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.7510  time: 0.5402  data: 0.0000  max mem: 31643
Train: data epoch: [18] Total time: 0:00:26 (0.5387 s / it)
Evaluation  [ 0/82]  eta: 0:02:08  loss: 0.6171  acc: 0.7031  time: 1.5690  data: 0.0043  max mem: 31643
Evaluation  [16/82]  eta: 0:02:28  loss: 0.7667  acc: 0.5938  time: 2.2458  data: 0.0030  max mem: 31643
Evaluation  [32/82]  eta: 0:02:01  loss: 0.6798  acc: 0.6406  time: 2.5897  data: 0.0054  max mem: 31643
Evaluation  [48/82]  eta: 0:01:22  loss: 0.7410  acc: 0.5938  time: 2.4455  data: 0.0032  max mem: 31643
Evaluation  [64/82]  eta: 0:00:43  loss: 0.6487  acc: 0.6562  time: 2.3304  data: 0.0025  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7387  acc: 0.5625  time: 2.2265  data: 0.0028  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3391  acc: 0.8125  time: 2.1343  data: 0.0027  max mem: 31643
Evaluation Total time: 0:03:11 (2.3414 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07384753227233887 uauc: 0.6544890271703095
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.003015279769897461 u-nDCG: 0.8501229169225354
rank_0 auc: 0.7095978317729366
Train: data epoch: [19]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.3252  time: 0.5262  data: 0.0000  max mem: 31643
Train: data epoch: [19]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.2687  time: 0.5344  data: 0.0000  max mem: 31643
Train: data epoch: [19] Total time: 0:00:26 (0.5336 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.7566  acc: 0.5781  time: 1.4117  data: 0.0091  max mem: 31643
Evaluation  [16/82]  eta: 0:02:24  loss: 0.8887  acc: 0.5625  time: 2.1934  data: 0.0027  max mem: 31643
Evaluation  [32/82]  eta: 0:01:54  loss: 0.5700  acc: 0.7344  time: 2.3594  data: 0.0037  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6595  acc: 0.6250  time: 2.3358  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.6794  acc: 0.6406  time: 2.3836  data: 0.0024  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.8172  acc: 0.5469  time: 2.3754  data: 0.0026  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4278  acc: 0.8125  time: 2.2851  data: 0.0025  max mem: 31643
Evaluation Total time: 0:03:09 (2.3050 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07918477058410645 uauc: 0.6451046020354924
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0015239715576171875 u-nDCG: 0.8496175386689853
rank_0 auc: 0.6896795545589949
Train: data epoch: [20]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.1117  time: 0.5101  data: 0.0000  max mem: 31643
Train: data epoch: [20]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4025  time: 0.5287  data: 0.0000  max mem: 31643
Train: data epoch: [20] Total time: 0:00:26 (0.5372 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.7154  acc: 0.5312  time: 1.4061  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:24  loss: 0.8540  acc: 0.5625  time: 2.1958  data: 0.0027  max mem: 31643
Evaluation  [32/82]  eta: 0:01:52  loss: 0.4975  acc: 0.7656  time: 2.3194  data: 0.0025  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.6141  acc: 0.5625  time: 2.3142  data: 0.0025  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6592  acc: 0.6406  time: 2.3072  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7238  acc: 0.6250  time: 2.2906  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3076  acc: 0.8125  time: 2.1976  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:06 (2.2727 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07290863990783691 uauc: 0.6341560494022422
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.003056764602661133 u-nDCG: 0.8442830790348624
rank_0 auc: 0.6875597927693791
Train: data epoch: [21]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.4915  time: 0.5372  data: 0.0000  max mem: 31643
Train: data epoch: [21]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.2639  time: 0.5336  data: 0.0000  max mem: 31643
Train: data epoch: [21] Total time: 0:00:26 (0.5375 s / it)
Evaluation  [ 0/82]  eta: 0:01:55  loss: 0.7228  acc: 0.5938  time: 1.4042  data: 0.0061  max mem: 31643
2025-11-15 16:25:28,895 [INFO] Start training
2025-11-15 16:25:28,901 [INFO] Start training epoch 27, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:25:55,779 [INFO] Averaged stats: lr: 0.000100  loss: 0.329508
2025-11-15 16:25:55,781 [INFO] Evaluating on valid.
2025-11-15 16:29:14,115 [INFO] Averaged stats: loss: 0.703473  acc: 0.608613 ***auc: 0.6727792948748121 ***uauc: 0.6161254449549977 ***u-nDCG: 0.8422087599745355
2025-11-15 16:29:14,136 [INFO] Start training
2025-11-15 16:29:14,154 [INFO] Start training epoch 28, 50 iters per inner epoch.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\tasks\base_task.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-15 16:29:44,516 [INFO] Averaged stats: lr: 0.000100  loss: 0.293352
2025-11-15 16:29:44,517 [INFO] Evaluating on valid.
2025-11-15 16:32:53,735 [INFO] Averaged stats: loss: 0.800177  acc: 0.612995 ***auc: 0.6690662966042088 ***uauc: 0.6204316188924973 ***u-nDCG: 0.8465009848120868
2025-11-15 16:32:53,738 [INFO] Early stop. The results has not changed up to 20 epochs.
2025-11-15 16:32:53,738 [INFO] Training time 1:46:14
Evaluation  [16/82]  eta: 0:02:21  loss: 0.8149  acc: 0.6250  time: 2.1473  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:51  loss: 0.5317  acc: 0.7500  time: 2.2961  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.6777  acc: 0.6250  time: 2.3662  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.7609  acc: 0.5625  time: 2.3538  data: 0.0028  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.8133  acc: 0.5938  time: 2.4375  data: 0.0047  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3999  acc: 0.8125  time: 2.3456  data: 0.0046  max mem: 31643
Evaluation Total time: 0:03:08 (2.3041 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07406258583068848 uauc: 0.6388071583881134
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030829906463623047 u-nDCG: 0.8470066330981613
rank_0 auc: 0.681407568749342
Train: data epoch: [22]  [ 0/50]  eta: 0:00:24  lr: 0.000100  loss: 0.1630  time: 0.4921  data: 0.0000  max mem: 31643
Train: data epoch: [22]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3311  time: 0.5427  data: 0.0000  max mem: 31643
Train: data epoch: [22] Total time: 0:00:26 (0.5375 s / it)
Evaluation  [ 0/82]  eta: 0:02:05  loss: 0.7100  acc: 0.6406  time: 1.5277  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.8333  acc: 0.5938  time: 2.1593  data: 0.0027  max mem: 31643
Evaluation  [32/82]  eta: 0:01:54  loss: 0.5779  acc: 0.7812  time: 2.3932  data: 0.0031  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.7053  acc: 0.5781  time: 2.3356  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.7074  acc: 0.6250  time: 2.3832  data: 0.0034  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7710  acc: 0.5938  time: 2.2902  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3699  acc: 0.7500  time: 2.1970  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:09 (2.3056 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07749581336975098 uauc: 0.637052509782979
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0033538341522216797 u-nDCG: 0.8502172006916532
rank_0 auc: 0.6723560403930735
Train: data epoch: [23]  [ 0/50]  eta: 0:00:27  lr: 0.000100  loss: 0.2785  time: 0.5462  data: 0.0000  max mem: 31643
Train: data epoch: [23]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.2796  time: 0.5368  data: 0.0000  max mem: 31643
Train: data epoch: [23] Total time: 0:00:26 (0.5369 s / it)
Evaluation  [ 0/82]  eta: 0:01:56  loss: 0.7569  acc: 0.6250  time: 1.4199  data: 0.0047  max mem: 31643
Evaluation  [16/82]  eta: 0:02:31  loss: 0.9009  acc: 0.5625  time: 2.2898  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:57  loss: 0.7440  acc: 0.6719  time: 2.4782  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:20  loss: 0.7589  acc: 0.6250  time: 2.3290  data: 0.0029  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.7746  acc: 0.6250  time: 2.3463  data: 0.0025  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.9359  acc: 0.5781  time: 2.2945  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.2711  acc: 0.8750  time: 2.1999  data: 0.0023  max mem: 31643
Evaluation Total time: 0:03:09 (2.3156 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07786083221435547 uauc: 0.6385557629388454
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030868053436279297 u-nDCG: 0.8538759418629558
rank_0 auc: 0.6938889358264242
Train: data epoch: [24]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.5165  time: 0.5026  data: 0.0000  max mem: 31643
Train: data epoch: [24]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.2056  time: 0.5309  data: 0.0000  max mem: 31643
Train: data epoch: [24] Total time: 0:00:26 (0.5328 s / it)
Evaluation  [ 0/82]  eta: 0:01:54  loss: 0.6799  acc: 0.6406  time: 1.3939  data: 0.0042  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.7369  acc: 0.6562  time: 2.1579  data: 0.0028  max mem: 31643
Evaluation  [32/82]  eta: 0:01:52  loss: 0.6455  acc: 0.7500  time: 2.3181  data: 0.0025  max mem: 31643
Evaluation  [48/82]  eta: 0:01:17  loss: 0.7213  acc: 0.6094  time: 2.3005  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.6708  acc: 0.6406  time: 2.3118  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7807  acc: 0.5938  time: 2.2788  data: 0.0028  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.3332  acc: 0.7500  time: 2.1856  data: 0.0027  max mem: 31643
Evaluation Total time: 0:03:05 (2.2603 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07495427131652832 uauc: 0.6475150837007034
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0015239715576171875 u-nDCG: 0.8571191917606052
rank_0 auc: 0.6811070365370849
Train: data epoch: [25]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.1854  time: 0.5371  data: 0.0000  max mem: 31643
Train: data epoch: [25]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.1572  time: 0.5371  data: 0.0000  max mem: 31643
Train: data epoch: [25] Total time: 0:00:26 (0.5317 s / it)
Evaluation  [ 0/82]  eta: 0:01:53  loss: 0.7987  acc: 0.5781  time: 1.3786  data: 0.0047  max mem: 31643
Evaluation  [16/82]  eta: 0:02:20  loss: 0.8300  acc: 0.6094  time: 2.1360  data: 0.0026  max mem: 31643
Evaluation  [32/82]  eta: 0:01:49  loss: 0.6568  acc: 0.6875  time: 2.2375  data: 0.0024  max mem: 31643
Evaluation  [48/82]  eta: 0:01:16  loss: 0.7143  acc: 0.5781  time: 2.3516  data: 0.0023  max mem: 31643
Evaluation  [64/82]  eta: 0:00:41  loss: 0.8102  acc: 0.5156  time: 2.3087  data: 0.0030  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.8427  acc: 0.5000  time: 2.2698  data: 0.0024  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.4350  acc: 0.7500  time: 2.1816  data: 0.0022  max mem: 31643
Evaluation Total time: 0:03:04 (2.2482 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07814168930053711 uauc: 0.6242791313230324
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.003069162368774414 u-nDCG: 0.8427284329143153
rank_0 auc: 0.6393539953188841
Train: data epoch: [26]  [ 0/50]  eta: 0:00:25  lr: 0.000100  loss: 0.2816  time: 0.5058  data: 0.0000  max mem: 31643
Train: data epoch: [26]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.3976  time: 0.5347  data: 0.0000  max mem: 31643
Train: data epoch: [26] Total time: 0:00:26 (0.5338 s / it)
Evaluation  [ 0/82]  eta: 0:02:05  loss: 0.7540  acc: 0.5469  time: 1.5260  data: 0.0061  max mem: 31643
Evaluation  [16/82]  eta: 0:02:22  loss: 0.8198  acc: 0.6250  time: 2.1648  data: 0.0027  max mem: 31643
Evaluation  [32/82]  eta: 0:01:52  loss: 0.6018  acc: 0.7188  time: 2.3362  data: 0.0026  max mem: 31643
Evaluation  [48/82]  eta: 0:01:21  loss: 0.6405  acc: 0.6875  time: 2.6181  data: 0.0068  max mem: 31643
Evaluation  [64/82]  eta: 0:00:43  loss: 0.7515  acc: 0.5781  time: 2.3966  data: 0.0048  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.7834  acc: 0.5781  time: 2.3007  data: 0.0023  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5955  acc: 0.8125  time: 2.2076  data: 0.0021  max mem: 31643
Evaluation Total time: 0:03:12 (2.3489 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07580137252807617 uauc: 0.6530016366250594
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0015079975128173828 u-nDCG: 0.8544369134630001
rank_0 auc: 0.6809162342185324
Train: data epoch: [27]  [ 0/50]  eta: 0:00:26  lr: 0.000100  loss: 0.0907  time: 0.5387  data: 0.0000  max mem: 31643
Train: data epoch: [27]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.4807  time: 0.5362  data: 0.0000  max mem: 31643
Train: data epoch: [27] Total time: 0:00:26 (0.5376 s / it)
Evaluation  [ 0/82]  eta: 0:01:54  loss: 0.7189  acc: 0.5781  time: 1.3969  data: 0.0046  max mem: 31643
Evaluation  [16/82]  eta: 0:02:25  loss: 0.6855  acc: 0.6250  time: 2.2056  data: 0.0025  max mem: 31643
Evaluation  [32/82]  eta: 0:01:53  loss: 0.6652  acc: 0.6406  time: 2.3235  data: 0.0027  max mem: 31643
Evaluation  [48/82]  eta: 0:01:18  loss: 0.6971  acc: 0.5781  time: 2.3365  data: 0.0024  max mem: 31643
Evaluation  [64/82]  eta: 0:00:43  loss: 0.6992  acc: 0.6562  time: 2.6074  data: 0.0051  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.6573  acc: 0.6406  time: 2.6548  data: 0.0054  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5716  acc: 0.6250  time: 2.5224  data: 0.0049  max mem: 31643
Evaluation Total time: 0:03:18 (2.4147 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.2884230613708496 uauc: 0.6161254449549977
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.007623195648193359 u-nDCG: 0.8422087599745355
rank_0 auc: 0.6727792948748121
Train: data epoch: [28]  [ 0/50]  eta: 0:00:27  lr: 0.000100  loss: 0.1770  time: 0.5445  data: 0.0000  max mem: 31643
Train: data epoch: [28]  [49/50]  eta: 0:00:00  lr: 0.000100  loss: 0.1304  time: 0.6056  data: 0.0000  max mem: 31643
Train: data epoch: [28] Total time: 0:00:30 (0.6072 s / it)
Evaluation  [ 0/82]  eta: 0:02:14  loss: 0.7872  acc: 0.6094  time: 1.6364  data: 0.0093  max mem: 31643
Evaluation  [16/82]  eta: 0:02:29  loss: 0.9196  acc: 0.5781  time: 2.2640  data: 0.0032  max mem: 31643
Evaluation  [32/82]  eta: 0:01:55  loss: 0.7269  acc: 0.7344  time: 2.3555  data: 0.0025  max mem: 31643
Evaluation  [48/82]  eta: 0:01:19  loss: 0.8624  acc: 0.5781  time: 2.3329  data: 0.0026  max mem: 31643
Evaluation  [64/82]  eta: 0:00:42  loss: 0.8991  acc: 0.5156  time: 2.3293  data: 0.0026  max mem: 31643
Evaluation  [80/82]  eta: 0:00:04  loss: 0.9892  acc: 0.4844  time: 2.3033  data: 0.0025  max mem: 31643
Evaluation  [81/82]  eta: 0:00:02  loss: 0.5452  acc: 0.6250  time: 2.2099  data: 0.0024  max mem: 31643
Evaluation Total time: 0:03:09 (2.3064 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.07883858680725098 uauc: 0.6204316188924973
only one interaction users (for nDCG): 48
computed user (for nDCG): 239 can not users: 44
u-nDCG for validation Cost: 0.0030317306518554688 u-nDCG: 0.8465009848120868
rank_0 auc: 0.6690662966042088
