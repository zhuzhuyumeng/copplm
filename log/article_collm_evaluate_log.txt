W1106 18:31:09.906729 46008 site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
2025-11-06 18:31:09,919 [INFO] Building datasets...
2025-11-06 18:31:10,113 [INFO] Movie OOD datasets, max history length:10
2025-11-06 18:31:10,142 [INFO] Movie OOD datasets, max history length:10
2025-11-06 18:31:10,304 [INFO] Movie OOD datasets, max history length:10
2025-11-06 18:31:10,343 [INFO] Movie OOD datasets, max history length:10
2025-11-06 18:31:10,380 [INFO] Movie OOD datasets, max history length:10
2025-11-06 18:31:10,524 [INFO] 
=====  Running Parameters    =====
2025-11-06 18:31:10,524 [INFO] {
    "amp": true,
    "batch_size_eval": 64,
    "batch_size_train": 16,
    "device": "cuda",
    "dist_url": "env://",
    "distributed": false,
    "evaluate": true,
    "init_lr": 0.001,
    "iters_per_epoch": 50,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 1000,
    "min_lr": 8e-05,
    "mode": "v2",
    "num_workers": 0,
    "output_dir": "minigpt4rec-log",
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "rec_pretrain",
    "test_splits": [
        "test",
        "valid"
    ],
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "valid"
    ],
    "warmup_lr": 1e-05,
    "warmup_steps": 200,
    "weight_decay": 0.001,
    "world_size": 1
}
2025-11-06 18:31:10,532 [INFO] 
======  Dataset Attributes  ======
2025-11-06 18:31:10,532 [INFO] 
======== movie_ood =======
2025-11-06 18:31:10,532 [INFO] {
    "build_info": {
        "storage": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
    },
    "data_type": "default",
    "path": "D:\\Pycoding\\CoLLM-main\\CoLLM-main\\collm-datasets\\ml-1m\\ml-1m\\"
}
2025-11-06 18:31:10,532 [INFO] 
======  Model Attributes  ======
2025-11-06 18:31:10,533 [INFO] {
    "ans_type": "v2",
    "arch": "mini_gpt4rec_v2",
    "ckpt": "collm-trained-models/collm-mf-ml-best.pth",
    "end_sym": "###",
    "freeze_lora": false,
    "freeze_proj": true,
    "freeze_rec": true,
    "item_num": -100,
    "llama_model": "Vicuna-7b-delta-v0",
    "lora_config": {
        "alpha": 16,
        "dropout": 0.05,
        "r": 8,
        "target_modules": [
            "q_proj",
            "v_proj"
        ],
        "use_lora": true
    },
    "max_txt_len": 1024,
    "model_type": "pretrain_vicuna",
    "proj_drop": 0,
    "proj_mid_times": 10,
    "proj_token_num": 1,
    "prompt_path": "prompts/collm_movie.txt",
    "prompt_template": "{}",
    "rec_config": {
        "embedding_size": 256,
        "item_num": 3256,
        "pretrained_path": "collm-trained-models/0912_ml1m_oodv2_best_model_d256lr-0.001wd0.0001.pth",
        "user_num": 839
    },
    "rec_model": "MF",
    "user_num": -100
}
2025-11-06 18:31:10,543 [INFO] freeze rec encoder
`torch_dtype` is deprecated! Use `dtype` instead!
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From \U0001f449v4.50\U0001f448 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
binary_path: D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll
CUDA SETUP: Loading binary D:\Anaconda3\envs\minigpt4\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll...
Not using distributed mode
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\train data size: (33891, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\valid_small data size: (5200, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\test data size: (7331, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\test_warm_cold data size: (3522, 7)
Movie OOD datasets, max history length: 10
data path: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\test_warm_cold data size: (3178, 7)
Movie OOD datasets, max history length: 10
data dir: D:\Pycoding\CoLLM-main\CoLLM-main\collm-datasets\ml-1m\ml-1m\
runing MiniGPT4Rec_v2 ...... 
Loading Rec_model
### rec_encoder: MF
creat MF model, user num: 839 item num: 3256
successfully load the pretrained model......
freeze rec encoder
Loading Rec_model Done
Loading LLama model: Vicuna-7b-delta-v0
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 3/3 [00:00<00:00, 163.06it/s]
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at Vicuna-7b-delta-v0 and are newly initialized: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From \U0001f449v4.50\U0001f448 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From \U0001f449v4.50\U0001f448 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-11-06 18:31:15,233 [INFO] !!!! freeze llama_proj...
Loading LLAMA Done
Setting Lora
Setting Lora Done
type: <class 'int'> 10
Load 4 training prompts
Prompt List: 
['#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:', '#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user\'s preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \\n#Answer:']
Load MiniGPT4Rec Checkpoint: collm-trained-models/collm-mf-ml-best.pth
loading message, msg.... 
 _IncompatibleKeys(missing_keys=['rec_encoder.user_embedding.weight', 'rec_encoder.item_embedding.weight', 'llama_model.base_model.model.model.embed_tokens.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.28.input_layernorm.weight', 'llama_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.29.input_layernorm.weight', 'llama_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.30.input_layernorm.weight', 'llama_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'llama_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq', 'llama_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.31.input_layernorm.weight', 'llama_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'llama_model.base_model.model.model.norm.weight', 'llama_model.base_model.model.lm_head.weight'], unexpected_keys=['llama_model_lora.base_model.model.model.embed_tokens.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.28.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.28.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.28.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.29.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.29.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.29.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.30.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.30.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.30.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.q_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.k_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.v_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.o_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.self_attn.rotary_emb.inv_freq', 'llama_model_lora.base_model.model.model.layers.31.mlp.gate_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.mlp.down_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.mlp.up_proj.weight', 'llama_model_lora.base_model.model.model.layers.31.input_layernorm.weight', 'llama_model_lora.base_model.model.model.layers.31.post_attention_layernorm.weight', 'llama_model_lora.base_model.model.model.norm.weight', 'llama_model_lora.base_model.model.lm_head.weight', 'llama_model.model.layers.0.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.1.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.2.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.3.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.4.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.5.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.6.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.7.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.8.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.9.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.10.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.11.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.12.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.13.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.14.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.15.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.16.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.17.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.18.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.19.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.20.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.21.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.22.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.23.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.24.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.25.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.26.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.27.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.28.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.29.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.30.self_attn.rotary_emb.inv_freq', 'llama_model.model.layers.31.self_attn.rotary_emb.inv_freq'])2025-11-06 18:31:22,812 [INFO] Evaluating on test.
2025-11-06 18:31:22,812 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2025-11-06 18:31:22,812 [INFO] Loaded 33891 records for train split from the dataset.
2025-11-06 18:31:22,812 [INFO] Loaded 5200 records for valid split from the dataset.
2025-11-06 18:31:22,812 [INFO] Loaded 7331 records for test split from the dataset.
2025-11-06 18:31:22,812 [INFO] Loaded 3522 records for test_warm split from the dataset.
2025-11-06 18:31:22,812 [INFO] Loaded 3178 records for test_cold split from the dataset.
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-06 19:00:12,757 [INFO] Averaged stats: loss: 6.273661  acc: 0.545571 ***auc: 0.4591001246197656 ***uauc:(0.7386548308628503, [4, 6, 16, 26, 30, 33, 37, 40, 41, 47, 48, 50, 51, 61, 67, 72, 73, 80, 82, 83, 92, 93, 95, 104, 110, 117, 118, 121, 126, 132, 142, 143, 145, 147, 155, 164, 165, 167, 169, 170, 171, 173, 177, 178, 182, 183, 184, 185, 192, 193, 199, 205, 207, 208, 215, 220, 226, 227, 228, 230, 231, 237, 242, 246, 247, 250, 254, 255, 256, 260, 261, 273, 275, 277, 279, 282, 286, 288, 291, 296, 300, 301, 302, 304, 308, 313, 314, 319, 322, 325, 327, 338, 343, 344, 345, 349, 353, 354, 358, 362, 366, 370, 371, 379, 395, 403, 404, 409, 413, 418, 426, 427, 433, 434, 436, 437, 444, 449, 454, 456, 461, 464, 466, 468, 469, 471, 474, 490, 500, 501, 504, 507, 515, 517, 521, 530, 533, 538, 544, 550, 552, 553, 559, 562, 563, 564, 566, 582, 583, 585, 588, 594, 598, 603, 607, 614, 617, 618, 623, 624, 627, 629, 631, 632, 635, 641, 645, 646, 648, 651, 653, 655, 658, 662, 664, 665, 672, 679, 680, 681, 690, 695, 697, 699, 703, 705, 707, 709, 710, 714, 717, 720, 721, 725, 728, 731, 732, 735, 736, 739, 747, 748, 758, 761, 762, 763, 766, 768, 773, 782, 784, 785, 788, 796, 797, 800, 802, 803, 805, 809, 812, 816, 821, 833], array([0.94097635, 0.83643745, 0.6999905 , 0.84305587, 0.99369828,
       0.63092975, 0.86880839, 0.81242101, 0.87227367, 0.76263724,
       0.79847474, 0.5       , 0.89906714, 0.94415923, 0.63092975,
       0.82910245, 0.56814554, 0.4249599 , 0.84240447, 0.88085963,
       0.97264576, 0.72386197, 0.63092975, 0.95071391, 0.63092975,
       0.91972079, 0.73515206, 0.27849016, 0.5       , 0.63092975,
       0.96462099, 0.6934264 , 0.81890682, 0.92621952, 0.47340414,
       0.66426339, 0.64798396, 0.99412055, 0.6603301 , 0.69014946,
       0.50269641, 0.43067656, 0.57064172, 0.72173273, 1.        ,
       0.78643374, 0.77843239, 0.74689069, 1.        , 0.48247556,
       0.97030931, 1.        , 0.62310505, 0.72931445, 0.89114419,
       0.85545003, 0.84397934, 1.        , 0.79555422, 0.43067656,
       0.74345325, 0.33333333, 0.94386615, 0.91655426, 0.63092975,
       0.55263867, 0.62963336, 1.        , 0.52163998, 0.97398463,
       0.5511174 , 0.85202718, 0.54870247, 0.91457355, 0.38685281,
       0.27780762, 0.6934264 , 0.93886994, 0.46203213, 0.49325936,
       0.28620922, 0.75385641, 0.91498351, 0.41685767, 0.58833002,
       0.41242293, 0.55528093, 0.81977592, 0.54799723, 0.50690498,
       0.71566323, 0.74863615, 0.65784141, 0.38685281, 0.88234401,
       0.81768294, 0.97336134, 0.53910656, 0.79436589, 0.8928882 ,
       1.        , 0.82208273, 0.74894107, 0.71263803, 0.54335368,
       0.80336571, 0.84986625, 1.        , 0.83889426, 0.91328617,
       0.91972079, 0.80058534, 0.73533468, 0.41402205, 0.6934264 ,
       0.7766629 , 0.69337527, 0.38685281, 0.85546509, 0.8510994 ,
       0.84978052, 0.60526024, 1.        , 0.77980811, 0.87721532,
       0.76063957, 0.66451735, 0.93252109, 0.63092975, 0.73282862,
       0.75610224, 0.85790752, 0.58941219, 0.63404995, 0.86664308,
       0.66531072, 0.81209598, 0.95049373, 0.6719508 , 0.41124538,
       0.59428177, 0.63092975, 0.8120411 , 0.68219854, 0.74498907,
       0.6182885 , 0.90453957, 1.        , 0.51948452, 0.74490501,
       0.69384312, 0.63150899, 0.23981247, 0.84758793, 0.63092975,
       1.        , 0.55080959, 0.63092975, 0.96418035, 1.        ,
       0.52569404, 1.        , 0.78748418, 0.92319132, 0.82424338,
       0.91972079, 1.        , 0.95068666, 0.82766456, 0.45749453,
       0.63092975, 0.75090028, 0.78551766, 0.53660038, 0.56794855,
       0.52157306, 0.51339511, 0.64382231, 0.88618498, 0.57129484,
       0.82320064, 0.89906901, 0.6934264 , 0.68860074, 0.60819306,
       0.78665032, 0.98037715, 0.63302583, 0.96733441, 0.88073461,
       0.98561566, 0.55670776, 0.71058282, 0.93307952, 0.87071524,
       0.8698825 , 0.94476603, 0.77238279, 0.70452308, 0.41957767,
       0.96032078, 0.48444261, 0.80724676, 0.84273287, 0.58494708,
       0.82525578, 0.88969551, 0.91972079, 0.91972079, 0.63092975,
       0.90682504, 0.60526024, 0.67973105, 0.87721532, 0.53126205,
       0.94941458, 0.82715069, 0.5       , 0.63092975, 0.80121199,
       0.90602544, 0.5       , 0.85519992, 1.        ]))
D:\Pycoding\CoLLM-main\CoLLM-main\minigpt4\models\rec_model.py:48: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
2025-11-06 19:28:09,315 [INFO] Averaged stats: loss: 6.431004  acc: 0.530297 ***auc: 0.4570818767642718 ***uauc:(0.7410303235007176, [20, 26, 33, 36, 37, 39, 40, 41, 49, 57, 72, 73, 79, 83, 85, 87, 93, 95, 104, 107, 108, 109, 110, 112, 119, 121, 124, 139, 142, 145, 147, 149, 153, 155, 159, 164, 165, 170, 171, 173, 174, 175, 177, 178, 180, 185, 186, 193, 197, 200, 204, 205, 207, 208, 210, 212, 214, 215, 221, 222, 226, 227, 228, 230, 231, 237, 242, 246, 247, 249, 250, 255, 260, 261, 264, 271, 273, 275, 277, 279, 282, 286, 291, 292, 296, 297, 298, 300, 304, 308, 309, 312, 314, 325, 327, 329, 336, 338, 340, 341, 345, 346, 350, 353, 356, 364, 367, 368, 370, 377, 381, 382, 384, 389, 402, 415, 426, 427, 428, 433, 434, 436, 441, 444, 446, 449, 451, 453, 458, 462, 466, 470, 472, 474, 475, 482, 484, 490, 494, 496, 498, 500, 512, 516, 521, 522, 529, 530, 531, 550, 558, 561, 562, 563, 566, 568, 569, 575, 576, 577, 581, 583, 598, 604, 607, 608, 609, 613, 617, 624, 626, 627, 628, 629, 630, 632, 633, 643, 645, 649, 650, 658, 664, 665, 669, 670, 671, 672, 676, 686, 690, 695, 696, 697, 698, 699, 700, 701, 702, 706, 707, 709, 712, 714, 718, 720, 725, 727, 728, 733, 748, 752, 753, 758, 760, 761, 762, 767, 769, 772, 773, 774, 779, 784, 786, 788, 789, 793, 795, 797, 798, 799, 802, 803, 804, 809, 814, 818, 833], array([1.        , 0.76043445, 0.6182885 , 0.89236236, 0.71226307,
       0.7816511 , 0.66638301, 0.84368422, 0.98083251, 0.95582959,
       0.92323491, 0.55533367, 0.58336079, 0.77372006, 0.97092864,
       0.61458374, 1.        , 0.45749453, 0.7856497 , 0.96334323,
       0.81045447, 1.        , 0.9848699 , 1.        , 0.87721532,
       0.72600275, 0.89489284, 0.88261309, 0.9888026 , 0.51730876,
       0.59678825, 0.74242428, 0.63092975, 0.40385767, 0.65092093,
       0.71982451, 0.84550052, 0.8955963 , 0.59543514, 1.        ,
       0.89275379, 0.96746798, 0.85326315, 0.5       , 0.97474295,
       0.90471723, 0.90602544, 0.79420042, 0.6934264 , 1.        ,
       1.        , 0.73651738, 0.55080959, 0.8999583 , 0.77986001,
       0.54377131, 0.57064172, 0.70696856, 0.78422809, 0.82967281,
       0.91034995, 0.46845052, 0.77986772, 0.64269972, 0.74819301,
       0.5       , 0.6934264 , 0.88545988, 1.        , 0.86141037,
       0.68977792, 0.90602544, 0.98604333, 0.68811696, 0.77446701,
       0.64446386, 0.86760262, 0.73728382, 0.64144188, 0.32263571,
       0.3978088 , 0.60624013, 0.30103   , 0.41349652, 0.79931042,
       0.51356818, 0.70440898, 0.65294622, 0.4193618 , 0.853106  ,
       0.83344343, 0.93018968, 0.64719551, 0.45560515, 0.91972079,
       0.63416179, 1.        , 0.54377131, 0.96746798, 0.65339336,
       0.73102357, 0.6934264 , 0.94474038, 0.69788173, 0.91972079,
       0.77447891, 0.64956145, 0.67973105, 0.76117403, 0.63092975,
       0.59351934, 0.70403199, 0.6182885 , 0.7499278 , 0.88545988,
       0.76063957, 0.8238767 , 0.73282862, 0.63092975, 0.74863615,
       0.63689439, 0.68223015, 0.64220627, 0.63092975, 0.72636751,
       0.63351291, 0.73282862, 0.85034491, 0.81152454, 0.86220708,
       0.63092975, 0.97522501, 0.43067656, 0.90682504, 0.63493567,
       0.6934264 , 1.        , 0.45560515, 0.93151718, 1.        ,
       0.85278523, 0.63092975, 0.95582959, 0.90062588, 0.33333333,
       0.78985689, 0.42278983, 0.66555969, 0.60856684, 0.85292787,
       0.66943526, 0.63092975, 1.        , 0.68510031, 0.63092975,
       0.87810157, 0.71192867, 0.73277114, 0.68714752, 0.73282862,
       0.6934264 , 0.51150755, 0.40298313, 1.        , 0.63432281,
       0.9548153 , 0.83733831, 0.84549297, 0.31546488, 0.75362666,
       0.90477596, 0.64205877, 0.63945623, 0.59405238, 0.91013701,
       0.90243683, 0.6934264 , 0.78445249, 1.        , 0.38685281,
       0.63092975, 0.65339336, 0.73282862, 0.89205081, 0.74125294,
       0.90471723, 0.42548359, 0.38685281, 0.76349942, 0.8113305 ,
       0.73282862, 1.        , 0.9523563 , 0.6934264 , 0.97368794,
       0.43067656, 0.73282862, 0.63092975, 0.91964617, 0.897394  ,
       0.7134682 , 0.81997779, 0.96972993, 0.71899681, 0.96863937,
       0.89858704, 0.88823634, 0.5       , 0.5       , 0.93534365,
       0.91972079, 0.90602544, 0.58334161, 0.76205755, 1.        ,
       0.63092975, 0.60465743, 0.63092975, 0.63092975, 0.63092975,
       0.66646599, 0.6813517 , 0.77867061, 0.7082044 , 0.63092975,
       0.67683443, 0.57064172, 0.5       , 0.63092975, 0.50548971,
       0.46440104, 0.66534971, 0.81511836, 0.96746798, 0.71935549,
       0.87029172, 0.93792415, 0.48600899, 0.84841726]))
2025-11-06 19:28:09,317 [INFO] Training time 0:56:50

answer token ids: pos: 3869 neg ids: 1939
Prompt Pos Example 
#Question: A user has given high ratings to the following movies: <ItemTitleList>. Additionally, we have information about the user's preferences encoded in the feature <UserID>. Using all available information, make a prediction about whether the user would enjoy the movie titled <TargetItemTitle> with the feature <TargetItemID>? Answer with "Yes" or "No". \n#Answer: Yes or No
training finish or just evaluation...
prompt example: <s>#Question: A user has given high ratings to the following movies: "War Room, The (1993)", "U2: Rattle and Hum (1988)", "Madonna: Truth or Dare (1991)", "Breaking Away (1979)", "Tombstone (1993)", "High Noon (1952)", "Space Cowboys (2000)", "Better Than Chocolate (1999)", "Deconstructing Harry (1997)", "Indecent Proposal (1993)". Additionally, we have information about the user's preferences encoded in the feature <unk>. Using all available information, make a prediction about whether the user would enjoy the movie titled "But I'm a Cheerleader (1999)" with the feature <unk>? Answer with "Yes" or "No". \n#Answer:
#######prmpt decoded example:  </s> </s> </s> </s> </s> </s> </s> </s> </s> <s> # Question : A user has given high ratings to the following mov ies : " When We W ere Kings ( 1 9 9 6 )", " Cell ul oid Cl os et , The ( 1 9 9 5 )", " W ar Room , The ( 1 9 9 3 )", " U 2 : R attle and Hum ( 1 9 8 8 )", " Mad onna : Tr uth or D are ( 1 9 9 1 )", " Bre aking A way ( 1 9 7 9 )", " T omb stone ( 1 9 9 3 )", " High No on ( 1 9 5 2 )", " Space Cow bo ys ( 2 0 0 0 )", " B etter Th an Ch oc olate ( 1 9 9 9 ) ". Additionally , we have information about the user ' s prefer ences encoded in the feature  <unk> . Using all available information , make a prediction about whether the user would enjoy the movie titled " De construct ing Harry ( 1 9 9 7 )" with the feature  <unk> ? Answer with " Yes " or " No ". \ n # Answer :
Evaluation  [  0/115]  eta: 0:25:25  loss: 5.4481  acc: 0.6250  time: 13.2630  data: 0.0210  max mem: 24548
Evaluation  [ 23/115]  eta: 0:26:00  loss: 3.6440  acc: 0.7344  time: 15.5398  data: 0.0043  max mem: 28249
Evaluation  [ 46/115]  eta: 0:18:39  loss: 7.9569  acc: 0.3906  time: 15.2543  data: 0.0059  max mem: 28465
Evaluation  [ 69/115]  eta: 0:12:28  loss: 5.6877  acc: 0.5938  time: 16.2712  data: 0.0026  max mem: 29279
Evaluation  [ 92/115]  eta: 0:06:07  loss: 4.3479  acc: 0.6719  time: 15.2786  data: 0.0026  max mem: 29279
Evaluation  [114/115]  eta: 0:00:15  loss: 5.6882  acc: 0.6000  time: 11.0777  data: 0.0024  max mem: 29279
Evaluation Total time: 0:28:49 (15.0428 s / it)
only one interaction users: 33
computed user: 224 can not users: 63
uauc for validation Cost: 0.0030319690704345703 uauc: 0.7386548308628503
rank_0 auc: 0.4591001246197656
Evaluation  [ 0/82]  eta: 0:30:43  loss: 5.8667  acc: 0.5625  time: 22.4825  data: 0.0047  max mem: 29279
Evaluation  [16/82]  eta: 0:22:27  loss: 6.6816  acc: 0.5000  time: 20.4114  data: 0.0073  max mem: 29279
Evaluation  [32/82]  eta: 0:18:05  loss: 8.5538  acc: 0.3750  time: 24.2157  data: 0.0061  max mem: 29279
Evaluation  [48/82]  eta: 0:11:35  loss: 7.5527  acc: 0.4531  time: 17.0245  data: 0.0055  max mem: 29279
Evaluation  [64/82]  eta: 0:06:14  loss: 7.1896  acc: 0.4844  time: 20.4132  data: 0.0068  max mem: 29279
Evaluation  [80/82]  eta: 0:00:41  loss: 6.1810  acc: 0.5625  time: 19.5151  data: 0.0055  max mem: 29279
Evaluation  [81/82]  eta: 0:00:20  loss: 6.8154  acc: 0.5000  time: 19.0605  data: 0.0054  max mem: 29279
Evaluation Total time: 0:27:56 (20.4453 s / it)
only one interaction users: 48
computed user: 239 can not users: 44
uauc for validation Cost: 0.01842331886291504 uauc: 0.7410303235007176
rank_0 auc: 0.4570818767642718
